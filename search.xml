<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[springboot-JPA]]></title>
    <url>%2F2018%2F04%2F22%2Fspringboot-JPA%2F</url>
    <content type="text"><![CDATA[spring-data-jpa其实JPA本身并不是一种框架，是一种规范，其全称是Java Persistence API，是是Sun官方提出的Java持久化规范，而他的出现主要是为了简化现有的持久化开发工作和整合ORM技术，并且其是在充分吸收了现有Hibernate，TopLink，JDO等ORM框架的基础上发展而来的，具有易于使用，伸缩性强等优点。而官网对spring data jpa是这么介绍的：Spring Data JPA是Spring Data系列的一部分，可以轻松实现基于JPA的存储库。该模块处理对基于JPA的数据访问层的增强的支持。这使得使用数据访问技术构建Spring供电的应用程序变得更加容易。 其本质就是像mybatis，hibernate那样与数据库进行交互的ORM技术，其原理就是整合了Hibernate的封装库，因为刚开始肯定要在网上找代码来看看具体逻辑到底是什么样的，这里我是以翟永超的springboot教程里面的整合spring-data-jpa这节来学习的，想学的朋友也可以直接到他的blog上去找项目来看，所有的项目的都在github上可以下载到。 教程链接：http://blog.didispace.com/springbootdata2/ 首先我是先看完代码后想自己模仿着完成一个小案例，将数据存入数据库内，再把它取出来（这tm是最简单的操作了把），没想到这个框架极恶心，总之我是困在了NullPointerException上了。。。 我先从他的开源项目上去分析一下他的代码。 首先他是将依赖包导入到POM中，看看导入代码： &lt;!-- Mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.21&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring-data-jpa --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; 以MySQL数据库为例，先引入MySQL连接的依赖包，然后直接在application.properties中配置数据源的参数信息。 spring.datasource.url=jdbc:mysql://localhost:3306/test spring.datasource.username=dbuser spring.datasource.password=dbpass spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.jpa.properties.hibernate.hbm2ddl.auto=create spring.jpa.properties.hibernate.hbm2ddl.auto是hibernate的配置属性，其主要作用是：自动创建、更新、验证数据库表结构。该参数的几种配置如下：-create：每次加载hibernate时都会删除上一次的生成的表，然后根据你的model类再重新来生成新表，哪怕两次没有任何改变也要这样执行，这就是导致数据库表数据丢失的一个重要原因。表示启动的时候先drop，再create. -create-drop：每次加载hibernate时根据model类生成表，但是sessionFactory一关闭,表就自动删除。也表示创建，只不过再系统关闭前执行一下drop. -update：最常用的属性，第一次加载hibernate时根据model类会自动建立起表的结构（前提是先建立好数据库），以后加载hibernate时根据model类自动更新表结构，即使表结构改变了但表中的行仍然存在不会删除以前的行。要注意的是当部署到服务器后，表结构是不会被马上建立起来的，是要等应用第一次运行起来后才会。这个操作启动的时候会去检查schema是否一致，如果不一致会做scheme更新.最常用的就是这个了。。。 -validate：每次加载hibernate时，验证创建数据库表结构，只会和数据库中的表进行比较，不会创建新表，但是会插入新值。 启动时验证现有schema与你配置的hibernate是否一致，如果不一致就抛出异常，并不做更新。 我在后面的时候要测试时发现数据在程序中已经执行成功存入数据库了，可是在查看数据库的时候却没有发现生成响应的表，我后来发现我用了create-drop这个参数，因为我用testNG测的，测试模块一结束，sessionFactory就关闭了，所以生成的表也就自动删除了，所以我这里改成create，这样数据导入的时候，我们可以在数据库里面看到相应的表生成。 如果是在本机上进行调试，则url可以写成jdbc:mysql:///test也可以。。。注意test是你的数据库名字，如果没有这个数据库就会报mysql错误，我刚开始也是以为会自动建库，小失误。事先要把这个test库给建立起来，mysql如何建库这个不说了吧。。 配置好后，这里先来看看一些jpa的基本注解，我们会用这些注解来写案例：1.@Entity ：修饰实体类，指明该类将映射到指定的数据表，例如：Customer 类默认的数据表名为 customer 2.@Table ：当实体类与映射的数据库表名不同名时需要使用 @Table 注解，该注解与 @Entity 注解并列使用，使用其 name 属性指明数据库的表名@Table(name = “JPA_CUSTOMER表名”) 3.@Id ：标识该属性为主键，一般标注在该属性的 getter 方法上 4.@GeneratedValue ：标注主键的生成策略，通过其 strategy 属性。通常与 @Id 注解一起使用。默认情况下 JPA 会自动选择一个最适合底层数据库的主键生成策略，MyS 5.默认为 AUTO，常用策略有：–IDENTITY：采用数据库 ID自增长的方式来自增主键字段，Oracle 不支持这种方式； –AUTO： JPA自动选择合适的策略，是默认选项； –SEQUENCE：通过序列产生主键，通过 @SequenceGenerator 注解指定序列名，MySql 不支持这种方式 –TABLE：通过表产生主键，框架借由表模拟序列产生主键，使用该策略可以使应用更易于数据库移植 例：@GenerateValue（strategy=GenerationType.AUTO）,写在id属性的上面，主键策略。 6.@Basic ：用于没有任何标注的 getXxx() 方法，默认即为 @Basic,所以若一个 getter 方法无任何注解，可以使用 @Basic 注解，也可以不使用 7.@Column ：当实体的属性与其映射的数据表的列不同名时使用，一般用于 getter 方法上。其 name 属性用来指明此属性在数据表中对应的列名；unique 属性指明是否为唯一约束；nullable 属性用来指明是否可以为空，false 为不能为空；length 属性指明此列的长度。通常写在普通属性上：@Column（name=”LAST_NAME”,length=50,nullable=false）nullable表示不允许为空。 8.@Transient ：标注此注解后在创建数据表的时候将会忽略该属性 Customer 类并没有 info 这个属性，所以数据库中也不应该有 info 这个字段 9.@Temporal ：向数据库映射日期（Date）属性时用来调整映射的精度。Date 类型的数据有 DATE, TIME, 和 TIMESTAMP 三种精度(即单纯的日期,时间,或者两者兼备). Birth 属性应该使用 DATE 类型(生日只具体到日即可，如：2015-10-22)，而 CreateTime 应该使用 TIMESTAMP 类型(创建时间应该具体到秒，如：2017-10-11 22:39:13) 注解讲完后，可以开始进入项目了，项目中我们需要建立实体类。（以User为例）实体类需要建立@Entity注解以标记为实体类，映射到数据库中，自动生成的表名与这个实体类的类名一致主键id需要加@Id和@GeneratedValue(strategy = GenerationType.IDENTITY) 原项目中的@GeneratedValue注解没有加后面的参数，所以为了保证不出错，加上参数，JPA为开发人员提供了四种主键生成策略，被定义在枚举类GenerationType中，包含（TABLE , SEQUENCE , IDENTITY , AUTO）.先介绍下这四种策略： （1）GenerationType.TABLE 使用一个特定的数据库表格来保存主键，持久化引擎通过关系数据库的一张特定的表格来生成主键。 策略的优点：不依赖于外部环境和数据库的具体实现，在不同数据库间可以很容易的进行移植。 缺点：不能充分利用数据库的特性，一般不会优先使用。 （2）GenerationType.SEQUENCE 在某些数据库中，不支持主键自增长，比如Oracle，其提供了一种叫做”系列(sequence)”的机制生成主键。 该策略只要部分数据库（Oracle/PostgreSQL/DB2）支持序列对象，所以该策略一般不应用与其他数据库。 （3）GenerationType.IDENTITY 此种主键生成策略就是通常所说的主键自增长，数据库在插入数据时，会自动给主键赋值，比如Mysql可以在创建表时声明”auto_increment”来指定主键自增长。大部分数据库都提供了该支持。我使用了这种。。。 （4）GenerationType.AUTO 把主键生成策略交给持久化引擎，持久化引擎会根据数据库在以上三种主键生成策略中选择其中一种。因为这种策略比较常用，所以JPA默认的生成策略就是AUTO.这种方式如果数据库中不存在这张表的时候，用它来指定自增方式没有问题，但是如果数据库中已经存在这张表并设计了自动方式，那么插入数据的时候就会报错。 @Entity public class User { @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; public User(){} public User(String name, Integer age) { this.name = name; this.age = age; } //省略get/set } 接下来是DAO：spring data jpa让我们解脱了DAO层的操作，基本上所有CRUD都可以依赖于它来实现，在里面就集成了一种方法就是自动编写SQL，但是如果我们要复杂SQL的时候，jpa也给我们提供了@Query用于我们自己编写sql语句，当然这是jpa特有的编写规范。它们分别实现了按name查询User实体和按name和age查询User实体，可以看到我们这里没有任何类SQL语句就完成了两个条件查询方法。这就是Spring-data-jpa的一大特性：通过解析方法名创建查询。除了通过解析方法名来创建查询外，它也提供通过使用@Query 注解来创建查询，您只需要编写JPQL语句，并通过类似“:name”来映射@Param指定的参数，就像例子中的第三个findUser函数一样。 public interface UserRepository extends JpaRepository&lt;User, Long&gt; { User findByName(String name); User findByNameAndAge(String name, Integer age); //：name是来映射@Param指定的参数，根据传递的参数来作为where查询的条件 //方法的参数个数必须和@Query里面需要的参数个数一致 //默认配置下， 使用了@Query注解后就不会再使用方法名解析的方式了 //这是命名化参数的形式，就是条件具体的指向参数，也可以用问号？索引方式来表达参数 @Query(&quot;from User u where u.name=:name&quot;) User findUser1(@Param(&quot;name&quot;) String name); //根据name和age的条件来查找数据，并返回User实例 @Query（value=&quot;select * from User u where u.name=?1 and u.age=?2&quot;,nativeQuery=true） User findUser2(@Param(&quot;name&quot;)String name,@Param(&quot;age&quot;)Integer age); } JPQL主要用于JPA查询数据，和SQL语句的语法大同小异；JPQL语言，即 Java Persistence Query Language 的简称。JPQL 是一种和 SQL 非常类似的中间性和对象化查询语言，它最终会被编译成针对不同底层数据库的 SQL 查询，从而屏蔽不同数据库的差异。 JPQL语言的语句可以是 select 语句、update 语句或delete语句，它们都通过 Query 接口封装执行。 Query注解自定义SQL查询 1.我们可以直接使用@Query来使用这个类的方法。 默认配置下， 使用了@Query注解后就不会再使用方法名解析的方式了…注意 @Query有几大属性，分别是：方法的参数个数必须和@Query里面需要的参数个数一致value里面的参数也可以用SPEL表达式来写，也就是#{…}语句，用于更灵活的配置参数，不用把sql语句写死。 -String value() default “”;//默认是空，value=sql语句，比如@Query(“from User u where u.name=:name”)就可以写成@Query(value=”from User u where u.name=:name”) -boolean nativeQuery() default false;//默认是false代表原生sql，所谓本地查询，就是使用原生的sql语句（根据数据库的不同，在sql的语法或结构方面可能有所区别）进行查询数据库的操作。调用这个以后就可以针对不同的数据库做对应的sql语法了（就像oracle和mysql一样，虽然大体相同但是还是存在一些小地方的差别） 当不需要表中的全字段时，可自定义dto类来接受查询结果，这种方法要注意使用new + dto类全路径+ （别名.field1, 别名.field2, 别名.field3）, 且dto类中必须有对应参数结构的构造函数！别忘记加上无参的构造函数！ @Query(&quot;select new com.user.domain.UserDto(a.userName, a.gender) from User a where userId = :userId&quot;) UserDto findByUserId(@Param(&quot;userId&quot;) userId); 2.修改操作加上 @Modify注解 @Query(value=&quot;update User set userId = :userId&quot;) @Modify User findByUserId(@Param(&quot;userId&quot;) userId); 3.如果是like（模糊查询），后面的参数需要前面或者后面加”%”，比如下面都对： @Query(&quot;select o from UserModel o where o.name like ?1%&quot;) public List&lt;UserModel&gt; findByUuidOrAge(String name); @Query(&quot;select o from UserModel o where o.name like %?1&quot;) public List&lt;UserModel&gt; findByUuidOrAge(String name); @Query(&quot;select o from UserModel o where o.name like %?1%&quot;) public List&lt;UserModel&gt; findByUuidOrAge(String name); 当然，这样在传递参数值的时候就可以不加’%’了，当然加了也不会错 4.还有个很有意思的，就是这个例子： @Query(value = &quot;select * from Book b where b.name=?1&quot;, nativeQuery = true) List&lt;Book&gt; findByName(String name); //此方法sql将会报错(java.lang.IllegalArgumentException)，看出原因了吗,若没看出来，请看下一个例子 因为指定了nativeQuery = true，即使用原生的sql语句查询。使用java对象’Book’作为表名来查自然是不对的。只需将Book替换为表名book。因为若按数据库方法去查就找不到这张表了。 5.用SPEL表达式 public interface BookQueryRepositoryExample extends Repository&lt;Book, Long&gt;{ @Query(value = &quot;select * from #{#entityName} b where b.name=?1&quot;, nativeQuery = true) List&lt;Book&gt; findByName(String name); }]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>spring-data-jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot+Nginx实现负载均衡]]></title>
    <url>%2F2018%2F04%2F19%2Fspringboot-nginx%2F</url>
    <content type="text"><![CDATA[什么是 NginxNginx 是俄罗斯人编写的十分轻量级的 HTTP 服务器,Nginx，它的发音为“engine X”，是一个高性能的HTTP和反向代理服务器，同时也是一个 IMAP/POP3/SMTP 代理服务器。Nginx 是由俄罗斯人 Igor Sysoev 为俄罗斯访问量第二的 Rambler.ru 站点开发的，它已经在该站点运行超过两年半了。Igor Sysoev 在建立的项目时,使用基于 BSD 许可。 官网：http://nginx.netSimplify your journey to microservices with the new NGINX Application Platform.Nginx 以事件驱动的方式编写，所以有非常好的性能，同时也是一个非常高效的反向代理、负载平衡。其拥有匹配 Lighttpd 的性能，同时还没有 Lighttpd 的内存泄漏问题，而且 Lighttpd 的 mod_proxy 也有一些问题并且很久没有更新。 现在，Igor 将源代码以类 BSD 许可证的形式发布。Nginx 因为它的稳定性、丰富的模块库、灵活的配置和低系统资源的消耗而闻名．业界一致认为它是 Apache2.2＋mod_proxy_balancer 的轻量级代替者，不仅是因为响应静态页面的速度非常快，而且它的模块数量达到 Apache 的近 2/3。对 proxy 和 rewrite 模块的支持很彻底，还支持 mod_fcgi、ssl、vhosts ，适合用来做 mongrel clusters 的前端 HTTP 响应。 Nginx在大型互联网站点中使用度很高，一台nginx服务器的处理性能基于服务器CPU，进程越高其nginx的质量越高。 Nginx 在启动后，在 unix 系统中会以 daemon 的方式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。我们也可以手动地关掉后台模式，让 Nginx 在前台运行，并且通过配置让 Nginx 取消 master 进程，从而可以使 Nginx 以单进程方式运行。很显然，生产环境下我们肯定不会这么做，所以关闭后台模式，一般是用来调试用的，在后面的章节里面，我们会详细地讲解如何调试 Nginx。 Nginx 最初是作为一个 Web 服务器创建的，用于解决 C10k 的问题。作为一个 Web 服务器，它可以以惊人的速度为您的数据服务。但 Nginx 不仅仅是一个 Web 服务器，你还可以将其用作反向代理，与较慢的上游服务器（如：Unicorn 或 Puma）轻松集成。你可以适当地分配流量（负载均衡器）、流媒体、动态调整图像大小、缓存内容等等。 基本的 nginx 体系结构由 master 进程和其 worker 进程组成。master 读取配置文件，并维护 worker 进程，而 worker 则会对请求进行实际处理。 Nginx的内部架构如图所示，当Nginx服务开启的时候，会启动一个master进程，及其多个worker（worker的多少与你搭建的服务器主机的性能有关，更多进程支持的强大CPU当然可以开启更多的worker），master 进程主要用来管理 worker 进程，包含：master接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态，当worker进程退出后(异常情况下)，会自动重新启动新的 worker 进程。而基本的网络事件，则是放在 worker 进程中来处理了。多个 worker 进程之间是对等的，他们同等竞争来自客户端的请求，各进程互相之间是独立的。一个请求，只可能在一个 worker 进程中处理，一个 worker 进程，不可能处理其它进程的请求。worker 进程的个数是可以设置的，一般我们会设置与机器cpu核数一致，这里面的原因与 Nginx 的进程模型以及事件处理模型是分不开的。 master 来管理 worker 进程，所以我们只需要与 master 进程通信就行了。master 进程会接收来自外界发来的信号，再根据信号做不同的事情。 例如：控制Nginx的一般命令有 -kill -HUP pid（这个命令现在较老，用处不大）：告诉 Nginx，从容地重启 Nginx，我们一般用这个信号来重启 Nginx，或重新加载配置，因为是从容地重启，因此服务是不中断的。master 进程在接收到 HUP 信号后是怎么做的呢？首先 master 进程在接到信号后，会先重新加载配置文件，然后再启动新的 worker 进程，并向所有老的 worker 进程发送信号，告诉他们可以光荣退休了。新的 worker 在启动后，就开始接收新的请求，而老的 worker 在收到来自 master 的信号后，就不再接收新的请求，并且在当前进程中的所有未处理完的请求处理完成后，再退出。 ./nginx -s reload：重启 Nginx，新的 Nginx 进程在解析到 reload 参数后，就知道我们的目的是控制 Nginx 来重新加载配置文件了，它会向 master 进程发送信号，然后接下来的动作，就和我们直接向 master 进程发送信号一样了。./nginx -s stop：停止 Nginx 的运行 我们知道了当我们在操作 Nginx 的时候，Nginx 内部做了些什么事情，那么，worker 进程又是如何处理请求的呢？我们前面有提到，worker 进程之间是平等的，每个进程，处理请求的机会也是一样的。当我们提供 80 端口的 http 服务时，一个连接请求过来，每个进程都有可能处理这个连接，怎么做到的呢？首先，每个 worker 进程都是从 master 进程 fork 过来，在 master 进程里面，先建立好需要 listen 的 socket（listenfd）之后，然后再 fork 出多个 worker 进程。所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。当一个 worker 进程在 accept 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接，这样一个完整的请求就是这样的了。我们可以看到，一个请求，完全由 worker 进程来处理，而且只在一个 worker 进程中处理。 Nginx 采用这种进程模型有什么好处呢？当然，好处肯定会很多了。首先，对于每个 worker 进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master 进程则很快启动新的 worker 进程。当然，worker 进程的异常退出，肯定是程序有 bug 了，异常退出，会导致当前 worker 上的所有请求失败，不过不会影响到所有请求，所以降低了风险。 为什么 Nginx 可以采用异步非阻塞的方式来处理呢，或者异步非阻塞到底是怎么回事呢？我们先回到原点，看看一个请求的完整过程。首先，请求过来，要建立连接，然后再接收数据，接收数据后，再发送数据。具体到系统底层，就是读写事件，而当读写事件没有准备好时，必然不可操作，如果不用非阻塞的方式来调用，那就得阻塞调用了，事件没有准备好，那就只能等了，等事件准备好了，你再继续吧。阻塞调用会进入内核等待，cpu 就会让出去给别人用了，对单线程的 worker 来说，显然不合适，当网络事件越多时，大家都在等待呢，cpu 空闲下来没人用，cpu利用率自然上不去了，更别谈高并发了。好吧，你说加进程数，这跟apache的线程模型有什么区别，注意，别增加无谓的上下文切换。所以，在 Nginx 里面，最忌讳阻塞的系统调用了。不要阻塞，那就非阻塞喽。非阻塞就是，事件没有准备好，马上返回 EAGAIN，告诉你，事件还没准备好呢，你慌什么，过会再来吧。好吧，你过一会，再来检查一下事件，直到事件准备好了为止，在这期间，你就可以先去做其它事情，然后再来看看事件好了没。虽然不阻塞了，但你得不时地过来检查一下事件的状态，你可以做更多的事情了，但带来的开销也是不小的。所以，才会有了异步非阻塞的事件处理机制，具体到系统调用就是像 select/poll/epoll/kqueue 这样的系统调用。它们提供了一种机制，让你可以同时监控多个事件，调用他们是阻塞的，但可以设置超时时间，在超时时间之内，如果有事件准备好了，就返回。这种机制正好解决了我们上面的两个问题，拿 epoll 为例(在后面的例子中，我们多以 epoll 为例子，以代表这一类函数)，当事件没准备好时，放到 epoll 里面，事件准备好了，我们就去读写，当读写返回 EAGAIN 时，我们将它再次加入到 epoll 里面。这样，只要有事件准备好了，我们就去处理它，只有当所有事件都没准备好时，才在 epoll 里面等着。这样，我们就可以并发处理大量的并发了，当然，这里的并发请求，是指未处理完的请求，线程只有一个，所以同时能处理的请求当然只有一个了，只是在请求间进行不断地切换而已，切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。与多线程相比，这种事件处理方式是有很大的优势的，不需要创建线程，每个请求占用的内存也很少，没有上下文切换，事件处理非常的轻量级。并发数再多也不会导致无谓的资源浪费（上下文切换）。更多的并发数，只是会占用更多的内存而已。 我之前有对连接数进行过测试，在 24G 内存的机器上，处理的并发请求数达到过 200 万。现在的网络服务器基本都采用这种方式，这也是nginx性能高效的主要原因。 我们之前说过，推荐设置 worker 的个数为 cpu 的核数，在这里就很容易理解了，更多的 worker 数，只会导致进程来竞争 cpu 资源了，从而带来不必要的上下文切换。而且，nginx为了更好的利用多核特性，提供了 cpu 亲缘性的绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来 cache 的失效。像这种小的优化在 Nginx 中非常常见，同时也说明了 Nginx 作者的苦心孤诣。比如，Nginx 在做 4 个字节的字符串比较时，会将 4 个字符转换成一个 int 型，再作比较，以减少 cpu 的指令数等等。 现在，知道了 Nginx 为什么会选择这样的进程模型与事件模型了。对于一个基本的 Web 服务器来说，事件通常有三种类型，网络事件、信号、定时器。从上面的讲解中知道，网络事件通过异步非阻塞可以很好的解决掉。如何处理信号与定时器？ 首先，信号的处理。对 Nginx 来说，有一些特定的信号，代表着特定的意义。信号会中断掉程序当前的运行，在改变状态后，继续执行。如果是系统调用，则可能会导致系统调用的失败，需要重入。关于信号的处理，大家可以学习一些专业书籍，这里不多说。对于 Nginx 来说，如果nginx正在等待事件（epoll_wait 时），如果程序收到信号，在信号处理函数处理完后，epoll_wait 会返回错误，然后程序可再次进入 epoll_wait 调用。 另外，再来看看定时器。由于 epoll_wait 等函数在调用的时候是可以设置一个超时时间的，所以 Nginx 借助这个超时时间来实现定时器。nginx里面的定时器事件是放在一颗维护定时器的红黑树里面，每次在进入 epoll_wait前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait 会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。由此可以看出，当我们写 Nginx 代码时，在处理网络事件的回调函数时，通常做的第一个事情就是判断超时，然后再去处理网络事件。 nginx应用场景： 1、反向代理反向代理应该是Nginx做的最多的一件事了，什么是反向代理呢，以下是百度百科的说法：反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。简单来说就是真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。 在物理层面上就是一般浏览器访问网站是对接到相应的真实服务器上，可能会存在多台服务器做集群来分布式处理，这时候我们若加上Nginx作为反向代理服务器（就是搭建一台主机给Nginx用），本来浏览器发送给真实服务器的请求就会先去到Nginx服务器上，再由Nginx分发请求给不同的真实服务器，这样做，一是为了减少服务端压力（统一处理），二是为了安全（浏览器访问到的服务器只是Nginx的地址而不是真实服务器）。 反向代理的定义：反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。反向代理的基本功能就是 隐藏原始服务器地址。其解决的是安全的问题。但目前的主流反向代理服务器还同时兼职做负载均衡，静态文件缓存和动态请求分发，压缩文件，预防DDoS攻击等功能。这些功能都在一定程度上可以提升服务器负载。 2、http服务器。Nginx是一个http服务可以独立提供http服务及其动静分离。Nginx本身也是一个静态资源的服务器，当只有静态资源的时候，就可以使用Nginx来做服务器，同时现在也很流行动静分离，就可以通过Nginx来实现，首先看看Nginx做静态资源服务器。 -静态资源储存 server { listen 80; server_name localhost; client_max_body_size 1024M; location / { root e:wwwroot; index index.html; } } 这样如果访问http://localhost 就会默认访问到E盘wwwroot目录下面的index.html，如果一个网站只是静态页面的话，那么就可以通过这种方式来实现部署。就是指定url与本地静态资源的关系，当用户发起访问，直接就可以通过地址对应到Nginx服务器下的静态资源，我们可以将网站的静态资源全都放在Nginx服务器下的储存中，可以有效减免服务器压力。 -动静分离（动态页面的动静分离） 一般来说，现在的网站大多都具备交互性，就是动态网站，可以与后台交互，静态网站多数都是一些博客和资料，静态页面是不能随时改动的，静态是一次性写好放在服务器上进行浏览的，如果想改动，必须在页面上修改，然后再上传服务器覆盖原来的页面，这样才能更新信息，比较麻烦，使用者不能随时修改。 动态页面是可以随时改变内容的，有前后台之分，管理员可以在后台随时更新网站的内容，前台页面的内容也会随之更新，比较简单易学。 动静分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路： upstream test{ server localhost:8080; server localhost:8081; } server { listen 80; server_name localhost; location / { root e:wwwroot; index index.html; } // 所有静态请求都由nginx处理，存放目录为html，静态资源和Nginx是同一台服务器，当然也可以不同 location ~ .(gif|jpg|jpeg|png|bmp|swf|css|js)$ { root e:wwwroot; //指的是e盘符下的wwwroot目录 } // 所有动态请求都转发给tomcat处理，jsp动态网页和.do请求 location ~ .(jsp|do)$ { proxy_pass http://test; } //错误的页面也可以定义为静态资源页 error_page 500 502 503 504 /50x.html; location = /50x.html { root e:wwwroot; } } 这样我们就可以吧HTML以及图片和css以及js放到wwwroot目录下，而tomcat只负责处理jsp和请求，例如当我们后缀为gif的时候，Nginx默认会从wwwroot获取到当前请求的动态图文件返回，当然这里的静态文件跟Nginx是同一台服务器，我们也可以在另外一台服务器，然后通过反向代理和负载均衡配置过去就好了，只要搞清楚了最基本的流程，很多配置就很简单了，另外localtion后面其实是一个正则表达式，所以非常灵活 3.负载均衡负载均衡也是Nginx常用的一个功能，负载均衡其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。简单而言就是当有2台或以上服务器时，根据规则随机的将请求分发到指定的服务器上处理，负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。而Nginx目前支持自带3种负载均衡策略，还有2种常用的第三方策略。 （1）RR（默认）即Round-Robin,轮询调度默认的负载均衡策略，当我们项目中的服务器有多台的时候，通过Nginx内部机制将这些请求进行随机分发而不是统统将所有的客户端请求都交给一台服务器去处理，这样也充当着一个请求管理者的作用， 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 简单配置： upstream test { server localhost:8080; server localhost:8081; } server { listen 81; server_name localhost; client_max_body_size 1024M; location / { proxy_pass http://test; proxy_set_header Host $host:$server_port; } } 负载均衡的核心代码为： upstream test { server localhost:8080; server localhost:8081; } 端口8080和8081分别代表两台不同的服务器，但是我们访问http://localhost 的时候,也不会有问题，会默认跳转到http://localhost:8080 具体是因为Nginx会自动判断服务器的状态，如果服务器处于不能访问（服务器挂了），就不会跳转到这台服务器，所以也避免了一台服务器挂了影响使用的情况，由于Nginx默认是RR策略，所以我们不需要其他更多的设置。 （2）权重,基于RR可以设置轮询哪个服务器的几率，weight和访问比率成正比，用于后端服务器性能不均的情况。例如项目中使用的服务器集群中存在性能差异，为了避免性能分配不均和利用率，我们可以设置RR权重。 例如： upstream test { server localhost:8080 weight=9; server localhost:8081 weight=1; } 那么10次一般只会有1次会访问到8081，而有9次会访问到8080 （3）ip_hash(见名知意，就是根据浏览器IP的Hash值) 上面的2种方式都有一个问题，那就是下一个请求来的时候请求可能分发到另外一个服务器，当我们的程序不是无状态的时候（采用了session保存数据），这时候就有一个很大的很问题了，比如把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录了，所以很多时候我们需要一个客户只访问一个服务器，那么就需要用iphash了，iphash的每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 如果利用Redis保存全局Session还会用到这个功能吗？ upstream test { ip_hash; server localhost:8080; server localhost:8081; } （4）fair（第三方）按后端服务器的响应时间来分配请求，响应时间短的优先分配。（可以根据减少服务器不必要时间的浪费） upstream backend { fair; server localhost:8080; server localhost:8081; } （5）url_hash（第三方）按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。 upstream backend { hash $request_uri; hash_method crc32; server localhost:8080; server localhost:8081; } 以上5种负载均衡各自适用不同情况下使用，所以可以根据实际情况选择使用哪种策略模式,不过fair和url_hash需要安装第三方模块才能使用。 4.正向代理 正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。当你需要把你的服务器作为代理服务器的时候，可以用Nginx来实现正向代理，但是目前Nginx有一个问题，那么就是不支持HTTPS，虽然我百度到过配置HTTPS的正向代理，但是到最后发现还是代理不了，当然可能是我配置的不对，所以也希望有知道正确方法的同志们留言说明一下。 resolver 114.114.114.114 8.8.8.8; server { resolver_timeout 5s; listen 81; access_log e:wwwrootproxy.access.log; error_log e:wwwrootproxy.error.log; location / { proxy_pass http://$host$request_uri; } } resolver是配置正向代理的DNS服务器，listen 是正向代理的端口，配置好了就可以在ie上面或者其他代理插件上面使用服务器ip+端口号进行代理了。 Nginx支持热启动Nginx是支持热启动的，也就是说当我们修改配置文件后，不用关闭Nginx，就可以实现让配置生效，当然我并不知道多少人知道这个，反正我一开始并不知道，导致经常杀死了Nginx线程再来启动。。。Nginx从新读取配置的命令是 即nginx -s reload windows下面就是：nginx.exe -s reload]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重返SpringBoot]]></title>
    <url>%2F2018%2F04%2F19%2Freturn-springboot%2F</url>
    <content type="text"><![CDATA[SpringBoot作为一些常用开发框架的集合（可以成为脚手架），其自身的自动化配置给我们的开发带来质的改变，不必去担心繁琐的XML配置（反正我到现在都没有去配什么XML文件），用一个application.properties文件就可以在日常开发中完全适用，我们通过各种功能性示例体验了Spring Boot的自动化配置给我们所带来的超便利的新开发方式。 但是SpringBoot的自动化配置也带入些许隐患(优&gt;&gt;&gt;弊) jar包冲突项目依赖复杂的情况下，由于依赖方的依赖组织不够严格，可能引入了一些实际我们不需要的依赖，从而导致我们的项目满足一些特定的自动化配置。（自动配置过剩问题，我遇到了很多这种问题，比如jpa与redis的冲突）传统Spring项目转换为Spring Boot项目的过程中，由于不同的组织方式问题，引发自动化配置加载的错误，比如：通过xml手工组织的多数据源配置等。(升级框架时会遇到各种兼容问题，对升级框架不太友好) 上面这些原因都会导致不必要的自动化配置加载而导致应用无法启动或触发/health的健康检查不通过等问题。 若是遇到jar包兼容问题，则是通过外部依赖的修改来解决：通过与依赖方沟通，在对方提供的API依赖中去掉不必要的依赖通过禁用指定的自动化配置来避免加载不必要的自动化配置，下面列举了禁用的方法： -使用了@EnableAutoConfiguration@EnableAutoConfiguration(exclude={DataSourceAutoConfiguration.class})禁止了DataSource自动加载的类 -使用了@SpringBootApplication@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class}) -通过配置文件来设置spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration 日志框架冲突jar包冲突中比较常见的是关于springboot本身的日志配置 Exception in thread “main” java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation 这个异常是由于打印日志的jar冲突导致，SpringBoot本身有打印日志的功能（springboot默认使用Commons Logging），如果跟本地的冲突，就需要去掉再调用其他的，如下 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;1.3.3.RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; springboot版本不兼容问题]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot+log4j日志管理]]></title>
    <url>%2F2018%2F04%2F16%2Fspring-logback%2F</url>
    <content type="text"><![CDATA[关于日志Spring Boot在所有内部日志中使用Commons Logging，但是默认配置也提供了对常用日志的支持，如：Java Util Logging，Log4J, Log4J2和Logback。每种Logger都可以通过配置使用控制台或者文件输出日志内容。Springboot帮我们整合了这些最常用的日志框架，只需要简单的按照约定配置即可在项目中很快的配置日志相关。 常用的日志框架(也称为日志门面库)有apache commons logging和slf4j，常用的日志系统(也称为日志实现库)有log4j,log4j2,JUL,logback等。日志框作为门面，日志系统要搭配日志框架来使用，日志框架调用日志系统，在分布式中，还会有专门的分布式日志收集系统，在集群的多个分布式机器中将日志收集并整理的框架有：apache flume，facebook scribe等。早期Java项目使用最多的日志门面是commons-logging，log4j是推荐的日志实现库，需要的jar包为commons-logging.jar、log4j.jar。 springboot中默认使用的也是commons logging+logback的组合，我们需要排除并且导入log4j的jar包 现今java项目推荐的日志门面是slf4j，log4j仍是推荐的日志实现库，需要的jar包为slf4j-api.jar、slf4j-log4j12.jar、log4j.jar，其中绑定包slf4j-log4j12.jar指定了要使用的实现库。 commons-logging存在osgi问题，但早期项目多使用其作为日志门面，为保证兼容仍使用其作为日志门面，但通过slf4j的静态绑定技术来加载具体的日志库log4j，需要的jar包为commons-logging.jar、jcl-over-slf4j.jar、slf4j-api.jar、slf4j-log4j12.jar、log4j.jar，其中jcl-over-slf4j.jar将日志的接口重定向到slf4j。 slf4j+log4j组合使用模式： slf4j-api-1.5.11.jar slf4j-log4j12-1.5.11.jar log4j-1.2.15.jar log4j.properties(也可以是 log4j.xml) JCL+Log4J组合使用模式（即commons-logging+log4j）： commons-logging-1.1.jar log4j-1.2.15.jar log4j.properties 不同的获取logger的方式 log4j：（单独使用）import org.apache.log4j.Logger;Logger logger= Logger.getLogger(xx.class); slf4j+log4j：（借助LogFactory）import org.slf4j.Logger;import org.slf4j.LoggerFactory;Logger logger = LoggerFactory.getLogger(xx.class); jcl+log4j:（借助LogFactory）import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;private static Log log = LogFactory.getLog(xx.class); 可以参考：https://blog.csdn.net/xydds/article/details/51606010 根据使用场景的不同我们采用不同的日志框架+日志系统。 总的来说，slf4j与commons-logging只是一个日志门面，实际还是要依赖真正的日志库log4j，虽然slf4j和commons-loggins自带了日志库，但是毕竟log4j才是日志系统（主要功能）。 现今java项目推荐的日志门面是slf4j，log4j仍是推荐的日志实现库，需要的jar包为slf4j-api.jar、slf4j-log4j12.jar、log4j.jar，其中绑定包slf4j-log4j12.jar指定了要使用的实现库。commons-logging存在osgi问题，但早期项目多使用其作为日志门面，为保证兼容仍使用其作为日志门面，但通过slf4j的静态绑定技术来加载具体的日志库log4j，需要的jar包为commons-logging.jar、jcl-over-slf4j.jar、slf4j-api.jar、slf4j-log4j12.jar、log4j.jar，其中jcl-over-slf4j.jar将日志的接口重定向到slf4j。 在日志系统中，主要分为三个配置，appenders，layouts和loggers，一般我们做项目时日志记录的历程是这样的，loggers负责记录事假，将事件记录到后转发给合适的appenders，然后appenders使用layouts将事件记录进行格式化，并将其发送给控制台，文件或其他目录位置。 在强调可重用组件开发的今天，除了自己从头到尾开发一个可重用的日志操作类外， Apache 为我们提供了一个强有力的日志操作包 -Log4j 。 Log4j 是 Apache 的一个开放源代码项目，通过使用 Log4j ，我们可以控制日志信息输送的目的地是控制台、文件、 GUI 组件、甚至是套接口(socket)服务器、 NT 的事件记录器、 UNIX Syslog 守护进程等；我们也可以控制每一条日志的输出格式；通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。最令人感兴趣的就 是，这些可以通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。 此外，通过 Log4j 其他语言接口，您可以在 C 、 C+ + 、 .Net 、 PL/SQL 程序中使用 Log4j ，其语法和用法与在 Java 程序中一样，使得多语言分布式系统得到一个统一一致的日志组件模块。而且，通 过使用各种第三方扩展，您可以很方便地将 Log4j 集成到 J2EE 、 JINI 甚至是 SNMP 应用中。 核心理解为：一个logger可以对应多个appender，一个appender只能对应一个layout。因为logger捕抓到消息后，可以在各种地方输出（控制台，文件，硬盘甚至数据库），一个输出只能对应一种输出格式。 1.appenders负责输出到目标位置，日志输出目的地，负责日志的输出 （输出到什么 地方）2.layouts负责日志信息的格式化，以什么形式展现3.loggers负责捕抓事件给相应的appenders，如何处理日志 Log4j提供的layout有以下几种（一般在工作中常用）org.apache.log4j.HTMLLayout（以HTML表格形式布局），org.apache.log4j.PatternLayout（可以灵活地指定布局模式），org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串），org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息） 常用的集中appender有：（一般在工作中常用，也可以支持自定义appender）org.apache.log4j.ConsoleAppender（控制台）org.apache.log4j.FileAppender（文件）org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件）org.apache.log4j.RollingFileAppender（（文件大小到达指定尺寸的时候产生一个新的文件）org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方） 例如：默认的日志输出如下：2016-04-13 08:23:50.120 INFO 37397 — [ main] org.hibernate.Version : HHH000412: Hibernate Core {4.3.11.Final} 输出内容元素具体如下：（包括这些） 时间日期 — 精确到毫秒日志级别 — ERROR, WARN, INFO, DEBUG or TRACE进程ID分隔符 — — 标识实际日志的开始线程名 — 方括号括起来（可能会截断控制台输出）Logger名 — 通常使用源代码的类名日志内容 级别在Spring Boot中默认配置了ERROR、WARN和INFO级别的日志输出到控制台。 这里要说明一下日志的级别：（共分为5级），级别也就是反映出一种status的意思，我们根据关键字可以看出程序运行后记录的状态，看这个级别参数，知道是错误，警告还是正确运行。 level为logger服务，用来定义日志的级别，他的值可以是： OFF（关闭）FATAL（致命的） ERROR（错误）WARN（警告） INFO（信息）DEBUG （调试） ALL（所有），输出日志的策略由此Level决定，例如：如果logger的Level设置为INFO，那么系统只输出INFO以及以上（WARN、ERROR、FATAL）信息。如果当前logger没有设定Level，那么它在输出日志时采用的策略是：它会使用它继承的Logger的Level作为它自己的Level来处理。如果它的上级也没有设置Level，那么就找上上级，几次类推，直到root为止，（root的Level是不能设为空的，所以最终一定会找到一个Level）。默认root的Level是INFO，其他logger的Level默认都是null，需要手动指定。 【level】日志输出级别（由小到大，级别最高为DEBUG） FATAL 0 ERROR 3 严重错误 WARN 4 一般警告 INFO 6 一般显示信息 DEBUG 7 程序调试信息 可以设置级别： debug&gt;info&gt;errordebug ：显示 debug 、 info 、 errorinfo ：显示 info 、 errorerror ：只 error也就是说只显示比大于等于当前级别的信息 如果我们不指定appenders输出到哪个文件，那么springboot是默认将信息输入到控制台的，ERROR是产生严重错误的级别，这个在我们产生错误导致程序不运行时显示的，WARN则是警告的情况，INFO则是正常信息的输出，程序正常运行。我们可以通过两种方式切换至DEBUG级别： 在运行命令后加入–debug标志，如：$ java -jar myapp.jar –debug在application.properties中配置debug=true，该属性置为true的时候，核心Logger（包含嵌入式容器、hibernate、spring）会输出更多内容，但是你自己应用的日志并不会输出为DEBUG级别。 这是springboot启动的三种方式：第一种是cmd到项目目录然后命令行mvn spring-boot:run启动项目， 第二种是运行Application.java入口类方法， 第三种是使用mvn install 生成jar后运行，先到项目根目录mvn installcd targetjava -jar xxxx.jar 上诉改debug也是通过在项目目录下输入指令来设置debug开启。。。 在Spring Boot中只需要在application.properties文件中进行配置完成日志记录的级别控制。配置格式：logging.level.=LEVELlogging.level：日志级别控制前缀，为包名或Logger名LEVEL：选项TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF 举例：logging.level.com.didispace=DEBUG：com.didispace包下所有class以DEBUG级别输出logging.level.root=WARN：root日志以WARN级别输出 其中， level 是日志记录的优先级，分为 OFF 、 FATAL 、 ERROR 、 WARN 、 INFO 、 DEBUG 、 ALL 或者您定义的级别。 Log4j 建议只使用四个级别 ，优先级从高到低分别是 ERROR 、 WARN 、 INFO 、 DEBUG 。通过在这里定义的级别，您可以控制到应用程序中相应级别的日志信息的开关。比如在这里定 义了 INFO 级别，则应用程序中所有 DEBUG 级别的日志信息将不被打印出来 。 控制台输出控制台选项Threshold=DEBUG:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。Target=System.err：默认情况下是：System.out,指定输出控制台FileAppender 选项Threshold=DEBUF:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。RollingFileAppender 选项Threshold=DEBUG:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。MaxFileSize=100KB: 后缀可以是KB, MB 或者是 GB. 在日志文件到达该大小时，将会自动滚动，即将原来的内容移到mylog.log.1文件。MaxBackupIndex=2:指定可以产生的滚动文件的最大数。 文件输出我们日常工作中经常要进行日志记录，并输出到响应位置的文件夹，甚至是mongoDB数据库（文件储存数据库）中，用于存放数据，我们要怎么用springboot内集成的框架来完成日志记录呢？ Spring Boot默认配置只会输出到控制台，并不会记录到文件中，但是我们通常生产环境使用时都需要以文件方式记录。 若要增加文件输出，需要在application.properties中配置logging.file或logging.path属性。 logging.file，设置文件，可以是绝对路径，也可以是相对路径。如：logging.file=my.loglogging.path，设置目录，会在该目录下创建spring.log文件，并写入日志内容，如：logging.path=/var/log 日志文件会在10Mb大小的时候被截断，产生新的日志文件，默认级别为：ERROR、WARN、INFO也就是说每产生一个10mb大小的日志就会新建一个新的日志用于记录。 自定义输出日志的格式在Spring Boot中可以通过在application.properties配置如下参数控制输出格式： logging.pattern.console：定义输出到控制台的样式（不支持JDK Logger）logging.pattern.file：定义输出到文件的样式（不支持JDK Logger） 一般来说是在日志的自定义文件中配置的： 如果使用pattern布局就要指定的打印信息的具体格式ConversionPattern，打印参数如下：日志信息格式中几个符号所代表的含义：-X号: X信息输出时左对齐；%p: 输出日志信息优先级，即DEBUG，INFO，WARN，ERROR，FATAL,%d: 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyy MMM dd HH:mm:ss,SSS}，输出类似：2002年10月18日 22：10：28，921%r: 输出自应用启动到输出该log信息耗费的毫秒数%c: 输出日志信息所属的类目，通常就是所在类的全名%t: 输出产生该日志事件的线程名%l: 输出日志事件的发生位置，相当于%C.%M(%F:%L)的组合,包括类目名、发生的线程，以及在代码中的行数。举例：Testlog4.main (TestLog4.Java:10)%x: 输出和当前线程相关联的NDC(嵌套诊断环境),尤其用到像Java servlets这样的多客户多线程的应用中。%%: 输出一个”%”字符%F: 输出日志消息产生时所在的文件名称%L: 输出代码中的行号%m: 输出代码中指定的消息,产生的日志具体信息%n: 输出一个回车换行符，Windows平台为”\r\n”，Unix平台为”\n”输出日志信息换行可以在%与模式字符之间加上修饰符来控制其最小宽度、最大宽度、和文本的对齐方式。如：1)%20c：指定输出category的名称，最小的宽度是20，如果category的名称小于20的话，默认的情况下右对齐。2)%-20c:指定输出category的名称，最小的宽度是20，如果category的名称小于20的话，”-“号指定左对齐。3)%.30c:指定输出category的名称，最大的宽度是30，如果category的名称大于30的话，就会将左边多出的字符截掉，但小于30的话也不会有空格。4)%20.30c:如果category的名称小于20就补空格，并且右对齐，如果其名称长于30字符，就从左边较远输出的字符截掉。 比如：%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n 的格式意思就是说： 时间{指定时间格式} （%p就是表INFO） 所在类（%c{1}只表示类名不是全限定名）：显示调用logger的代码行 - 指定消息及回车（一般最后都会加一个%n） 具体规范参考：https://blog.csdn.net/reserved_person/article/details/52849505 http://logging.apache.org/ 自定义日志文件配置由于日志服务一般都在ApplicationContext创建前就初始化了，它并不是必须通过Spring的配置文件控制。因此通过系统属性和传统的Spring Boot外部配置文件依然可以很好的支持日志控制和管理。 根据不同的日志系统，你可以按如下规则组织配置文件名，就能被正确加载： Logback：logback-spring.xml, logback-spring.groovy, logback.xml, logback.groovyLog4j：log4j-spring.properties, log4j-spring.xml, log4j.properties, log4j.xmlLog4j2：log4j2-spring.xml, log4j2.xmlJUL (Java Util Logging)：logging.properties Spring Boot官方推荐优先使用带有-spring的文件名作为你的日志配置（如使用logback-spring.xml，而不是logback.xml） 讲讲springboot+log4j在POM中引入： &lt;!-- 如果要使用Log4j来记录，要先把springboot默认的logback给排除掉 本身 spring-boot-starter就是包含在spring-boot-starter-web中的，我们 要把它重新声明出来去排除spring-boot-starter-logging（默认的Commons Logging日志框架） 改用SLF4j日志框架来记录--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- 引入log4j的相关jar包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt; &lt;version&gt;1.3.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; 新建log4j.properties文件，是log4j的自定义配置文件 例： //root是控制整个工程的日志，rootLogger就是根记录器级别，配置根Logger // 此句为将等级为INFO的日志信息输出到stdout和file这两个目的地， // console和file的定义在下面的代码，可以任意起名。 // 等级可分为OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL， // 如果配置OFF则不打出任何信息，如果配置为INFO这样只显示INFO, WARN, ERROR的log信息， // 而DEBUG信息不会被显示，具体讲解可参照第三部分定义配置文件中的logger。 // 弃用：log4j.rootCategory=[level],appenderName1,appenderName2... // 改用：log4j.rootLogger = [ level ] , appenderName1 , appenderName2 , … // 级别后面写的就是各appender的名称 // 需要分别配置 //但是根据最新版本的lig4j来看，log4j.rootCategory仿佛已经被弃用了，取而代之的是rootLogger //这个类拓展了Category这个类，即生成Category对象时同样也生成一个LOgger，所以，建议用rootLogger //log4j.rootCategory=INFO, console, file log4j.rootLogger=INFO, console, file // 控制台输出 log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.layout=org.apache.log4j.PatternLayout log4j.appender.console.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n // root日志输出 // 输出到logs/all.log文件中 log4j.appender.file=org.apache.log4j.DailyRollingFileAppender log4j.appender.file.file=D:\\logs\\all.log log4j.appender.file.DatePattern=&apos;.&apos;yyyy-MM-dd log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n // 出来上述的全局输出以外，还有工作中常用的分类输出 // 即可以定义一个具体的包的日志输出 // 假设在com.mybatis包下设置日志输出 //如果是rootCategory，则是控制整个工程 log4j.category.com.mybatis=DEBUG,didifile log4j.appender.didifile=org.apache.log4j.DailyRollingFileAppender log4j.appender.didifile.file=D:\\logs\\my.log log4j.appender.didifile.DatePattern=&apos;.&apos;yyyy-MM-dd log4j.appender.didifile.layout=org.apache.log4j.PatternLayout log4j.appender.didifile.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L ---- %m%n // 除了对包分类，还可以针对级别进行分类 // 比如发生了ERROR或者WARN可以输出到不同的文件中 //这个算是专门输出异常的文件 log4j.logger.error=errorfile // error日志输出 log4j.appender.errorfile=org.apache.log4j.DailyRollingFileAppender log4j.appender.errorfile.file=D:\\logs\\error.log log4j.appender.errorfile.DatePattern=&apos;.&apos;yyyy-MM-dd log4j.appender.errorfile.Threshold = ERROR ## 输出ERROR级别以上的日志！！ log4j.appender.errorfile.layout=org.apache.log4j.PatternLayout log4j.appender.errorfile.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n 下面是一段log4j官方对于logger与category的区别：原话是：public class Categoryextends java.lang.Objectimplements AppenderAttachableThis class has been deprecated and replaced by the Logger subclass. It will be kept around to preserve backward compatibility until mid 2003.Logger is a subclass of Category, i.e. it extends Category. In other words, a logger is a category. Thus, all operations that can be performed on a category can be performed on a logger. Internally, whenever log4j is asked to produce a Category object, it will instead produce a Logger object. Log4j 1.2 will never produce Category objects but only Logger instances. In order to preserve backward compatibility, methods that previously accepted category objects still continue to accept category objects.For example, the following are all legal and will work as expected.// Deprecated form: Category cat = Category.getInstance(“foo.bar”) // Preferred form for retrieving loggers: Logger logger = Logger.getLogger(“foo.bar”) The first form is deprecated and should be avoided.There is absolutely no need for new client code to use or refer to the Category class. Whenever possible, please avoid referring to it or using it.See the short manual for an introduction on this class.See the document entitled preparing for log4j 1.3 for a more detailed discussion. Author:Ceki Gülcü, Anders Kristensen 足以解释这个问题了，logger代替了category这个类的功能，并且比其更加强大和好用。 //弃用形式：Category cat = Category.getInstance（“foo.bar”）//用于检索记录器的首选表单：Logger logger = Logger.getLogger（“foo.bar”）第一种形式已被弃用，应该避免。绝对不需要新的客户端代码来使用或引用Category类。只要有可能，请避免提及或使用它。 下面我们再举一个LOG4J日志配置的例子，将这个日志同时在控制台，文件，回滚文件，发送日志邮件，输出到数据库日志表并且自定义标签等全套功能配置： LOG4J 的配置之简单使它遍及于越来越多的应用中了： Log4J 配置文件实现了输出到控制台、文件、回滚文件、发送日志邮件、输出到数据库日志表、自定义标签等全套功能。择其一二使用就够用了， //设置级别和多个目的地 log4j.rootLogger=DEBUG,CONSOLE,A1,im log4j.addivity.org.apache=true //应用于控制台 log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.Threshold=DEBUG log4j.appender.CONSOLE.Target=System.out log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n //log4j.appender.CONSOLE.layout.ConversionPattern=[start]%d{DATE}[DATE]%n%p[PRIORITY]%n%x[NDC]%n%t//[THREAD] n%c[CATEGORY]%n%m[MESSAGE]%n%n // 应用于文件 log4j.appender.FILE=org.apache.log4j.FileAppender log4j.appender.FILE.File=file.log log4j.appender.FILE.Append=false log4j.appender.FILE.layout=org.apache.log4j.PatternLayout log4j.appender.FILE.layout.ConversionPattern=[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n // Use this layout for LogFactor 5 analysis // 应用于文件回滚 log4j.appender.ROLLING_FILE=org.apache.log4j.RollingFileAppender log4j.appender.ROLLING_FILE.Threshold=ERROR log4j.appender.ROLLING_FILE.File=rolling.log // 文件位置 , 也可以用变量 ${java.home} 、 rolling.log log4j.appender.ROLLING_FILE.Append=true //true: 添加 false: 覆盖 log4j.appender.ROLLING_FILE.MaxFileSize=10KB // 文件最大尺寸 log4j.appender.ROLLING_FILE.MaxBackupIndex=1 // 备份数 log4j.appender.ROLLING_FILE.layout=org.apache.log4j.PatternLayout log4j.appender.ROLLING_FILE.layout.ConversionPattern=[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n // 应用于 socket log4j.appender.SOCKET=org.apache.log4j.RollingFileAppender log4j.appender.SOCKET.RemoteHost=localhost log4j.appender.SOCKET.Port=5001 log4j.appender.SOCKET.LocationInfo=true // Set up for Log Facter 5 log4j.appender.SOCKET.layout=org.apache.log4j.PatternLayout log4j.appender.SOCET.layout.ConversionPattern=[start]%d{DATE}[DATE]%n%p[PRIORITY]%n%x[NDC]%n%t[THREAD]%n%c[CATEGORY]%n%m[MESSAGE]%n%n // Log Factor 5 Appender log4j.appender.LF5_APPENDER=org.apache.log4j.lf5.LF5Appender log4j.appender.LF5_APPENDER.MaxNumberOfRecords=2000 // 发送日志给邮件 log4j.appender.MAIL=org.apache.log4j.net.SMTPAppender log4j.appender.MAIL.Threshold=FATAL log4j.appender.MAIL.BufferSize=10 www.wuset.com “&gt;log4j.appender.MAIL.From=web@www.wuset.com log4j.appender.MAIL.SMTPHost=www.wusetu.com log4j.appender.MAIL.Subject=Log4J Message www.wusetu.com “&gt;log4j.appender.MAIL.To=web@www.wusetu.com log4j.appender.MAIL.layout=org.apache.log4j.PatternLayout log4j.appender.MAIL.layout.ConversionPattern=[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n // 用于数据库 log4j.appender.DATABASE=org.apache.log4j.jdbc.JDBCAppender log4j.appender.DATABASE.URL=jdbc:mysql://localhost:3306/test log4j.appender.DATABASE.driver=com.mysql.jdbc.Driver log4j.appender.DATABASE.user=root log4j.appender.DATABASE.password=294823013 log4j.appender.DATABASE.sql=INSERT INTO LOG4J (Message) VALUES (‘[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n’) log4j.appender.DATABASE.layout=org.apache.log4j.PatternLayout log4j.appender.DATABASE.layout.ConversionPattern=[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n log4j.appender.A1=org.apache.log4j.DailyRollingFileAppender log4j.appender.A1.File=SampleMessages.log4j log4j.appender.A1.DatePattern=yyyyMMdd-HH’.log4j’ log4j.appender.A1.layout=org.apache.log4j.xml.XMLLayout // 自定义 Appender log4j.appender.im = net.cybercorlin.util.logger.appender.IMAppender log4j.appender.im.host = mail.cybercorlin.net log4j.appender.im.username = username log4j.appender.im.password = password log4j.appender.im.recipient = corlin@cybercorlin.net log4j.appender.im.layout=org.apache.log4j.PatternLayout log4j.appender.im.layout.ConversionPattern =[framework] %d - %c -%-4r [%t] %-5p %c %x - %m%n 在代码中使用Logger以上说了那么多的日志配置，那么当我们配置好后，如何去使用呢？ 得到记录器使用 Log4j ，第一步就是获取日志记录器，这个记录器将负责控制日志信息。其语法为：public static Logger getLogger( String name)通过指定的名字获得记录器，如果必要的话，则为这个名字创建一个新的记录器。 Name 一般取本类的名字，比如：static Logger logger = Logger.getLogger ( ServerWithLog4j.class.getName () ) 这里注意，看你需要的日志框架是什么来确定你获取Logger对象的方式： // slf4j接口使用 // getLogger内调用slf4j-log4j.jar的StaticLoggerBinder类的getLogger，获取对应的log4j private static final Logger logger = LoggerFactory.getLogger(LogTest.class); // commons-logging接口使用 // 与slf4j基本一样，一个用Logger，一个用Log private static final Log logger = LoggerFactory.getLogger(LogTest.class); 读取配置文件当获得了日志记录器之后，第二步将配置 Log4j 环境，其语法为：BasicConfigurator.configure () ： 自动快速地使用缺省 Log4j 环境。PropertyConfigurator.configure ( String configFilename) ：读取使用 Java 的特性文件编写的配置文件。DOMConfigurator.configure ( String filename ) ：读取 XML 形式的配置文件。 插入记录信息（格式化日志信息）当上两个必要步骤执行完毕，您就可以轻松地使用不同优先级别的日志记录语句插入到您想记录日志的任何地方，其语法如下：Logger.debug ( Object message ) ;Logger.info ( Object message ) ;Logger.warn ( Object message ) ;Logger.error ( Object message ) ; 例如： //可以作为基类logger，需要的类可以直接继承 //似乎只需要声明其Logger对象就可以使用了。。。 public abstract class HelloDao { private static Logger logger = Logger.getLogger(HelloDao.class); public static void main(String[] args) { // 记录debug级别的信息 logger.debug(&quot;This is debug message from Dao.&quot;); // 记录info级别的信息 logger.info(&quot;This is info message from Dao.&quot;); // 记录error级别的信息 logger.error(&quot;This is error message from Dao.&quot;); } } 如果这个类作为基类，如J2EE中的BaseDao、BaseAction、BaseService等等，则我们可以将各层的日志信息分类输出到各个文件。 为了提高效率，我们可以在写日志前增加判断： // 记录debug级别的信息 if (logger.isDebugEnabled()) { logger.debug(&quot;This is debug message from Dao.&quot;); } // 记录info级别的信息 if (logger.isInfoEnabled()) { logger.info(&quot;This is info message from Dao.&quot;); } // 记录error级别的信息 logger.error(&quot;This is error message from Dao.&quot;); 具体完整的配置可以参考：https://blog.csdn.net/anlina_1984/article/details/5313023 http://www.iteye.com/topic/378077 Spring Boot中对log4j进行多环境不同日志级别的控制根据开发-测试-生产阶段的多环境log配置，SpringBoot也为此量身打造了基于多环境的级别控制 开发-测试-生产环境的级别控制 创建多环境配置文件application-dev.properties：开发环境application-test.properties：测试环境application-prod.properties：生产环境application.properties中添加属性：spring.profiles.active=dev（默认激活application-dev.properties配置）application-dev.properties和application-test.properties配置文件中添加日志级别定义：logging.level.com.didispace=DEBUGapplication-prod.properties配置文件中添加日志级别定义：logging.level.com.didispace=INFO 通过上面的定义，根据logging.level.com.didispace在不同环境的配置文件中定义了不同的级别，但是我们已经把日志交给了log4j管理，看看我们log4j.properties中对com.didispace包下的日志定义是这样的，固定定义了DEBUG级别，并输出到名为didifile定义的appender中。 // LOG4J配置 log4j.category.com.didispace=DEBUG, didifile 那么，要如何动态的改变这个DEBUG级别呢？用到参数调用，用SPEL表达式来实现动态替换 // 动态LOG4J配置 log4j.category.com.didispace=${logging.level.com.didispace}, didifile 对于不同环境的使用人员也不需要改变代码或打包文件，只需要通过执行命令中参加参数即可，比如我想采用生产环境的级别，那么我可以这样运行应用：更改对应环境配置文件即可，而不是去大费周章的修改代码了。 java -jar xxx.jar --spring.profiles.active=prod Springboot使用slf4j+logback我们添加spring-boot-starter-web这个包后，spring-boot-starter-web依赖spring-boot-starter，而spring-boot-starter其内部有一个jar包是spring-boot-starter-logging包含的就是slf4j-api.jar和logback-core.jar。 所以说我们若决定用slf4j+logback的组合，那么就添加这个spring-boot-starter-web包即可。 我们在日志冲突中若决定使用其他日志框架或其他日志系统的时候，只需要更改jar包即可，若是我们想采用log4j的话，那我们就需要把这个spring-boot-starter-logging给exclusion掉才能使用。比如我们要使用slf4j+log4j的组合的话，那么要去掉spring-boot-starter-logging，添加spring-boot-starter-log4j，里面包含了slf4j-api+log4j的相关jar包。 无论从设计上还是实现上，Logback相对log4j而言有了相对多的改进。不过尽管难以一一细数，这里还是列举部分理由为什么选择logback而不是log4j。牢记logback与log4j在概念上面是很相似的，它们都是有同一群开发者建立。所以如果你已经对log4j很熟悉，你也可以很快上手logback。如果你喜欢使用log4j,你也许会迷上使用logback。 基于我们先前在log4j上的工作，logback 重写了内部的实现，在某些特定的场景上面，甚至可以比之前的速度快上10倍。在保证logback的组件更加快速的同时，同时所需的内存更加少。 Logback 历经了几年，数不清小时数的测试。尽管log4j也是测试过的，但是Logback的测试更加充分，跟log4j不在同一个级别。我们认为，这正是人们选择Logback而不是log4j的最重要的原因。人们都希望即使在恶劣的条件下，你的日记框架依然稳定而可靠。 所以这才是我们考虑用springboot原生集成的WEB开发包spring-boot-starter-web，里面就天然集成了slf4j+logback 日志记录相关依赖，首选Spring-Boot”原生态”的logback，也就是说我们在实际开发中，现在直接使用spring-boot-starter-web自带的slf4j+logback即可。 Logback是由 log4j创始人设计的又一个开源日志组件 logback当前分成三个模块：logback-corelogback- classiclogback-access logback-core是其它两个模块的基础模块 logback-classic 非常自然的实现了SLF4J，logback-classic 包下包括了logback-core+slf4j，我们不用额外导入。 logback-classic中的登陆类自然的实现了SLF4J。当你使用 logback-classic作为底层实现时，涉及到LF4J日记系统的问题你完全不需要考虑。更进一步来说，由于 logback-classic强烈建议使用SLF4J作为客户端日记系统实现，如果需要切换到log4j或者其他，你只需要替换一个jar包即可，不需要去改变那些通过SLF4J API 实现的代码。这可以大大减少更换日记系统的工作量。 关于logback的配置使用XML配置文件或者Groovy（Gradle区别于maven所建工程，Gradle基于Groovy语言，而maven基于xml） 配置logback的传统方法是通过XML文件。在文档中，大部分例子都是是用XML语法。但是，对于logback版本0.9.22，通过Groovy编写的配置文件也得到支持。相比于XML，Groovy风格的配置文件更加直观，连贯和简短的语法。 现在， 已经有一个工具自动把logback.xml文件迁移至logback.groovy。 若是我们要使用groovy构造工程的话，要在eclipse对应版本上安装groovy的集成开发环境，以为这是和maven类似的第三方集成库工程系统。 这些就不明说了，关于如何构建groovy工程，在专门的文章中有阐述。 不过这里不能像log4j那样直接使用properties文件来配置，我觉得有点麻烦。。。 先来看看logback中的日志配置与log4j又有那些不一样 如果配置文件 logback-test.xml 和 logback.xml 都不存在，那么 logback 默认地会调用BasicConfigurator ，创建一个最小化配置。最小化配置由一个关联到根 logger 的ConsoleAppender 组成。输出用模式为%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n 的 PatternLayoutEncoder 进行格式化。root logger 默认级别是 DEBUG。 在springboot中默认查找logback.xml和logback-spring.xml文件。 logback.xml配置如下： &lt;configuration&gt; &lt;!-- %m输出的信息,%p日志级别,%t线程名,%d日期,%c类的全名,%i索引【从数字0开始递增】,,, --&gt; &lt;!-- appender是configuration的子节点，是负责写日志的组件。 --&gt; &lt;!-- ConsoleAppender：把日志输出到控制台 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%d %p (%file:%line\)- %m%n&lt;/pattern&gt; &lt;!-- 控制台也要使用UTF-8，不要使用GBK，否则会中文乱码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 --&gt; &lt;!-- 以下的大概意思是：1.先按日期存日志，日期变了，将前一天的日志文件名重命名为XXX%日期%索引，新的日志仍然是sys.log --&gt; &lt;!-- 2.如果日期没有发生变化，但是当前日志的文件大小超过1KB时，对当前日志进行分割 重命名--&gt; &lt;appender name=&quot;syslog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;File&gt;log/sys.log&lt;/File&gt; &lt;!-- rollingPolicy:当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。 --&gt; &lt;!-- TimeBasedRollingPolicy： 最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责出发滚动 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!-- 活动文件的名字会根据fileNamePattern的值，每隔一段时间改变一次 --&gt; &lt;!-- 文件名：log/sys.2017-12-05.0.log --&gt; &lt;fileNamePattern&gt;log/sys.%d.%i.log&lt;/fileNamePattern&gt; &lt;!-- 每产生一个日志文件，该日志文件的保存期限为30天 --&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP&quot;&gt; &lt;!-- maxFileSize:这是活动文件的大小，默认值是10MB,本篇设置为1KB，只是为了演示 --&gt; &lt;maxFileSize&gt;1KB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;!-- pattern节点，用来设置日志的输入格式 --&gt; &lt;pattern&gt; %d %p (%file:%line\)- %m%n &lt;/pattern&gt; &lt;!-- 记录日志的编码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 控制台输出日志级别 --&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt; &lt;!-- 指定项目中某个包，当有日志操作行为时的日志记录级别 --&gt; &lt;!-- com.appley为根包，也就是只要是发生在这个根包下面的所有日志操作行为的权限都是DEBUG --&gt; &lt;!-- 级别依次为【从高到低】：FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE --&gt; &lt;logger name=&quot;com.appleyk&quot; level=&quot;DEBUG&quot;&gt; &lt;appender-ref ref=&quot;syslog&quot; /&gt; &lt;/logger&gt; &lt;/configuration&gt; 可以看出，logback在configuration标签中进行配置，包含3个最主要的子标签appender，logger和root可以这样描述配置文件的基本结构：以开头，后面有零个或多个元素，有零个或多个元素，有最多一个元素。 1.根节点，包含下面三个属性：-scan: 当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。-scanPeriod: 设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。-debug: 当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 例如： &lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;!--其他配置省略--&gt; &lt;/configuration&gt; 2.子节点 -：用来设置上下文名称，每个logger都关联到logger上下文，默认上下文名称为default。但可以使用设置成其他名字，用于区分不同应用程序的记录。一旦设置，不能修改。其实就是用于标记这个logback配置文件，作为一个全局名称来使用 myAppName -子节点：用来定义变量值，它有两个属性name和value，通过定义的值会被插入到logger上下文中，可以使“${}”来使用变量。就是自定义配置参数的意思，用户可以通过这个xml文件来修改参数。name: 变量的名称value: 的值时变量定义的值 例如： &lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;property name=&quot;APP_Name&quot; value=&quot;myAppName&quot; /&gt; &lt;contextName&gt;${APP_Name}&lt;/contextName&gt; &lt;!--其他配置省略--&gt; &lt;/configuration&gt; -子节点：获取时间戳字符串，他有两个属性key和datePatternkey: 标识此 的名字；datePattern: 设置将当前时间（解析配置文件的时间）转换为字符串的模式，遵循java.txt.SimpleDateFormat的格式。例如： &lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;timestamp key=&quot;bySecond&quot; datePattern=&quot;yyyyMMdd&apos;T&apos;HHmmss&quot;/&gt; &lt;contextName&gt;${bySecond}&lt;/contextName&gt; &lt;!-- 其他配置省略--&gt; &lt;/configuration&gt; -主要子节点：负责写日志的组件，它有两个必要属性name和class。name指定appender名称，class指定appender的全限定名 （1）ConsoleAppender 把日志输出到控制台，有以下子节点： ：对日志进行格式化。（具体参数稍后讲解 ） ：字符串System.out(默认)或者System.err（区别不多说了） &lt;configuration&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger{35} - %msg %n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--root用于指定级别 ref指向对应的appender的name属性值--&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt; &lt;/configuration&gt; 上述配置表示把&gt;=DEBUG级别的日志都输出到控制台 （2）FileAppender：把日志添加到文件，有以下子节点： ：被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 ：如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。 ：对记录事件进行格式化。（具体参数稍后讲解 ） ：如果是 true，日志会被安全的写入文件，即使其他的FileAppender也在向此文件做写入操作，效率低，默认是 false。例如： &lt;configuration&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;file&gt;testFile.log&lt;/file&gt; &lt;append&gt;true&lt;/append&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger{35} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- DEBUG以上的级别都会被输出 ref指向对应的appender的name属性值--&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;/root&gt; &lt;/configuration&gt; 上述配置表示把&gt;=DEBUG级别的日志都输出到testFile.log （3）RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。有以下子节点： ：被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。 ：如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。 :当发生滚动时，决定RollingFileAppender的行为，涉及文件移动和重命名。属性class定义具体的滚动策略类class=”ch.qos.logback.core.rolling.TimeBasedRollingPolicy”： 最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责出发滚动。有以下子节点：：必要节点，包含文件名及“%d”转换符，“%d”可以包含一个java.text.SimpleDateFormat指定的时间格式，如：%d{yyyy-MM}。如果直接使用 %d，默认格式是 yyyy-MM-dd。RollingFileAppender的file字节点可有可无，通过设置file，可以为活动文件和归档文件指定不同位置，当前日志总是记录到file指定的文件（活动文件），活动文件的名字不会改变；如果没设置file，活动文件的名字会根据fileNamePattern 的值，每隔一段时间改变一次。“/”或者“\”会被当做目录分隔符。:可选节点，控制保留的归档文件的最大数量，超出数量就删除旧文件。假设设置每个月滚动，且是6，则只保存最近6个月的文件，删除之前的旧文件。注意，删除旧文件是，那些为了归档而创建的目录也会被删除。class=”ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy”： 查看当前活动文件的大小，如果超过指定大小会告知RollingFileAppender 触发当前活动文件滚动。只有一个节点::这是活动文件的大小，默认值是10MB。：当为true时，不支持FixedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1不支持也不允许文件压缩，2不能设置file属性，必须留空。: 告知 RollingFileAppender 合适激活滚动。class=”ch.qos.logback.core.rolling.FixedWindowRollingPolicy” 根据固定窗口算法重命名文件的滚动策略。有以下子节点：:窗口索引最小值:窗口索引最大值，当用户指定的窗口过大时，会自动将窗口设置为12。:必须包含“%i”例如，假设最小值和最大值分别为1和2，命名模式为 mylog%i.log,会产生归档文件mylog1.log和mylog2.log。还可以指定文件压缩选项，例如，mylog%i.log.gz 或者 没有log%i.log.zip例如： logFile.%d{yyyy-MM-dd}.log 30 %-4relative [%thread] %-5level %logger{35} - %msg%n 上述配置表示每天生成一个日志文件，保存30天的日志文件。 test.log tests.%i.log.zip 1 3 &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;maxFileSize&gt;5MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger{35} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;/root&gt; &lt;/configuration&gt; 上述配置表示按照固定窗口模式生成日志文件，当文件大于20MB时，生成新的日志文件。窗口大小是1到3，当保存了3个归档文件后，将覆盖最早的日志。 ：对记录事件进行格式化。负责两件事，一是把日志信息转换成字节数组，二是把字节数组写入到输出流。PatternLayoutEncoder 是唯一有用的且默认的encoder ，有一个节点，用来设置日志的输入格式。使用“%”加“转换符”方式，如果要输出“%”，则必须用“\”对“\%”进行转义。 (4) 还有SocketAppender、SMTPAppender、DBAppender、SyslogAppender、SiftingAppender，并不常用，这里就不详解了。大家可以参考官方文档（http://logback.qos.ch/documentation.html），还可以编写自己的Appender。 -子节点：用来设置某一个包或具体的某一个类的日志打印级别、以及指定。仅有一个name属性，一个可选的level和一个可选的addtivity属性。 可以包含零个或多个元素，标识这个appender将会添加到这个logername: 用来指定受此loger约束的某一个包或者具体的某一个类。level: 用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL和OFF，还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。 如果未设置此属性，那么当前loger将会继承上级的级别。addtivity: 是否向上级loger传递打印信息。默认是true。同一样，可以包含零个或多个元素，标识这个appender将会添加到这个loger。 常用loger配置有： &lt;!-- show parameters for hibernate sql 专为 Hibernate 定制 --&gt; &lt;logger name=&quot;org.hibernate.type.descriptor.sql.BasicBinder&quot; level=&quot;TRACE&quot; /&gt; &lt;logger name=&quot;org.hibernate.type.descriptor.sql.BasicExtractor&quot; level=&quot;DEBUG&quot; /&gt; &lt;logger name=&quot;org.hibernate.SQL&quot; level=&quot;DEBUG&quot; /&gt; &lt;logger name=&quot;org.hibernate.engine.QueryParameters&quot; level=&quot;DEBUG&quot; /&gt; &lt;logger name=&quot;org.hibernate.engine.query.HQLQueryPlan&quot; level=&quot;DEBUG&quot; /&gt; &lt;!--myibatis log configure--&gt; &lt;logger name=&quot;com.apache.ibatis&quot; level=&quot;TRACE&quot;/&gt; &lt;logger name=&quot;java.sql.Connection&quot; level=&quot;DEBUG&quot;/&gt; &lt;logger name=&quot;java.sql.Statement&quot; level=&quot;DEBUG&quot;/&gt; &lt;logger name=&quot;java.sql.PreparedStatement&quot; level=&quot;DEBUG&quot;/&gt; -子节点:它也是元素，但是它是根loger,是所有的上级。只有一个level属性，因为name已经被命名为”root”,且已经是最上级了。level: 用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL和OFF，不能设置为INHERITED或者同义词NULL。 默认是DEBUG。 基本上常用的子节点已经介绍完了，再给个常用的DEMO吧： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;configuration debug=&quot;false&quot;&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt; &lt;property name=&quot;LOG_HOME&quot; value=&quot;/home&quot; /&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 按照每天生成日志文件 --&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;!--日志文件输出的文件名--&gt; &lt;FileNamePattern&gt;${LOG_HOME}/TestWeb.log.%d{yyyy-MM-dd}.log&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;!--日志文件最大的大小--&gt; &lt;triggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy&quot;&gt; &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt; &lt;/configuration&gt; 对应的JAVA代码应该怎么记录呢？ import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class App { private final static Logger logger = LoggerFactory.getLogger(App.class); public static void main(String[] args) { logger.info(&quot;logback-info 成功了&quot;); logger.error(&quot;logback-error 成功了&quot;); logger.debug(&quot;logback-debug 成功了&quot;); } } 与Log4j的不同，是用LoggerFactory的getLogger(getClass())来获取Logger对象的，这里要注意，若是用logback则用这种方式。 参考: https://blog.csdn.net/haidage/article/details/6794509/ https://www.cnblogs.com/warking/p/5710303.html https://blog.csdn.net/liuweixiao520/article/details/78900779 springboot整合slf4j+log4j2相当于又把logback的缺点重新改进，过程可以说是很有意思的，又log4j在性能上的缺失–》演变到使用性能更好的logback–》又演变到更好的log4j2框架上，可以说，选择很多种。。。 Log4j 升级Log4j 2 后的性能简单比较： 可见在同步日志模式下, Logback的性能是最糟糕的.而log4j2的性能无论在同步日志模式还是异步日志模式下都是最佳的. 其根本原因在于log4j2使用了LMAX, 一个无锁的线程间通信库代替了, logback和log4j之前的队列. 并发性能大大提升。关于LMAX，可以单独总结。 关于log4j2的新特性-丢数据这种情况少，可以用来做审计功能。而且自身内部报的exception会被发现，但是logback和log4j不会。-log4j2使用了disruptor技术，在多线程环境下，性能高于logback等10倍以上。-(garbage free）之前的版本会产生非常多的临时对象，会造成GC频繁，log4j2则在这方面上做了优化，减少产生临时对象。尽可能少的GC-利用插件系统，使得扩展新的appender,filter,layout等变得容易，log4j不可以扩展 插件？？？？-因为插件系统的简单性，所以在配置的时候，可以不用具体指定所要处理的类型。class-可以自定义level-Java 8 lambda support for lazy logging-Support for Message objects-对filter的功能支持的更强大-系统日志(Syslog)协议supports both TCP and UDP-利用jdk1.5并发的特性，减少了死锁的发生。-Socket LogEvent SerializedLayout-支持kafka queue 第一步：按照上面所说的，springboot本身默认集成的是logback，我们若是想用其他的，需要修改pom 去掉spring-boot-starter-logging &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; 添加Log4j2 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j2&lt;/artifactId&gt; &lt;/dependency&gt; 第二步：由于本身的properties默认配置对log4j不太友好，所以依然采用XML的外部文件配置形式然后需要在resource下面添加log4j2.xml配置文件，当然了如果你不添加，springboo会提示你没有对应文件，并使用默认的配置文件，这个时候级别可以在application.properties中配置。 logging.level.root=error 当然了，使用配置文件，配置可以多样化,下面是默认的log4j2配置,log4j2支持xml、json、yml格式的配置 例如： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;configuration status=&quot;OFF&quot;&gt; &lt;appenders&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot;/&gt; &lt;/Console&gt; &lt;/appenders&gt; &lt;loggers&gt; &lt;root level=&quot;error&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt; &lt;/configuration&gt; 大体上与log4j没什么区别。 appenders里设置日志的输出方式、级别和格式loggers里设置全局的级别和绑定appenders里的name File 日志输出到文件，可配置覆盖还是追加RollingFile “滚动文件”可作为按日输出日志的方式Console 控制台日志 PatternLayout 格式化输出日志 ThresholdFilter“阈值筛选器” 可单独设置appender的输出级别 loggers里需要匹配每个appender的名称 name 我的服务一般放在linux服务器上跑，可能要实时查看日志，现有这个需求“我要打印到控制台的日志级别为Error，日志文件里保存的是INFO级别的日志”这样在产生错误的时候，就不会被大量无用的代码干扰。要使用ThresholdFilter，配置如下： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;Configuration status=&quot;OFF&quot;&gt; &lt;Appenders&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&gt; &lt;ThresholdFilter level=&quot;ERROR&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;%d{yyyy.MM.dd &apos;at&apos; HH:mm:ss z} %-5level %class{36} %M() @%L - %msg%n&quot;/&gt; &lt;/Console&gt; &lt;File name=&quot;ERROR&quot; fileName=&quot;logs/error.log&quot; append=&quot;false&quot;&gt; &lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;%d{yyyy.MM.dd &apos;at&apos; HH:mm:ss z} %-5level %class{36} %M() @%L - %msg%n&quot;/&gt; &lt;/File&gt; &lt;!--这个会打印出所有的信息，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档--&gt; &lt;RollingFile name=&quot;RollingFile&quot; fileName=&quot;logs/app.log&quot; filePattern=&quot;log/$${date:yyyy-MM}/app-%d{MM-dd-yyyy}-%i.log.gz&quot;&gt; &lt;PatternLayout pattern=&quot;%d{yyyy.MM.dd &apos;at&apos; HH:mm:ss z} %-5level %class{36} %M() @%L - %msg%n&quot;/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;5MB&quot;/&gt; &lt;/RollingFile&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;ERROR&quot; /&gt; &lt;appender-ref ref=&quot;RollingFile&quot;/&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/Root&gt; &lt;/Loggers&gt; &lt;/Configuration&gt; 三个appender：Console、File、RollingFile Console 通过ThresholdFilter过滤规则只输出ERROR级别的错误(onMatch=”ACCEPT” onMismatch=”DENY” 匹配到的接受，没有匹配的走人) File 也通过ThresholdFilter的方式输出到日志，当然了append=”false” 会在服务每次启动的时候清空日志(覆盖) RollingFile 因为日志全局设置的为INFO，所以不需要ThresholdFilter,这里只需要指定filePattern和SizeBasedTriggeringPolicy就行了 多环境分别使用不同的log4j2的配置文件在多环境中（开发-测试-生产），有不同的properties文件，每个文件对应的日志配置不一样，那么我们如何均衡这种关系。 主properties配置文件中激活指定的properties配置文件，如激活 dev的properties文件只需添加“spring.profiles.active=dev ”即可，这是进入”application-dev.properties”配置文件，在该文件中添加“logging.config=classpath:log4j2-dev.xml”，这时候dev开发环境将使用“log4j2-dev.xml”配置信息来输出日志。这样就对应了每个环境下的配置文件能够对应不同的日志系统 参考：https://www.oschina.net/translate/reasons-to-prefer-logbak-over-log4j https://blog.csdn.net/liuweixiao520/article/details/78900779 https://logging.apache.org/log4j/2.x/manual/configuration.html#AutomaticConfiguration]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>logback</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LDAP服务器结合Springboot]]></title>
    <url>%2F2018%2F04%2F15%2FLDAP%2F</url>
    <content type="text"><![CDATA[LDAP（轻量级目录访问协议，Lightweight Directory Access Protocol)是实现提供被称为目录服务的信息服务。目录服务是一种特殊的数据库系统，其专门针对读取，浏览和搜索操作进行了特定的优化。目录一般用来包含描述性的，基于属性的信息并支持精细复杂的过滤能力。目录一般不支持通用数据库针对大量更新操作操作需要的复杂的事务管理或回卷策略。就是说LDAP精于读取和查询，在写入功能上表现较差，不支持事务和回滚操作，而目录服务的更新则一般都非常简单。这种目录可以存储包括个人信息、web链结、jpeg图像等各种信息。为了访问存储在目录中的信息，就需要使用运行在TCP/IP 之上的访问协议—LDAP。 目录简单来说就是一种树状结构的数据库。 而目录服务是一种以树状结构的目录数据库为基础，外加各种访问协议的信息查询服务。 顾名思义，目录天生就是用来查询的。 目录服务是由目录数据库和一套访问协议组成的系统。类似以下的信息适合储存在目录中： 企业员工信息，如姓名、电话、邮箱等；公用证书和安全密钥；公司的物理设备信息，如服务器，它的IP地址、存放位置、厂商、购买时间等；LDAP是轻量目录访问协议(Lightweight Directory Access Protocol)的缩写，LDAP是从X.500目录访问协议的基础上发展过来的，目前的版本是v3.0。与LDAP一样提供类似的目录服务软件还有ApacheDS、Active Directory、Red Hat Directory Service 。 LDAP特点LDAP的结构用树来表示，而不是用表格。正因为这样，就不能用SQL语句了LDAP可以很快地得到查询结果，不过在写方面，就慢得多LDAP提供了静态数据的快速查询方式Client/server模型，Server 用于存储数据，Client提供操作目录信息树的工具这些工具可以将数据库的内容以文本格式（LDAP 数据交换格式，LDIF）呈现在您的面前LDAP是一种开放Internet标准，LDAP协议是跨平台的Interent协议 LDAP组织数据的方式 Entry条目，也叫记录项，是LDAP中最基本的颗粒，就像字典中的词条，或者是数据库中的记录。通常对LDAP的添加、删除、更改、检索都是以条目为基本对象的。 dn：每一个条目都有一个唯一的标识名（distinguished Name ，DN），如上图中一个 dn：”cn=baby,ou=marketing,ou=people,dc=mydomain,dc=org” 。通过DN的层次型语法结构，可以方便地表示出条目在LDAP树中的位置，通常用于检索。 rdn：一般指dn逗号最左边的部分，如cn=baby。它与RootDN不同，RootDN通常与RootPW同时出现，特指管理LDAP中信息的最高权限用户。 Base DN：LDAP目录树的最顶部就是根，也就是所谓的“Base DN”，如”dc=mydomain,dc=org”。 Attribute每个条目都可以有很多属性（Attribute），比如常见的人都有姓名、地址、电话等属性。每个属性都有名称及对应的值，属性值可以有单个、多个，比如你有多个邮箱。 属性不是随便定义的，需要符合一定的规则，而这个规则可以通过schema制定。比如，如果一个entry没有包含在 inetorgperson 这个 schema 中的objectClass: inetOrgPerson，那么就不能为它指定employeeNumber属性，因为employeeNumber是在inetOrgPerson中定义的。 LDAP为人员组织机构中常见的对象都设计了属性(比如commonName，surname)。下面有一些常用的别名： ObjectClass对象类是属性的集合，LDAP预想了很多人员组织机构中常见的对象，并将其封装成对象类。比如人员（person）含有姓（sn）、名（cn）、电话(telephoneNumber)、密码(userPassword)等属性，单位职工(organizationalPerson)是人员(person)的继承类，除了上述属性之外还含有职务（title）、邮政编码（postalCode）、通信地址(postalAddress)等属性。 通过对象类可以方便的定义条目类型。每个条目可以直接继承多个对象类，这样就继承了各种属性。如果2个对象类中有相同的属性，则条目继承后只会保留1个属性。对象类同时也规定了哪些属性是基本信息，必须含有(Must或Required，必要属性)：哪些属性是扩展信息，可以含有（May或Optional，可选属性）。 对象类有三种类型：结构类型（Structural）、抽象类型(Abstract)和辅助类型（Auxiliary）。结构类型是最基本的类型，它规定了对象实体的基本属性，每个条目属于且仅属于一个结构型对象类。抽象类型可以是结构类型或其他抽象类型父类，它将对象属性中共性的部分组织在一起，称为其他类的模板，条目不能直接集成抽象型对象类。辅助类型规定了对象实体的扩展属性。每个条目至少有一个结构性对象类。 对象类本身是可以相互继承的，所以对象类的根类是top抽象型对象类。以常用的人员类型为例，他们的继承关系： 下面是inetOrgPerson对象类的在schema中的定义，可以清楚的看到它的父类SUB和可选属性MAY、必要属性MUST(继承自organizationalPerson)，关于各属性的语法则在schema中的attributetype定义。 `# inetOrgPerson` `# The inetOrgPerson represents people who are associated with an` `# organization in some way. It is a structural class and is derived` `# from the organizationalPerson which is defined in X.521 [X521].` objectclass ( 2.16.840.1.113730.3.2.2 NAME &apos;inetOrgPerson&apos; DESC &apos;RFC2798: Internet Organizational Person&apos; SUP organizationalPerson STRUCTURAL MAY ( audio $ businessCategory $ carLicense $ departmentNumber $ displayName $ employeeNumber $ employeeType $ givenName $ homePhone $ homePostalAddress $ initials $ jpegPhoto $ labeledURI $ mail $ manager $ mobile $ o $ pager $ photo $ roomNumber $ secretary $ uid $ userCertificate $ x500uniqueIdentifier $ preferredLanguage $ userSMIMECertificate $ userPKCS12 ) ) Schema对象类（ObjectClass）、属性类型（AttributeType）、语法（Syntax）分别约定了条目、属性、值，他们之间的关系如下图所示。所以这些构成了模式(Schema)——对象类的集合。条目数据在导入时通常需要接受模式检查，它确保了目录中所有的条目数据结构都是一致的。 schema（一般在/etc/ldap/schema/目录）在导入时要注意前后顺序。 backend &amp; databaseldap的后台进程slapd接收、响应请求，但实际存储数据、获取数据的操作是由Backends做的，而数据是存放在database中，所以你可以看到往往你可以看到backend和database指令是一样的值如 bdb 。一个 backend 可以有多个 database instance，但每个 database 的 suffix 和 rootdn 不一样。openldap 2.4版本的模块是动态加载的，所以在使用backend时需要moduleload back_bdb指令。 bdb是一个高性能的支持事务和故障恢复的数据库后端，可以满足绝大部分需求。许多旧文档里（包括官方）说建议将bdb作为首选后端服务（primary backend），但2.4版文档明确说hdb才是被首先推荐使用的，这从 2.4.40 版默认安装后的配置文件里也可以看出。hdb是基于bdb的，但是它通过扩展的索引和缓存技术可以加快数据访问，修改entries会更有效率，有兴趣可以访问上的链接或slapd.backends。 另外config是特殊的backend，用来在运行时管理slapd的配置，它只能有一个实例，甚至无需显式在slapd.conf中配置。 TLS &amp; SASL分布式LDAP 是以明文的格式通过网络来发送信息的，包括client访问ldap的密码（当然一般密码已然是二进制的），SSL/TLS 的加密协议就是来保证数据传送的保密性和完整性。 SASL （Simple Authenticaion and Security Layer）简单身份验证安全框架，它能够实现openldap客户端到服务端的用户验证，也是ldapsearch、ldapmodify这些标准客户端工具默认尝试与LDAP服务端认证用户的方式（前提是已经安装好 Cyrus SASL）。SASL有几大工业实现标准：Kerveros V5、DIGEST-MD5、EXTERNAL、PLAIN、LOGIN。 Kerveros V5是里面最复杂的一种，使用GSSAPI机制，必须配置完整的Kerberos V5安全系统，密码不再存放在目录服务器中，每一个dn与Kerberos数据库的主体对应。DIGEST-MD5稍微简单一点，密码通过saslpasswd2生成放在sasldb数据库中，或者将明文hash存到LDAP dn的userPassword中，每一个authid映射成目录服务器的dn，常和SSL配合使用。参考 https://docs.oracle.com/cd/E19957-01/820-0293/6nc1tbp0h/index.html EXTERNAL一般用于初始化添加schema时使用，如ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/core.ldif。 LDIFLDIF 是一种普遍使用的文件格式，用来描述目录信息或可对目录执行的修改操作。LDIF（LDAP Data Interchange Format，数据交换格式）是LDAP数据库信息的一种文本格式，用于数据的导入导出，每行都是“属性: 值”对。 结构参考：https://blog.csdn.net/daily11/article/details/51030464https://zhuanlan.zhihu.com/p/32732045 openLDAP服务器安装参考：http://seanlook.com/2015/01/21/openldap-install-guide-ssl/ SpringBoot整合LDAP很多时候，我们在构建系统的时候都会自己创建用户管理体系，这对于开发人员来说并不是什么难事，但是当我们需要维护多个不同系统并且相同用户跨系统使用的情况下，如果每个系统维护自己的用户信息，那么此时用户信息的同步就会变的比较麻烦，对于用户自身来说也会非常困扰，很容易出现不同系统密码不一致啊等情况出现。如果此时我们引入LDAP来集中存储用户的基本信息并提供统一的读写接口和校验机制，那么这样的问题就比较容易解决了。下面就来说说当我们使用Spring Boot开发的时候，如何来访问LDAP服务端。 LDAP目录中的信息是是按照树型结构组织，具体信息存储在条目(entry)的数据结构中。条目相当于关系数据库中表的记录；条目是具有区别名DN （Distinguished Name）的属性（Attribute），DN是用来引用条目的，DN相当于关系数据库表中的关键字（Primary Key）。属性由类型（Type）和一个或多个值（Values）组成，相当于关系数据库中的字段（Field）由字段名和数据类型组成，只是为了方便检索的需要，LDAP中的Type可以有多个Value，而不是关系数据库中为降低数据的冗余性要求实现的各个域必须是不相关的。LDAP中条目的组织一般按照地理位置和组织关系进行组织，非常的直观。LDAP把数据存放在文件中，为提高效率可以使用基于索引的文件数据库，而不是关系数据库。类型的一个例子就是mail，其值将是一个电子邮件地址。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析与挖掘--POWER BI]]></title>
    <url>%2F2018%2F04%2F14%2Fpower-bi%2F</url>
    <content type="text"><![CDATA[大数据已经成为互联网时代的进程之一，海量数据的处理可以通过hadoop+spark和分布式架构系统进行实现，所有数据的可视化需求显得更加迫切，如何将大数据更加直观的展现给非技术人员去理解，这就是power BI。]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>powerbi</tag>
        <tag>数据分析</tag>
        <tag>生产工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ORM框架--Hibernate]]></title>
    <url>%2F2018%2F04%2F09%2Fhibernate%2F</url>
    <content type="text"><![CDATA[Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，它将POJO与数据库表建立映射关系，是一个全自动的orm框架，hibernate可以自动生成SQL语句，自动执行，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。 Hibernate可以应用在任何使用JDBC的场合，既可以在Java的客户端程序使用，也可以在Servlet/JSP的Web应用中使用，最具革命意义的是，Hibernate可以在应用EJB的J2EE架构中取代CMP，完成数据持久化的重任。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis在spring和springboot框架中的操作总结]]></title>
    <url>%2F2018%2F04%2F08%2Fmybatis%2F</url>
    <content type="text"><![CDATA[Mybatis在我上个项目中结合spring+springMVC框架的SSM架构中有用到，Mybatis作为一个半自动化的ORM框架，在编程的难度对比上我觉得比Hibernate的性能要好，对简单数据做操作时当然是Hibernate更为方便因为不用写SQL语句，但是在操作的自由性和性能的比较上我觉得mybatis要更好些，但是我最近又系统学习了spring-data-jpa框架，我觉得这个框架用来写数据交互真的要比Mybatis或者Hibernate爽太多了。。。（个人意见，可能有一部分是因为结合了Springboot） mybatis是一个半自动化的orm框架，所谓半自动化就是mybaitis只支持数据库查出的数据映射到pojo类上，而实体到数据库的映射需要自己编写sql语句实现，相较于hibernate这种完全自动化的框架我更喜欢mybatis，mybatis非常灵活，可以随心所欲的编写自己的sql语句来实现复杂的数据库操作，还会有一种畅酣淋漓的编写sql语句的潇洒感，但是以前的mybaits需要一大堆的配置文件，以及各种mapper和dao和实体的关联，导致使用mybatis还是不够简洁，后来mybatis也发现了这个弊端，开发了mybatis generator工具来自动化生成实体类、mapper配置文件、dao层代码来减轻开发工作量，在后期也是实现了抛弃mapper配置文件基于注解的开发模式，直到现在，mybatis看spring boot这么火热，也开发了一套基于spring boot的模式：mybatis-spring-boot-starter。spring boot简约轻巧的风格正在逐渐被越来越多的厂商及开发者所认可，包括阿里的开源RPC框架dubbo也准备开发一套对spring boot应用的支持（dubbo-spring-boot-starter启动配置模块） 在springboot框架下开发项目很爽，因为需要集成的基本单元已经帮你集成好了，因为使用了java配置的原因，所以零XML配置使得整个开发过程很放松，让工程师能够更好的将精力集中在编写实际的业务逻辑上。 Mybatis+Spring+SpringMVC的SSM架构使用mybatis generator工具来简化开发。Mybatis+Springboot+SpringMVC的新架构就采用mybatis-spring-boot-starter。 spring+mybatis我这里从最传统的spring+Mybatis项目开始讲起，因为要产生对比才能更好的理解新架构对开发的改变！ 传统模式在原先的项目中简单分析下mybatis与数据库在DAO层进行交互的一般流程： 第一步：首先先编写实体类，满足javaBean规范（有包，实现序列化接口，有无参数构造器，各个属性有相应的get/set方法），实体类对应数据库中的表，这里我用的是mysql数据库来做项目。 第二步：编写DAO接口，里面有一些对这个表所做的操作，具体的实现在mapper文件中写sql语句。 第三步：写mapper.xml映射文件，主要是映射实体类，和实现DAO接口的逻辑并且在配置文件中要进行org.mybatis.spring.SqlSessionFactoryBean的配置，指定所有mapper文件所存在的包名mapper文件的xml头格式也有要求：&lt;?xml version=”1.0” encoding=”UTF-8” ?&gt;&lt;!DOCTYPE mapper PUBLIC “-//ibatis.apache.org//DTD Mapper 3.0//EN” “http://ibatis.apache.org/dtd/ibatis-3-mapper.dtd&quot;&gt;… 这是传统的模式与数据库进行交互，因为要编写大量的实体类entity，Dao接口，mapper文件，很难管理且开发太过恶心。 后来出现了mybatis generator工具来自动来自动生成这些东西，居然还有基于注解的去mapper化开发模式，这也是我今天总结的时候才发现的，这不是学jpa吗？ 先来看看这个mybatis generator工具到底是啥玩意。。 mybatis generator工具模式 ##’mybatis generator工具官网:http://mbg.cndocs.ml/mybatis-generator 是一个代码自动生成工具，手动写入一个个实体类和mapper还有xml配置文件感觉会很麻烦，使用mybatis generator只需要简单的配置就能完成我们的工作，这里简述一下开发步骤。 MyBatis Generator (MBG) 是一个Mybatis的代码生成器 MyBatis 和 iBATIS. 他可以生成Mybatis各个版本的代码，和iBATIS 2.2.0版本以后的代码。 他可以内省数据库的表（或多个表）然后生成可以用来访问（多个）表的基础对象。 这样和数据库表进行交互时不需要创建对象和配置文件。 MBG的解决了对数据库操作有最大影响的一些简单的CRUD（插入，查询，更新，删除）操作。 您仍然需要对联合查询和存储过程手写SQL和对象。 mybatis generator的原理就是根据你数据库中已有的表自动生成对应的实体类，mapper以及DAO接口，我们先来操作一下(表已经事先在数据库中创建好了) 第一步：导入相关的jar包 &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.annotation&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.ejb&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.weld&lt;/groupId&gt; &lt;artifactId&gt;weld-osgi-bundle&lt;/artifactId&gt; &lt;version&gt;1.0.1-SP3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.glassfish&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; &lt;!--测试框架 --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.7&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Mysql 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.6&lt;/version&gt; &lt;/dependency&gt; &lt;!--生成代码插件--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.6&lt;/source&gt; &lt;target&gt;1.6&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 第二步:新建生成代码的配置文件mybatis-generator-config.xml具体更详细的各参数配置在这里：https://www.jianshu.com/p/e09d2370b796注意这个文件是最坑的，一定不要设置错了，不然很尴尬。。。自动生成虽然好用，但是也要多做才能完全掌握其中奥义。 &lt;generatorConfiguration&gt; &lt;context id=&quot;prod&quot;&gt; &lt;!-- RowBounds pagination --&gt; &lt;plugin type=&quot;org.mybatis.generator.plugins.RowBoundsPlugin&quot; /&gt; &lt;plugin type=&quot;org.mybatis.generator.plugins.CaseInsensitiveLikePlugin&quot; /&gt; &lt;plugin type=&quot;org.mybatis.generator.plugins.SerializablePlugin&quot; /&gt; &lt;commentGenerator&gt; &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;true&quot; /&gt; &lt;/commentGenerator&gt; &lt;!-- jdbc连接 --&gt; &lt;jdbcConnection driverClass=&quot;com.mysql.jdbc.Driver&quot; connectionURL=&quot;jdbc:mysql:///test&quot; userId=&quot;root&quot; password=&quot;294823013&quot; /&gt; &lt;!-- 在javaModelGenerator标签下配置你需要生成的数据库实体的地址 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.mybatis.entity&quot; targetProject=&quot;src/main/java&quot;&gt; &lt;!-- 是否针对string类型的字段在set的时候进行trim调用 --&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot; /&gt; &lt;/javaModelGenerator&gt; &lt;!-- 在sqlMapGenerator标签下配置mysql的xml配置文件 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mappers&quot; targetProject=&quot;src/main/java&quot; /&gt; &lt;!-- 在javaClientGenerator标签下配置mapper方法，相当于DAO接口，业务层调用里面的方法对数据库进行操作 --&gt; &lt;javaClientGenerator targetPackage=&quot;com.mybatis.mapper&quot; targetProject=&quot;src/main/java&quot; type=&quot;XMLMAPPER&quot; /&gt; &lt;!-- 在table标签下配置数据库的表名和生成实体的表名，表名是koro_table，所生成的实体类是KoroTable --&gt; &lt;table tableName=&quot;koro_table&quot; domainObjectName=&quot;KoroTable&quot;&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt; 第三步：新建批处理类main方法利用mybatis generator来进行自动生成，编写 public class App { public static void main(String[] args) { args = new String[] { &quot;-configfile&quot;, &quot;src\\main\\resources\\mybatis-generator-config.xml&quot;, &quot;-overwrite&quot; }; ShellRunner.main(args); } } 执行这个类就可以自动生成了，生成后的目录结构是这样的： 我们会发现我们生成了两个实体对象，一个是数据库映射对象，一个是Example对象。Example对象就是为了方便我们执行sql操作的类，可以使用Example类进行数据库的条件查询。同时mybatis-generator还帮助我们生成了sql的CRUD等操作。com.mybatis.mapper下的KoroTableMapper.java就是DAO接口，里面是一些crud操作 public interface KoroTableMapper { int countByExample(KoroTableExample example); int deleteByExample(KoroTableExample example); int deleteByPrimaryKey(Integer id); int insert(KoroTable record); int insertSelective(KoroTable record); List&lt;KoroTable&gt; selectByExampleWithRowbounds(KoroTableExample example, RowBounds rowBounds); List&lt;KoroTable&gt; selectByExample(KoroTableExample example); KoroTable selectByPrimaryKey(Integer id); int updateByExampleSelective(@Param(&quot;record&quot;) KoroTable record, @Param(&quot;example&quot;) KoroTableExample example); int updateByExample(@Param(&quot;record&quot;) KoroTable record, @Param(&quot;example&quot;) KoroTableExample example); int updateByPrimaryKeySelective(KoroTable record); int updateByPrimaryKey(KoroTable record); } mappers目录下的是自动生成的mapper.xml文件，对应DAO接口的各个方法的实现 &lt;mapper namespace=&quot;com.mybatis.mapper.KoroTableMapper&quot; &gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.mybatis.entity.KoroTable&quot; &gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; jdbcType=&quot;INTEGER&quot; /&gt; &lt;result column=&quot;NAME&quot; property=&quot;name&quot; jdbcType=&quot;VARCHAR&quot; /&gt; &lt;result column=&quot;age&quot; property=&quot;age&quot; jdbcType=&quot;INTEGER&quot; /&gt; &lt;/resultMap&gt; &lt;sql id=&quot;Example_Where_Clause&quot; &gt; &lt;where &gt; &lt;foreach collection=&quot;oredCriteria&quot; item=&quot;criteria&quot; separator=&quot;or&quot; &gt; &lt;if test=&quot;criteria.valid&quot; &gt; &lt;trim prefix=&quot;(&quot; suffix=&quot;)&quot; prefixOverrides=&quot;and&quot; &gt; &lt;foreach collection=&quot;criteria.criteria&quot; item=&quot;criterion&quot; &gt; &lt;choose &gt; &lt;when test=&quot;criterion.noValue&quot; &gt; and ${criterion.condition} &lt;/when&gt; &lt;when test=&quot;criterion.singleValue&quot; &gt; and ${criterion.condition} #{criterion.value} &lt;/when&gt; &lt;when test=&quot;criterion.betweenValue&quot; &gt; and ${criterion.condition} #{criterion.value} and #{criterion.secondValue} &lt;/when&gt; &lt;when test=&quot;criterion.listValue&quot; &gt; and ${criterion.condition} &lt;foreach collection=&quot;criterion.value&quot; item=&quot;listItem&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot; &gt; `#{listItem}` &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id=&quot;Update_By_Example_Where_Clause&quot; &gt; &lt;where &gt; &lt;foreach collection=&quot;example.oredCriteria&quot; item=&quot;criteria&quot; separator=&quot;or&quot; &gt; &lt;if test=&quot;criteria.valid&quot; &gt; &lt;trim prefix=&quot;(&quot; suffix=&quot;)&quot; prefixOverrides=&quot;and&quot; &gt; &lt;foreach collection=&quot;criteria.criteria&quot; item=&quot;criterion&quot; &gt; &lt;choose &gt; &lt;when test=&quot;criterion.noValue&quot; &gt; and ${criterion.condition} &lt;/when&gt; &lt;when test=&quot;criterion.singleValue&quot; &gt; and ${criterion.condition} #{criterion.value} &lt;/when&gt; &lt;when test=&quot;criterion.betweenValue&quot; &gt; and ${criterion.condition} #{criterion.value} and #{criterion.secondValue} &lt;/when&gt; &lt;when test=&quot;criterion.listValue&quot; &gt; and ${criterion.condition} &lt;foreach collection=&quot;criterion.value&quot; item=&quot;listItem&quot; open=&quot;(&quot; close=&quot;)&quot; separator=&quot;,&quot; &gt; `#{listItem}` &lt;/foreach&gt; &lt;/when&gt; &lt;/choose&gt; &lt;/foreach&gt; &lt;/trim&gt; &lt;/if&gt; &lt;/foreach&gt; &lt;/where&gt; &lt;/sql&gt; &lt;sql id=&quot;Base_Column_List&quot; &gt; id, NAME, age &lt;/sql&gt; &lt;select id=&quot;selectByExample&quot; resultMap=&quot;BaseResultMap&quot; parameterType=&quot;com.mybatis.entity.KoroTableExample&quot; &gt; select &lt;if test=&quot;distinct&quot; &gt; distinct &lt;/if&gt; &lt;include refid=&quot;Base_Column_List&quot; /&gt; from koro_table &lt;if test=&quot;_parameter != null&quot; &gt; &lt;include refid=&quot;Example_Where_Clause&quot; /&gt; &lt;/if&gt; &lt;if test=&quot;orderByClause != null&quot; &gt; order by ${orderByClause} &lt;/if&gt; &lt;/select&gt; &lt;select id=&quot;selectByPrimaryKey&quot; resultMap=&quot;BaseResultMap&quot; parameterType=&quot;java.lang.Integer&quot; &gt; select &lt;include refid=&quot;Base_Column_List&quot; /&gt; from koro_table where id = #{id,jdbcType=INTEGER} &lt;/select&gt; &lt;delete id=&quot;deleteByPrimaryKey&quot; parameterType=&quot;java.lang.Integer&quot; &gt; delete from koro_table where id = #{id,jdbcType=INTEGER} &lt;/delete&gt; &lt;delete id=&quot;deleteByExample&quot; parameterType=&quot;com.mybatis.entity.KoroTableExample&quot; &gt; delete from koro_table &lt;if test=&quot;_parameter != null&quot; &gt; &lt;include refid=&quot;Example_Where_Clause&quot; /&gt; &lt;/if&gt; &lt;/delete&gt; &lt;insert id=&quot;insert&quot; parameterType=&quot;com.mybatis.entity.KoroTable&quot; &gt; insert into koro_table (id, NAME, age ) values (#{id,jdbcType=INTEGER}, #{name,jdbcType=VARCHAR}, #{age,jdbcType=INTEGER} ) &lt;/insert&gt; &lt;insert id=&quot;insertSelective&quot; parameterType=&quot;com.mybatis.entity.KoroTable&quot; &gt; insert into koro_table &lt;trim prefix=&quot;(&quot; suffix=&quot;)&quot; suffixOverrides=&quot;,&quot; &gt; &lt;if test=&quot;id != null&quot; &gt; id, &lt;/if&gt; &lt;if test=&quot;name != null&quot; &gt; NAME, &lt;/if&gt; &lt;if test=&quot;age != null&quot; &gt; age, &lt;/if&gt; &lt;/trim&gt; &lt;trim prefix=&quot;values (&quot; suffix=&quot;)&quot; suffixOverrides=&quot;,&quot; &gt; &lt;if test=&quot;id != null&quot; &gt; `#{id,jdbcType=INTEGER},` &lt;/if&gt; &lt;if test=&quot;name != null&quot; &gt; `#{name,jdbcType=VARCHAR},` &lt;/if&gt; &lt;if test=&quot;age != null&quot; &gt; `#{age,jdbcType=INTEGER},` &lt;/if&gt; &lt;/trim&gt; &lt;/insert&gt; &lt;select id=&quot;countByExample&quot; parameterType=&quot;com.mybatis.entity.KoroTableExample&quot; resultType=&quot;java.lang.Integer&quot; &gt; select count(*) from koro_table &lt;if test=&quot;_parameter != null&quot; &gt; &lt;include refid=&quot;Example_Where_Clause&quot; /&gt; &lt;/if&gt; &lt;/select&gt; &lt;update id=&quot;updateByExampleSelective&quot; parameterType=&quot;map&quot; &gt; update koro_table &lt;set &gt; &lt;if test=&quot;record.id != null&quot; &gt; id = #{record.id,jdbcType=INTEGER}, &lt;/if&gt; &lt;if test=&quot;record.name != null&quot; &gt; NAME = #{record.name,jdbcType=VARCHAR}, &lt;/if&gt; &lt;if test=&quot;record.age != null&quot; &gt; age = #{record.age,jdbcType=INTEGER}, &lt;/if&gt; &lt;/set&gt; &lt;if test=&quot;_parameter != null&quot; &gt; &lt;include refid=&quot;Update_By_Example_Where_Clause&quot; /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id=&quot;updateByExample&quot; parameterType=&quot;map&quot; &gt; update koro_table set id = #{record.id,jdbcType=INTEGER}, NAME = #{record.name,jdbcType=VARCHAR}, age = #{record.age,jdbcType=INTEGER} &lt;if test=&quot;_parameter != null&quot; &gt; &lt;include refid=&quot;Update_By_Example_Where_Clause&quot; /&gt; &lt;/if&gt; &lt;/update&gt; &lt;update id=&quot;updateByPrimaryKeySelective&quot; parameterType=&quot;com.mybatis.entity.KoroTable&quot; &gt; update koro_table &lt;set &gt; &lt;if test=&quot;name != null&quot; &gt; NAME = #{name,jdbcType=VARCHAR}, &lt;/if&gt; &lt;if test=&quot;age != null&quot; &gt; age = #{age,jdbcType=INTEGER}, &lt;/if&gt; &lt;/set&gt; where id = #{id,jdbcType=INTEGER} &lt;/update&gt; &lt;update id=&quot;updateByPrimaryKey&quot; parameterType=&quot;com.mybatis.entity.KoroTable&quot; &gt; update koro_table set NAME = #{name,jdbcType=VARCHAR}, age = #{age,jdbcType=INTEGER} where id = #{id,jdbcType=INTEGER} &lt;/update&gt; &lt;select resultMap=&quot;BaseResultMap&quot; parameterType=&quot;com.mybatis.entity.KoroTableExample&quot; id=&quot;selectByExampleWithRowbounds&quot; &gt; select &lt;if test=&quot;distinct&quot; &gt; distinct &lt;/if&gt; &lt;include refid=&quot;Base_Column_List&quot; /&gt; from koro_table &lt;if test=&quot;_parameter != null&quot; &gt; &lt;include refid=&quot;Example_Where_Clause&quot; /&gt; &lt;/if&gt; &lt;if test=&quot;orderByClause != null&quot; &gt; order by ${orderByClause} &lt;/if&gt; &lt;/select&gt; &lt;/mapper&gt; 标签中是对应数据库表的所有字段的映射等等，mybatis-generator为我们全自动化的根据数据库表结构生成实体类，DAO接口以及mapper映射，可以说是很方便。 springboot与mybatis整合第一步：springboot与mybatis整合的导包：mysql连接与mybatis-spring-boot-starter即可 &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; 第二步：在application.properties文件中配置mybatis的相关项 #mybatis.config-location=classpath:mybatis-config.xml #mybatis mapper文件的位置mybatis.mapper-locations=classpath:mapper/.xml #扫描pojo类的位置,在此处指明扫描实体类的包，在mapper中就可以不用写pojo类的全路径名了mybatis.type-aliases-package=com.domain jdbc.type=mysqlspring.datasource.url=jdbc:mysql://localhost:3306/testspring.datasource.username=rootspring.datasource.password=294823013spring.datasource.driver-class-name=com.mysql.jdbc.Driver 第三步：在数据库中建立表meaage并给些数据（准备好数据库） 第四步：编写实体类Message，名称与表名对应 public class Message { private int id; private String content; private String name; //省略get/set方法 //添加无参构造器 } 第五步：编写Dao接口 //加上该注解才能使用@MapperScan扫描到 @Mapper public interface MessageDao { Message findById(@Param(&quot;id&quot;)int id); } 第六步：编写Dao对应的mapper文件 &lt;mapper namespace=&quot;com.mybatis.domain.MessageDao&quot;&gt; &lt;select id=&quot;findById&quot; parameterType=&quot;int&quot; resultType=&quot;com.mybatis.domain.Message&quot;&gt; SELECT * FROM message WHERE id=#{id} &lt;/select&gt; &lt;/mapper&gt; 第七步：启动类 @SpringBootApplication @MapperScan(&quot;mapper&quot;) public class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } } 添加@MapperScan(“mapper”)，以扫描mapper文件夹下的那些mapper文件 第八步：添加Controller类来简单测试 @RestController public class MessageController { @Autowired private MessageDao messageDao; @RequestMapping(&quot;/msg&quot;) public Message msg(){ Message msg=messageDao.findById(1); System.out.println(msg); return msg; } } 第九步：测试，浏览器输入：http://localhost:8088/msg 得到结果：{“id”:1,”content”:”aaaaaaaaaa”,”name”:”o1”} 测试成功！！ springboot+mybatis的操作就是这些。。。（总之我觉得改变的话就是springboot将一对的xml配置信息全部简化放在application.properties中去进行配置，我们只需要去完成逻辑即可） 当我们需要一个很简单的DML功能时，如果去创建mapper文件并编写一堆语句的时候也许会显得很麻烦，这个时候就可以通过注解的方式简化配置，新建一个UserAnnotationDao通过注解的方式来实现增删改查: 接下来介绍不需要mapper文件来对应Dao接口的方法（其实本质就是用java配置取代xml配置的意思） 新建MessageAnnotationDao，但是这次不用mapper文件来实现Dao中的方法，用@Select和@Results注解来配置在使用简单的DML操作时，为了不用mapper那么麻烦，我们可以采用注解方式直接配置。控制层直接调用即可。 @Mapper public interface MessageAnnotationDao { @Select(&quot;SELECT * FROM message where id=#{id}&quot;) @Results({ @Result(property=&quot;id&quot;,column=&quot;id&quot;), @Result(property=&quot;content&quot;,column=&quot;content&quot;), @Result(property=&quot;name&quot;,column=&quot;name&quot;) }) public Message findById(int id); } 使用注解自后就不需要mapper文件了，分别测试这几个方法均能正确执行，使用注解和使用xml的方式都差不多，通常情况下，如果没有复杂的连接查询，我们可以使用注解的方式，当设计到复杂的sql还是使用xml的方式更好掌控一些，所以通常情况下两种方式都会使用，根据sql的复杂程度选择不同的方式来提高开发效率。 关于不用mapper文件配置sql语句的注解有哪些？1.传参方式下面通过几种不同传参方式来实现前文中实现的插入操作。 -使用@Param这个注解在控制层也会用到，用于声明html文件中form表单提交时的key与传参的参数名对应，这样就可以更加保险的传递参数。 Insert(“INSERT INTO USER(NAME, AGE) VALUES(#{name}, #{age})”)int insert(@Param(“name”) String name, @Param(“age”) Integer age); 这种方式很好理解，@Param中定义的name对应了SQL中的#{name}，age对应了SQL中的#{age}。 -使用Map如下代码，通过Map对象来作为传递参数的容器： @Insert(“INSERT INTO USER(NAME, AGE) VALUES(#{name,jdbcType=VARCHAR}, #{age,jdbcType=INTEGER})”)int insertByMap(Map&lt;String, Object&gt; map); 对于Insert语句中需要的参数，我们只需要在map中填入同名的内容即可，具体如下面代码所示： Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();map.put(“name”, “CCC”);map.put(“age”, 40);userMapper.insertByMap(map); 系统会自动识别，从map中找相同的key -使用对象除了Map对象，我们也可直接使用普通的Java对象来作为查询条件的传参，比如我们可以直接使用User对象: @Insert(“INSERT INTO USER(NAME, AGE) VALUES(#{name}, #{age})”)int insertByUser(User user); 这样语句中的#{name}、#{age}就分别对应了User对象中的name和age属性。 2.增删改查（CRUD）MyBatis针对不同的数据库操作分别提供了不同的注解来进行配置，在之前的示例中演示了@Insert，下面针对User表做一组最基本的增删改查作为示例： public interface UserMapper { @Select(&quot;SELECT * FROM user WHERE name = #{name}&quot;) User findByName(@Param(&quot;name&quot;) String name); @Insert(&quot;INSERT INTO user(name, age) VALUES(#{name}, #{age})&quot;) int insert(@Param(&quot;name&quot;) String name, @Param(&quot;age&quot;) Integer age); @Update(&quot;UPDATE user SET age=#{age} WHERE name=#{name}&quot;) void update(User user); @Delete(&quot;DELETE FROM user WHERE id =#{id}&quot;) void delete(Long id); } 在完成了一套增删改查后，不妨我们试试下面的单元测试来验证上面操作的正确性： @RunWith(SpringJUnit4ClassRunner.class) @SpringApplicationConfiguration(classes = Application.class) @Transactional public class ApplicationTests { @Autowired private UserMapper userMapper; @Test @Rollback public void testUserMapper() throws Exception { // insert一条数据，并select出来验证 userMapper.insert(&quot;AAA&quot;, 20); User u = userMapper.findByName(&quot;AAA&quot;); Assert.assertEquals(20, u.getAge().intValue()); // update一条数据，并select出来验证 u.setAge(30); userMapper.update(u); u = userMapper.findByName(&quot;AAA&quot;); Assert.assertEquals(30, u.getAge().intValue()); // 删除这条数据，并select验证 userMapper.delete(u.getId()); u = userMapper.findByName(&quot;AAA&quot;); Assert.assertEquals(null, u); } } 3.返回结果的绑定 对于增、删、改操作相对变化较小。而对于“查”操作，我们往往需要进行多表关联，汇总计算等操作，那么对于查询的结果往往就不再是简单的实体对象了，往往需要返回一个与数据库实体不同的包装类，那么对于这类情况，就可以通过@Results和@Result注解来进行绑定，具体如下： @Results({ @Result(property = &quot;name&quot;, column = &quot;name&quot;), @Result(property = &quot;age&quot;, column = &quot;age&quot;) }) @Select(&quot;SELECT name, age FROM user&quot;) List&lt;User&gt; findAll(); 在上面代码中，@Result中的property属性对应User对象中的成员名，column对应SELECT出的字段名。在该配置中故意没有查出id属性，只对User对应中的name和age对象做了映射配置，这样可以通过下面的单元测试来验证查出的id为null，而其他属性不为null： @Test @Rollback public void testUserMapper() throws Exception { List&lt;User&gt; userList = userMapper.findAll(); for(User user : userList) { Assert.assertEquals(null, user.getId()); Assert.assertNotEquals(null, user.getName()); } } 具体的更多注解配置在mybatis开发文档中：http://www.mybatis.org/mybatis-3/zh/java-api.html springboot+mybatis结合mybatis-generator工具为了能最大程度的简化开发，我们如果在项目中使用springboot+mybatis的组合，那么在生成对应的文件上可以采用mybatis-generator工具。 在前言中说到，mybatis也发现了我们需要重复的去创建pojo类、mapper文件以及dao类并且需要配置它们之间的依赖关系可能会很麻烦，所以mybtis提供了一个mybatis generator工具来帮我们自动创建pojo类、mapper文件以及dao类并且会帮我们配置好它们的依赖关系，而我们只需要关系我们的业务逻辑直接使用就行了。要使用mybatis generator工具需要在pom.xml文件中添加一个generator的maven工具： 接下来就以一个项目来讲，这个generatorConfig的xml文件是最容易出错的，但是只要自己独立完成了以后就可以完美掌握了，挽歌互勉。。。 参考文章：https://blog.csdn.net/pucao_cug/article/details/64499355 （这篇是配置maven generator插件的，很有借鉴意义）https://blog.csdn.net/w410589502/article/details/70756764 第一步：我还是先把自己的POM包给你们看下 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;myself.my&lt;/groupId&gt; &lt;artifactId&gt;spring-annotation&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;properties&gt; &lt;tomcat.version&gt;7.0.69&lt;/tomcat.version&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- springBoot的web支持 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-logging-juli&lt;/artifactId&gt; &lt;version&gt;8.0.23&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;${project.artifactId}&lt;/finalName&gt; &lt;!-- plugin配置用于指定使用插件 --&gt; &lt;plugins&gt; &lt;!-- 这个plugin里面又使用dependencies引入了mysql 的驱动和mybatis的相关jar包， 这个不能省略。 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;Generate MyBatis Files&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;generate&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;generate&lt;/phase&gt; &lt;configuration&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;!-- 资源文件拷贝插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- java编译插件 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!-- spring-boot-maven-plugin插件 在SpringBoot项目中开启的方式有两种 一种是run java.application 还有一种就是这个插件开启--&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;!-- 用于在子Pom中使用,继承中使用 --&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;!-- Tomcat配置 用于远程部署java web项目--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt; &lt;/project&gt; 首先注意的:这里用到spring-boot-starter基础和spring-boot-starter-test用来做单元测试验证数据访问引入连接mysql的必要依赖mysql-connector-java引入整合MyBatis的核心依赖mybatis-spring-boot-starter这里不引入spring-boot-starter-jdbc依赖，是由于mybatis-spring-boot-starter中已经包含了此依赖 这是application.properties文件中的配置参数 `#Mybatis Generator configuration` `#dao类和实体类的位置` project =src/main/java `#mapper文件的位置` resources=src/main/java `#根据数据库中的表生成对应的pojo类、dao、mapper` jdbc_driver =com.mysql.jdbc.Driver jdbc_url=jdbc:mysql:///test jdbc_user=root jdbc_password=294823013 注意这个配置文件，跟之前的generator配置是一样的，格式千万注意，project是注明实体类生成的位置，resource是注明mapper文件生成的位置。 最容易出错的地方就在于配置这个generator的文件时，数据的格式不要写错！！！ &lt;!-- 配置生成器 --&gt; &lt;generatorConfiguration&gt; &lt;!--执行generator插件生成文件的命令： call mvn mybatis-generator:generate -e --&gt; &lt;!-- 引入配置文件 --&gt; &lt;properties resource=&quot;application.properties&quot;/&gt; &lt;!--classPathEntry:数据库的JDBC驱动,换成你自己的驱动位置 可选 --&gt; &lt;!-- 一个数据库一个context --&gt; &lt;!--defaultModelType=&quot;flat&quot; 大数据字段，不分表 --&gt; &lt;context id=&quot;MysqlTables&quot; targetRuntime=&quot;MyBatis3Simple&quot; defaultModelType=&quot;flat&quot;&gt; &lt;!-- 自动识别数据库关键字，默认false，如果设置为true，根据SqlReservedWords中定义的关键字列表； 一般保留默认值，遇到数据库关键字（Java关键字），使用columnOverride覆盖 --&gt; &lt;property name=&quot;autoDelimitKeywords&quot; value=&quot;true&quot; /&gt; &lt;!-- 生成的Java文件的编码 --&gt; &lt;property name=&quot;javaFileEncoding&quot; value=&quot;utf-8&quot; /&gt; &lt;!-- beginningDelimiter和endingDelimiter：指明数据库的用于标记数据库对象名的符号，比如ORACLE就是双引号，MYSQL默认是`反引号； --&gt; &lt;property name=&quot;beginningDelimiter&quot; value=&quot;`&quot; /&gt; &lt;property name=&quot;endingDelimiter&quot; value=&quot;`&quot; /&gt; &lt;!-- 格式化java代码 --&gt; &lt;property name=&quot;javaFormatter&quot; value=&quot;org.mybatis.generator.api.dom.DefaultJavaFormatter&quot;/&gt; &lt;!-- 格式化XML代码 --&gt; &lt;property name=&quot;xmlFormatter&quot; value=&quot;org.mybatis.generator.api.dom.DefaultXmlFormatter&quot;/&gt; &lt;plugin type=&quot;org.mybatis.generator.plugins.SerializablePlugin&quot; /&gt; &lt;plugin type=&quot;org.mybatis.generator.plugins.ToStringPlugin&quot; /&gt; &lt;!-- 注释 --&gt; &lt;commentGenerator &gt; &lt;property name=&quot;suppressAllComments&quot; value=&quot;false&quot;/&gt;&lt;!-- 是否取消注释 --&gt; &lt;property name=&quot;suppressDate&quot; value=&quot;true&quot; /&gt; &lt;!-- 是否生成注释代时间戳--&gt; &lt;/commentGenerator&gt; &lt;!-- jdbc连接参数，用application.properties文件的参数，SPEL表达式取值即可 因为这用到了mybatis generator技术来将数据库中的表自动生成mapper文件和实体类，所以直接用参数 配置数据库连接就不起作用了，必须要在这里手动配置 --&gt; &lt;jdbcConnection driverClass=&quot;${jdbc_driver}&quot; connectionURL=&quot;${jdbc_url}&quot; userId=&quot;${jdbc_user}&quot; password=&quot;${jdbc_password}&quot; /&gt; &lt;!-- 类型转换 --&gt; &lt;javaTypeResolver&gt; &lt;!-- 是否使用bigDecimal， false可自动转化以下类型（Long, Integer, Short, etc.） --&gt; &lt;property name=&quot;forceBigDecimals&quot; value=&quot;false&quot;/&gt; &lt;/javaTypeResolver&gt; &lt;!-- 最主要的4个generator配置，这很容易写错 --&gt; &lt;!-- 生成实体类地址 --&gt; &lt;javaModelGenerator targetPackage=&quot;com.mybatis.domain&quot; targetProject=&quot;${project}&quot; &gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;trimStrings&quot; value=&quot;true&quot;/&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成mapxml文件 --&gt; &lt;sqlMapGenerator targetPackage=&quot;mapper&quot; targetProject=&quot;${resources}&quot; &gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- 生成mapxml对应client，也就是接口dao --&gt; &lt;javaClientGenerator targetPackage=&quot;com.mybatis.domain&quot; targetProject=&quot;${project}&quot; type=&quot;XMLMAPPER&quot; &gt; &lt;property name=&quot;enableSubPackages&quot; value=&quot;false&quot; /&gt; &lt;/javaClientGenerator&gt; &lt;!-- table可以有多个,每个数据库中的表都可以写一个table，tableName表示要匹配的数据库表,也可以在tableName属性中通过使用%通配符来匹配所有数据库表,只有匹配的表才会自动生成文件 --&gt; &lt;table tableName=&quot;user&quot; enableCountByExample=&quot;true&quot; enableUpdateByExample=&quot;true&quot; enableDeleteByExample=&quot;true&quot; enableSelectByExample=&quot;true&quot; selectByExampleQueryId=&quot;true&quot;&gt; &lt;property name=&quot;useActualColumnNames&quot; value=&quot;false&quot; /&gt; &lt;!-- 数据库表主键 --&gt; &lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;Mysql&quot; identity=&quot;true&quot; /&gt; &lt;/table&gt; &lt;/context&gt; &lt;/generatorConfiguration&gt; 这些都配置完后，我们要用插件启动测试能否自动生成 如何使用插件呢？ 我们配置好POM中的插件后，在项目上右键–》run as–》run configrations 在点击Run Configurations以后，会弹出对话框，在对话框上找到Maven Build，然后右键并且点击new，如下图: 在新出现的界面上填写Name，Base directory，Goals这三个地方，其中Name可以随便写，Base directory是你的工程的路径，例如我的是E:\eclipse_workspace_2015\springmybatis，Goals这个地方不用变，照着图写，这个是maven插件的命令。至于Maven Runtime下拉框可以不选，也可以选择自己安装在eclipse外面的那个。 执行mybatis-generator:generate命令 点击Apply,在点击 Run，稍等一会，你可以看到generator执行成功了，如图： 生成文件： 注意配置不要错了即可，这就是spring或者springboot环境下与mybatis的整合。。。 使用数据库版本控制器创建表的过程我们在实际开发系统的时候会经常使用，但是一直有一个问题存在，由于一个系统的程序版本通过git得到了很好的版本控制，而数据库结构并没有，即使我们通过Git进行了语句的版本化，那么在各个环境的数据库中如何做好版本管理呢？下面我们就通过本文来学习一下在Spring Boot中如何使用Flyway来管理数据库的版本。 Flyway简介 Flyway是一个简单开源数据库版本控制器（约定大于配置），主要提供migrate、clean、info、validate、baseline、repair等命令。它支持SQL（PL/SQL、T-SQL）方式和Java方式，支持命令行客户端等，还提供一系列的插件支持（Maven、Gradle、SBT、ANT等）。 Flyway是一款开源的数据库版本管理工具，它更倾向于规约优于配置的方式。Flyway可以独立于应用实现管理并跟踪数据库变更，支持数据库版本自动升级，并且有一套默认的规约，不需要复杂的配置，Migrations可以写成SQL脚本，也可以写在Java代码中，不仅支持Command Line和Java API，还支持Build构建工具和Spring Boot等，同时在分布式环境下能够安全可靠地升级数据库，同时也支持失败恢复等。 避免不正当的操作损坏数据库的结构导致严重事故，目前支持的数据库主要有：Oracle, SQL Server, SQL Azure, DB2, DB2 z/OS, MySQL(including Amazon RDS), MariaDB, Google Cloud SQL, PostgreSQL(including Amazon RDS and Heroku), Redshift, Vertica, H2, Hsql, Derby, SQLite, SAP HANA, solidDB, Sybase ASE and Phoenix. 为什么使用Flyway?通常在项目开始时会针对数据库进行全局设计，但在开发产品新特性过程中，难免会遇到需要更新数据库Schema的情况，比如：添加新表，添加新字段和约束等，这种情况在实际项目中也经常发生。那么，当开发人员完成了对数据库更的SQL脚本后，如何快速地在其他开发者机器上同步？并且如何在测试服务器上快速同步？以及如何保证集成测试能够顺利执行并通过呢？假设以Spring Boot技术栈项目为例，可能有人会说，本地使用Hibernate自动更新数据库Schema模式，然后让QA或DEV到测试服务器上手动执行SQL脚本，同时可以写一个Gradle任务自动执行更新。个人觉得，对于Hibernate自动更新数据库，感觉不靠谱，不透明，控制自由度不高，而且有时很容易就会犯错，比如：用SQL创建的某个字段为VARCHAR类型，而在Entity中配置的为CHAR类型，那么在运行集成测试时，自动创建的数据库表中的字段为CHAR类型，而实际SQL脚本期望的是VARCHAR类型，虽然测试通过了，但不是期望的行为，并且在本地bootRun或服务器上运行Service时都会失败。另外，到各测试服务器上手动执行SQL脚本费时费神费力的，干嘛不自动化呢，当然，对于高级别和PROD环境，还是需要DBA手动执行的。最后，写一段自动化程序来自动执行更新，想法是很好的，那如果已经有了一些插件或库可以帮助你更好地实现这样的功能，为何不好好利用一下呢，当然，如果是为了学习目的，重复造轮子是无可厚非的。其实，以上问题可以通过Flyway工具来解决，Flyway可以实现自动化的数据库版本管理，并且能够记录数据库版本更新记录，Flyway官网对Why database migrations结合示例进行了详细的阐述，有兴趣可以参阅一下。 https://flywaydb.org/ （官方网站） 第一步：首先先加入依赖 &lt;dependency&gt; &lt;groupId&gt;org.flywaydb&lt;/groupId&gt; &lt;artifactId&gt;flyway-core&lt;/artifactId&gt; &lt;version&gt;5.0.3&lt;/version&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>spring</tag>
        <tag>springboot</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA新测试框架--testng和mockito]]></title>
    <url>%2F2018%2F04%2F05%2Fjava-%E6%96%B0%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[测试是检查应用程序的功能的过程是否按要求工作，在开发人员层面进行单元测试，在采取适当措施来测试每一个实体（类或方法）以确保最终产品符合要求。单元测试是非常必要的，这是软件公司向他们的客户提供高质量的软件产品必要前提。 较常用的测试框架是Junit4，这个是老框架，现在的互联网开发公司常用的新框架有testng和mockito两种。单元测试用例是代码的一部分从而确保代码（方法）的另一部分工作正常。要快速实现这些理想的效果，测试框架是必需的。JUnit对于Java编程语言是完美的单元测试框架。单元测试可以通过两种方式来完成： 手动测试： 手动执行测试用例，没有任何工具支持称为手动测试。费时和乏味：由于测试案例是由人力的，所以它是非常缓慢而乏味的执行。巨大的人力资源的投入：作为测试用例需要手动执行，所以更多的测试都需要手动测试。较不可靠：手动测试是为测试可能不会被精确地每次执行，因为人为错误导致不可靠。非可编程：无需编程就可以做，获取信息隐藏复杂的测试 自动测试： 以工具支持，并通过使用自动化工具则称为自动化测试执行测试用例。快速自动化运行测试用例比人力显著更快。人力资源的投入较少：测试用例是通过使用自动化工具，所以较少测试者都需要在自动化测试执行。更可靠：自动化测试在每次运行的时间进行精确的相同操作。可编程：测试人员可以编写复杂的测试，以带出隐藏的信息。 JUnit是一个Java编程语言编写的单元测试框架。 重要的是在测试驱动开发中，并且是一个家族的统称为xUnit单元测试框架中的一个。 JUnit促进“先测试再编码”，它强调建立测试数据的一段代码可以被测试，先测试再编码实现的想法。这种做法就像是“试了一下，码了一点，测试了一下，代码一点点……”这增加了程序员的工作效率和程序代码的稳定性，减少程序员的压力和花在调试的时间。 而TestNG和mockito是全新的测试体系，在junit4的基础上，很多更加简便的测试框架被开发出来，为什么介绍这两种框架，因为现在的软件开放公司都是利用junit+TestNG+mockito的方式，运用大家的优点来搭建总体的测试体系。不过似乎在功能方面，TestNG可以完全替代Junit的功能，不过Junit作为传统的xunit测试体系，在新出的Junit5测试框架也有很强的性能（在下面会做分析）。 TestngJUnit让开发人员了解测试的实用性，尤其是在单元测试这一模块上比任何其他测试框架都要简单明了。凭借一个相当简单，务实，严谨的架构，JUnit已经能够“感染”了一大批开发人员。 JUnit缺点：1.最初的设计，使用于单元测试，现在只用于各种测试。2.不能依赖测试3.配置控制欠佳(安装/拆卸)4.侵入性(强制扩展类，并以某种方式命名方法)5.静态编程模型(不必要的重新编译)6.不适合管理复杂项目应用，JUnit复杂项目中测试非常棘手。 而TetsNG是什么样的框架？TestNG是一个测试框架，其灵感来自JUnit和NUnit（也就是xUnit家族的产品），但引入了一些新的功能，使其功能更强大，使用更方便。 TestNG是一个开源自动化测试框架;TestNG表示下一代(Next Generation的首字母)。 TestNG类似于JUnit(特别是JUnit 4)，但它不是JUnit框架的扩展。它的灵感来源于JUnit。它的目的是优于JUnit，尤其是在用于测试集成多类时。 TestNG的创始人是Cedric Beust(塞德里克·博伊斯特)。 TestNG消除了大部分的旧框架的限制，使开发人员能够编写更加灵活和强大的测试。 因为它在很大程度上借鉴了Java注解(JDK5.0引入的)来定义测试，它也可以显示如何使用这个新功能在真实的Java语言生产环境中。 TestNG的特点： 1.注解2.TestNG使用Java和面向对象的功能3.支持综合类测试(例如，默认情况下，不用创建一个新的测试每个测试方法的类的实例)4.独立的编译时测试代码和运行时配置/数据信息5.灵活的运行时配置6.主要介绍“测试组”。当编译测试，只要要求TestNG运行所有的“前端”的测试，或“快”，“慢”，“数据库”等7.支持依赖测试方法，并行测试，负载测试，局部故障8.灵活的插件API9.支持多线程测试 TestNG(Next Generation)是一个测试框架，它受到JUnit和NUnit的启发，而引入了许多新的创新功能，如依赖测试，分组概念，使测试更强大，更容易做到。 它旨在涵盖所有类别的测试：单元，功能，端到端，集成等… 如何使用TestNG？展示如何开始使用TestNG单元测试框架，使用的开发环境依然是Eclipse m.3+maven3+JDK1.8这里选用TestNG的版本为TestNG 6.10版本 第一步：先去官网或者是ali的maven库中去复制TestNG 6.10的版本，然后将其放在pom中的dependencies标签下。 第二步：安装Eclipse的插件支持，在Eclipse Market中搜索“TestNG for Eclipse”，然后install，重启Eclipse。 第三步：查看是否安装成功，在eclipse的工具栏中，点击file-》new-》other，看到TestNG文件夹，即表示安装插件成功。 第四步：这就可以使用TestNG用于测试组件或者单元了，在想要测试的项目的src/test/java下新建“TestNG class”，这个类就相当于Junit的测试类，在这个类中，去写测试代码，然后右键run as–》选择“TestNG test”就可以测试了。(其实测试方法有两种，接下来就讲) TestNG的测试方式第一种直接执行：右键要执行的方法， 点Run As -&gt;TestNG Test 第二种: 通过testng.xml文件来执行. 把要执行的case, 放入testng.xml文件中。 右键点击testng.xml, 点Run As这个xml文件放在src/test/java里即可 在testng.xml中，可以控制测试用例按顺序执行。 当preserve-order=”true”是，可以保证节点下面的方法是按顺序执行的 断言（ASSERT)的用法这里介绍一下载TestNG中同样也支持断言Assert，在实际开发中用于检测错误。如何用断言Assert，断言里面又有什么方法？那个方法较为常用呢？ 在经过对其进行一定了解之后，对其作用及用法有了一定的了解，assert()的用法像是一种“契约式编程”，在我的理解中，其表达的意思就是，程序在我的假设条件下，能够正常良好的运作，其实就相当于一个if语句：其本质就是代替if-else，因为如果要用if-else来测的话，整个测试结构就会变得很复杂，可读性也不好，程序结构比较臃肿，用了Assert就好多了。 if(假设成立) { 程序正常运行； } else { 报错&amp;&amp;终止程序！（避免由程序运行引起更大的错误） } 如果断言的条件错误，则直接程序报错，可以说是一种保障正确的一种方式，org.testng.Assert 用来校验接口测试的结果，那么它提供哪些方法呢? 提供了以下这些方法：（根据实际情况需要而去测试） assertTrue 判断是否为true。assertFalse 判断是否为false。assertSame 判断引用地址是否相等。assertNotSame 判断引用地址是否不相等。assertNull 判断是否为nullassertNotNull 判断是否不为nullassertEquals 判断是否相等，Object类型的对象需要实现hashCode及equals方法，集合类型Collection/Set/Map 中的对象也需要实现hashCode及equals方法，3个double参数时比较好玩，前两个double相等，或者前两个double的差值小于传入的第三个double值，即偏移量小于多少时，认为相等。assertNotEquals 判断是否不相等assertEqualsNoOrder 判断忽略顺序是否相等 例子： StringGenerator类 public class StringGenerator { public String generate(){ return &quot;testng demo!&quot;; } } Test类 public class TestNgDemo { @Test public void test1() { StringGenerator sg=new StringGenerator(); String content=sg.generate(); /* * testng框架中的Assert断言的作用 * */ Assert.assertNotNull(content);//判断是否为空 Assert.assertEquals(content, &quot;testng demo!&quot;);//判断内容是否相等 } } 控制台输出内容的部分： [RemoteTestNG] detected TestNG version 6.10.0 [TestNG] Running: C:\Users\Administrator\AppData\Local\Temp\testng-eclipse--46238976\testng-customsuite.xml PASSED: test1 =============================================== Default test Tests run: 1, Failures: 0, Skips: 0 =============================================== =============================================== Default suite Total tests run: 1, Failures: 0, Skips: 0 =============================================== 这样就是TestNG测试框架的使用，当然这个测试框架还可以应用在很多方面，比如：TestNG预期异常测试TestNG忽略测试TestNG超时测试TestNG分组测试TestNG套件测试TestNG依赖测试TestNG参数化测试TestNG参数测试实例TestNG + Selenium负载测试TestNG + Spring集成测试 介绍TestNG的各种测试案例TestNG和Junit4相对比：JUnit 4和TestNG都是Java中非常受欢迎的单元测试框架。两种框架在功能上看起来非常相似。 哪一个更好？ 在Java项目中应该使用哪个单元测试框架？ 下面表中概括了JUnit 4和TestNG之间的功能比较。如下图所示 - 注释支持注释/注解支持在JUnit 4和TestNG中是非常类似的。 JUnit4和TestNG之间的主要注释差异是： 在JUnit 4中，我们必须声明“@BeforeClass”和“@AfterClass”方法作为静态方法。 TestNG在方法声明中更灵活，它没有这个约束。 3个额外的setUp / tearDown级别：suite和group(@Before / AfterSuite，@Before / After Test，@Before / After Group)。 JUnit 4 @BeforeClass public static void oneTimeSetUp() { // one-time initialization code System.out.println(&quot;@BeforeClass - oneTimeSetUp&quot;); } TestNG @BeforeClass public void oneTimeSetUp() { // one-time initialization code System.out.println(&quot;@BeforeClass - oneTimeSetUp&quot;); } 在JUnit 4中，注释命名约定有点混乱，例如“Before”，“After”和“Expected”，我们并不真正了解“Before”和“After”之前的内容，以及要测试中的“预期” 方法。TestiNG更容易理解，它使用类似“BeforeMethod”，“AfterMethod”和“ExpectedException”就很明了。 异常测试 “异常测试”是指从单元测试中抛出的异常，此功能在JUnit 4和TestNG中都可实现。 JUnit 4 @Test(expected = ArithmeticException.class) public void divisionWithException() { int i = 1/0; } TestNG @Test(expectedExceptions = ArithmeticException.class) public void divisionWithException() { int i = 1/0; } 忽略测试 “忽略”表示是否应该忽略单元测试，该功能在JUnit 4和TestNG中均可实现。 JUnit 4 @Ignore(&quot;Not Ready to Run&quot;) @Test public void divisionWithException() { System.out.println(&quot;Method is not ready yet&quot;); } TestNG @Test(enabled=false) public void divisionWithException() { System.out.println(&quot;Method is not ready yet&quot;); } 时间测试 “时间测试”表示如果单元测试所花费的时间超过指定的毫秒数，则测试将会终止，并将其标记为失败，此功能在JUnit 4和TestNG中均可实现。 JUnit 4 @Test(timeout = 1000) public void infinity() { while (true); } TestNG @Test(timeOut = 1000) public void infinity() { while (true); } 套件测试 “套件测试”是指捆绑几个单元测试并一起运行。 此功能在JUnit 4和TestNG中都可实现。 然而，两者都使用非常不同的方法来实现它。 JUnit 4 “@RunWith”和“@Suite”用于运行套件测试。下面的类代码表示在JunitTest5执行之后，单元测试“JunitTest1”和“JunitTest2”一起运行。 所有的声明都是在类内定义的。 @RunWith(Suite.class) @Suite.SuiteClasses({ JunitTest1.class, JunitTest2.class }) public class JunitTest5 { } TestNG XML文件用于运行套件测试。以下XML文件表示单元测试“TestNGTest1”和“TestNGTest2”将一起运行。 &lt;suite name=&quot;My test suite&quot;&gt; &lt;test name=&quot;testing&quot;&gt; &lt;classes&gt; &lt;class name=&quot;com.fsecure.demo.testng.TestNGTest1&quot; /&gt; &lt;class name=&quot;com.fsecure.demo.testng.TestNGTest2&quot; /&gt; &lt;/classes&gt; &lt;/test&gt; &lt;/suite&gt; TestNG可以做捆绑类测试，也可以捆绑方法测试。 凭借TestNG独特的“分组”概念，每种方法都可以与一个组合相结合，可以根据功能对测试进行分类(分组)。 例如， 下面是一个有四个方法的类，三个组(method1，method2和method3) @Test(groups=&quot;method1&quot;) public void testingMethod1() { System.out.println(&quot;Method - testingMethod1()&quot;); } @Test(groups=&quot;method2&quot;) public void testingMethod2() { System.out.println(&quot;Method - testingMethod2()&quot;); } @Test(groups=&quot;method1&quot;) public void testingMethod1_1() { System.out.println(&quot;Method - testingMethod1_1()&quot;); } @Test(groups=&quot;method4&quot;) public void testingMethod4() { System.out.println(&quot;Method - testingMethod4()&quot;); } 使用以下XML文件，可以仅使用组“method1”执行单元测试。 &lt;suite name=&quot;My test suite&quot;&gt; &lt;test name=&quot;testing&quot;&gt; &lt;groups&gt; &lt;run&gt; &lt;include name=&quot;method1&quot;/&gt; &lt;/run&gt; &lt;/groups&gt; &lt;classes&gt; &lt;class name=&quot;com.fsecure.demo.testng.TestNGTest5_2_0&quot; /&gt; &lt;/classes&gt; &lt;/test&gt; &lt;/suite&gt; 通过“分组”测试概念，集成测试的可能性是无限制的。 例如，我们只能从所有单元测试类中测试“DatabaseFuntion”分组。 参数化测试 “参数化测试”是指单位测试参数值的变化。 此功能在JUnit 4和TestNG中都实现。 然而，两者都使用非常不同的方法来实现它。 JUnit 4 “@RunWith”和“@Parameter”用于提供单元测试的参数值，@Parameters必须返回List []，参数将作为参数传入类构造函数。 @RunWith(value = Parameterized.class) public class JunitTest6 { private int number; public JunitTest6(int number) { this.number = number; } @Parameters public static Collection&lt;Object[]&gt; data() { Object[][] data = new Object[][] { { 1 }, { 2 }, { 3 }, { 4 } }; return Arrays.asList(data); } @Test public void pushTest() { System.out.println(&quot;Parameterized Number is : &quot; + number); } } 这里有很多限制，我们必须遵循“JUnit”的方式来声明参数，并且必须将参数传递给构造函数才能初始化类成员作为测试的参数值。参数类的返回类型为“List []”，数据已被限制为String或用于测试的原始类型值。 TestNG XML文件或“@DataProvider”用于提供不同参数进行测试。 用于参数化测试的XML文件 - 只有“@Parameters”在需要参数测试的方法中声明，参数化数据将在TestNG的XML配置文件中提供。 通过这样做，我们可以使用不同数据集的单个测试用例，甚至获得不同的结果。 另外，即使是最终用户，QA还是QE都可以在XML文件中提供自己的数据进行测试。 public class TestNGTest6_1_0 { @Test @Parameters(value=&quot;number&quot;) public void parameterIntTest(int number) { System.out.println(&quot;Parameterized Number is : &quot; + number); } } XML文件的内容如下 - &lt;suite name=&quot;My test suite&quot;&gt; &lt;test name=&quot;testing&quot;&gt; &lt;parameter name=&quot;number&quot; value=&quot;2&quot;/&gt; &lt;classes&gt; &lt;class name=&quot;com.fsecure.demo.testng.TestNGTest6_0&quot; /&gt; &lt;/classes&gt; &lt;/test&gt; &lt;/suite&gt; @DataProvider用于参数化测试 将数据值拉入XML文件可能非常方便，但测试偶尔会需要复杂的类型，这些类型不能被表示为一个字符串或一个原始类型值。 TestNG使用@DataProvider注解来处理这种情况，这有助于将复杂参数类型映射到测试方法。 @DataProvider for Vector，String或Integer作为参数，参考如下代码 - @Test(dataProvider = &quot;Data-Provider-Function&quot;) public void parameterIntTest(Class clzz, String[] number) { System.out.println(&quot;Parameterized Number is : &quot; + number[0]); System.out.println(&quot;Parameterized Number is : &quot; + number[1]); } //This function will provide the patameter data @DataProvider(name = &quot;Data-Provider-Function&quot;) public Object[][] parameterIntTestProvider() { return new Object[][]{ {Vector.class, new String[] {&quot;java.util.AbstractList&quot;, &quot;java.util.AbstractCollection&quot;}}, {String.class, new String[] {&quot;1&quot;, &quot;2&quot;}}, {Integer.class, new String[] {&quot;1&quot;, &quot;2&quot;}} }; } @DataProvider作为对象的参数 “TestNGTest6_3_0”是一个简单的对象，只需使用get/set方法进行演示。 @Test(dataProvider = &quot;Data-Provider-Function&quot;) public void parameterIntTest(TestNGTest6_3_0 clzz) { System.out.println(&quot;Parameterized Number is : &quot; + clzz.getMsg()); System.out.println(&quot;Parameterized Number is : &quot; + clzz.getNumber()); } //This function will provide the patameter data @DataProvider(name = &quot;Data-Provider-Function&quot;) public Object[][] parameterIntTestProvider() { TestNGTest6_3_0 obj = new TestNGTest6_3_0(); obj.setMsg(&quot;Hello&quot;); obj.setNumber(123); return new Object[][]{ {obj} }; } TestNG的参数化测试非常用户友好和灵活(在XML文件或类内)。 它可以支持许多复杂的数据类型作为参数值，可能性是无限的。 如上例所示，我们甚至可以传入我们自己的对象(TestNGTest6_3_0)进行参数化测试 依赖性测试 “参数化测试”表示方法是依赖性测试，它将在所需方法之前执行。 如果依赖方法失败，则所有后续测试将会被跳过，不会被标记为失败。 JUnit 4 JUnit框架着重于测试隔离; 目前它不支持此功能。 TestNG 它使用“dependOnMethods”来实现依赖测试如下 - @Test public void method1() { System.out.println(&quot;This is method 1&quot;); } @Test(dependsOnMethods={&quot;method1&quot;}) public void method2() { System.out.println(&quot;This is method 2&quot;); } “method2()”只有在“method1()”运行成功的情况下才会执行，否则“method2()”将跳过测试。 结论 在考虑所有功能比较之后，建议使用TestNG作为Java项目的核心单元测试框架，因为TestNG在参数化测试，依赖测试和套件测试(分组概念)方面更加突出。 TestNG用于高级测试和复杂集成测试。 它的灵活性对于大型测试套件尤其有用。 此外，TestNG还涵盖了整个核心的JUnit4功能。这样说来，好像也没有理由使用JUnit了。 TestNG与spring或spring boot结合mockitomockito的官网：http://site.mockito.org/在实际项目中写单元测试的过程中我们会发现需要测试的类有很多依赖，这些依赖项又会有依赖，导致在单元测试代码里几乎无法完成构建，尤其是当依赖项尚未构建完成时会导致单元测试无法进行。为了解决这类问题我们引入了Mock的概念，简单的说就是模拟这些需要构建的类或者资源，提供给需要测试的对象使用。业内的Mock工具有很多，也已经很成熟了，这里我们将直接使用最流行的Mockito进行实战演练，完成mockito教程。 EasyMock 以及 Mockito 都因为可以极大地简化单元测试的书写过程而被许多人应用在自己的工作中，但是这两种 Mock 工具都不可以实现对静态函数、构造函数、私有函数、Final 函数以及系统函数的模拟，但是这些方法往往是我们在大型系统中需要的功能。 如何使用Mockito？第一步：在项目的pom中添加依赖，我用的阿里库的mockito版本为2.9.0的版本，ali的maven库中找到mockito-core的依赖jar，导入项目pom也可以直接去官网下载相应的jar包，更新到了2.17版本。 第二步：另外Mockito需要Junit配合使用，在Pom文件中同样引入：junit4的版本。 第三步：然后为了使代码更简洁，最好在测试类中导入静态资源，还有为了使用常用的junit关键字，也要引入junit的两个类Before和Test： import static org.mockito.Mockito.*; import static org.junit.Assert.*; import org.junit.Before; import org.junit.Test; 第四步：然后就可以编写测试代码进行使用了，这里用junit的测试步骤来测试即可，mockito主要的功能就是模拟。。。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-理解文章汇总（不断更新）]]></title>
    <url>%2F2018%2F04%2F03%2Fjava-se%2F</url>
    <content type="text"><![CDATA[JAVA基础总结，持续更新，以防忘记。 接口JDK8后的接口新规在jdk8之前，interface之中可以定义变量和方法，变量必须是public、static、final的，方法必须是public、abstract的。由于这些修饰符都是默认的，所以在JDK8之前，下面的写法都是等价的。 public interface JDK8BeforeInterface { public static final int field1 = 0; int field2 = 0; public abstract void method1(int a) throws Exception; void method2(int a) throws Exception; } 接口变量的默认修饰是public static final，就是自动声明为常量，这些都是隐式默认的规则，所以直接用public声明也是一样的效果，变量必须是常量，而方法也必须的抽象方法，且限定修饰符必须是public，这些也是隐式规定的。 JDK8及以后，允许我们在接口中定义static方法和default方法。也就是静态方法和默认方法。static方法和default方法都可以直接在接口里写方法体（方法的具体逻辑）了。这就是java变相的让接口能够结合抽象类的功能。 public interface JDK8Interface { // static修饰符定义静态方法 static void staticMethod() { System.out.println(&quot;接口中的静态方法&quot;); } // default修饰符定义默认方法 default void defaultMethod() { System.out.println(&quot;接口中的默认方法&quot;); } } 再定义一个接口的实现类： public class JDK8InterfaceImpl implements JDK8Interface { //实现接口后，因为默认方法不是抽象方法，所以可以不重写，但是如果开发需要，也可以重写 } default修饰的方法可以不重写，根据其实现类的对象来调用这个接口的default方法而静态方法则是只能这个接口名来直接调用。静态方法，只能通过接口名调用，不可以通过实现类的类名或者实现类的对象调用。default方法，只能通过接口实现类的对象来调用。 public class Main { public static void main(String[] args) { // static方法必须通过接口类调用 JDK8Interface.staticMethod(); //default方法必须通过实现类的对象调用 new JDK8InterfaceImpl().defaultMethod(); } } 当然如果接口中的默认方法不能满足某个实现类需要，那么实现类可以覆盖默认方法。 public class AnotherJDK8InterfaceImpl implements JDK8Interface { // 签名跟接口default方法一致,但是不能再加default修饰符 @Override public void defaultMethod() { System.out.println(&quot;接口实现类覆盖了接口中的default&quot;); } } 由于java支持一个实现类可以实现多个接口，如果多个接口中存在同样的static和default方法会怎么样呢？如果有两个接口中的静态方法一模一样，并且一个实现类同时实现了这两个接口，此时并不会产生错误，因为jdk8只能通过接口类调用接口中的静态方法，所以对编译器来说是可以区分的。但是如果两个接口中定义了一模一样的默认方法，并且一个实现类同时实现了这两个接口，那么必须在实现类中重写默认方法，否则编译失败。 例如JDK8Interface1也有一个defaultMethod() public interface JDK8Interface1 { // static修饰符定义静态方法 static void staticMethod() { System.out.println(&quot;JDK8Interface1接口中的静态方法&quot;); } // default修饰符定义默认方法 default void defaultMethod() { System.out.println(&quot;JDK8Interface1接口中的默认方法&quot;); } } 必须要覆盖defaultMethod()，然后调用的时候JVM才不会混乱，不然JVM不知道调用哪个default方法 public class JDK8InterfaceImpl implements JDK8Interface,JDK8Interface1 { // 由于JDK8Interface和JDK8Interface1中default方法一样,所以这里必须覆盖 @Override public void defaultMethod() { System.out.println(&quot;接口实现类覆盖了接口中的default&quot;); } } 对象调用的是重写后的defaultMethod()。 public class Main { public static void main(String[] args) { JDK8Interface.staticMethod(); JDK8Interface1.staticMethod(); new JDK8InterfaceImpl().defaultMethod(); } } 多态多态是同一个行为具有多个不同表现形式或形态的能力。多态就是同一个接口，使用不同的实例而执行不同操作，如图所示： 多态性是对象多种表现形式的体现。 现实中，比如我们按下 F1 键这个动作：如果当前在 Flash 界面下弹出的就是 AS 3 的帮助文档；如果当前在 Word 下弹出的就是 Word 帮助；在 Windows 下弹出的就是 Windows 帮助和支持。 同一个事件发生在不同的对象上会产生不同的结果。 多态的优点1.消除类型之间的耦合关系2.可替换性3.可扩充性4.接口性5.灵活性6.简化性 多态存在的三个必要条件 1.继承2.重写3.父类引用指向子类对象 比如：Parent p = new Child();//这就是典型的多态 当使用多态方式调用方法时，首先检查父类中是否有该方法，如果没有，则编译错误；如果有，再去调用子类的同名方法。多态的好处：可以使程序有良好的扩展，并可以对所有类的对象进行通用处理。以下是一个多态实例的演示，详细说明请看注释： public class Test { public static void main(String[] args) { show(new Cat()); // 以 Cat 对象调用 show 方法 show(new Dog()); // 以 Dog 对象调用 show 方法 Animal a = new Cat(); // 向上转型 a.eat(); // 调用的是 Cat 的 eat Cat c = (Cat)a; // 向下转型 c.work(); // 调用的是 Cat 的 work } public static void show(Animal a) { a.eat(); // 类型判断 if (a instanceof Cat) { // 猫做的事情 Cat c = (Cat)a; c.work(); } else if (a instanceof Dog) { // 狗做的事情 Dog c = (Dog)a; c.work(); } } } abstract class Animal { abstract void eat(); } class Cat extends Animal { public void eat() { System.out.println(&quot;吃鱼&quot;); } public void work() { System.out.println(&quot;抓老鼠&quot;); } } class Dog extends Animal { public void eat() { System.out.println(&quot;吃骨头&quot;); } public void work() { System.out.println(&quot;看家&quot;); } } 执行以上程序，输出结果为： 吃鱼 抓老鼠 吃骨头 看家 吃鱼 抓老鼠 java多态，如何理解父类引用指向子类对象 要理解多态性，首先要知道什么是“向上转型”。 我定义了一个子类Cat，它继承了Animal类，那么后者就是前者是父类。我可以通过 Cat c = new Cat(); 实例化一个Cat的对象，这个不难理解。 但当我这样定义时： Animal a = new Cat(); 表示定义了一个Animal类型的引用，指向新建的Cat类型的对象。由于Cat是继承自它的父类Animal，所以Animal类型的引用是可以指向Cat类型的对象的。 那么这样做有什么意义呢？因为子类是对父类的一个改进和扩充，所以一般子类在功能上较父类更强大，属性较父类更独特， 定义一个父类类型的引用指向一个子类的对象既可以使用子类强大的功能，又可以抽取父类的共性。 所以，父类类型的引用可以调用父类中定义的所有属性和方法，而对于子类中定义而父类中没有的方法，它是无可奈何的； 同时，父类中的一个方法只有在父类中定义而在子类中没有重写的情况下，才可以被父类类型的引用调用； 对于父类中定义的方法，如果子类中重写了该方法，那么父类类型的引用将会调用子类中的这个方法，这就是动态连接。也可以叫做动态绑定。 动态绑定是指”在执行期间（而非编译期间）“判断所引用对象的实际类型，根据实际的类型调用其相应的方法。 看下面这段程序： class Father { public void func1() { func2(); } // 这是父类中的func2()方法，因为下面的子类中重写了该方法 ，所以在父类类型的引用中调用时，这个方法将不再有效，取而代之的是将调用子类中重写的func2()方法 public void func2() { System.out.println(&quot;AAA&quot;); } } class Child extends Father { // func1(int i)是对func1()方法的一个重载 由于在父类中没有定义这个方法，所以它不能被父类类型的引用调用 所以在下面的main方法中child.func1(68)是不对的 public void func1(int i) { System.out.println(&quot;BBB&quot;); } // func2()重写了父类Father中的func2()方法 如果父类类型的引用中调用了func2()方法，那么必然是子类中重写的这个方法 public void func2() { System.out.println(&quot;CCC&quot;); } } public class PolymorphismTest { public static void main(String[] args) { Father child = new Child(); child.func1();// 打印结果将会是什么？ } } 上面的程序是个很典型的多态的例子。子类Child继承了父类Father，并重载了父类的func1()方法，重写了父类的func2()方法。重载后的 func1(int i)和func1()不再是同一个方法，由于父类中没有func1(int i)，那么，父类类型的引用child就不能调用func1(int i)方法。而子类重写了func2()方法，那么父类类型的引用child在调用该方法时将会调用子类中重写的func2() } } 那么该程序将会打印出什么样的结果呢？ 很显然，应该是“CCC”。 对于多态，可以总结它为： 一、使用父类类型的引用指向子类的对象； 二、该引用只能调用父类中定义的方法和变量； 三、如果子类中重写了父类中的一个方法，那么在调用这个方法的时候，将会调用子类中的这个方法；（动态连接、动态调用） 四、变量不能被重写（覆盖），”重写“的概念只针对方法，如果在子类中”重写“了父类中的变量，那么在编译时会报错。 多态的3个必要条件：1.继承 2.重写 3.父类引用指向子类对象。 向上转型： Person p = new Man() ; //向上转型不需要强制类型转化向下转型： Man man = (Man)new Person() ; //必须强制类型转化 虚方法我们将介绍在Java中，当设计类时，被重写的方法的行为怎样影响多态性。我们已经讨论了方法的重写，也就是子类能够重写父类的方法。当子类对象调用重写的方法时，调用的是子类的方法，而不是父类中被重写的方法。要想调用父类中被重写的方法，则必须使用关键字super。 /* 文件名 : Employee.java */ public class Employee { private String name; private String address; private int number; public Employee(String name, String address, int number) { System.out.println(&quot;Employee 构造函数&quot;); this.name = name; this.address = address; this.number = number; } public void mailCheck() { System.out.println(&quot;邮寄支票给： &quot; + this.name + &quot; &quot; + this.address); } public String toString() { return name + &quot; &quot; + address + &quot; &quot; + number; } public String getName() { return name; } public String getAddress() { return address; } public void setAddress(String newAddress) { address = newAddress; } public int getNumber() { return number; } } 假设下面的类继承Employee类： /* 文件名 : Salary.java */ public class Salary extends Employee { private double salary; // 全年工资 public Salary(String name, String address, int number, double salary) { super(name, address, number); setSalary(salary); } public void mailCheck() { System.out.println(&quot;Salary 类的 mailCheck 方法 &quot;); System.out.println(&quot;邮寄支票给：&quot; + getName() + &quot; ，工资为：&quot; + salary); } public double getSalary() { return salary; } public void setSalary(double newSalary) { if(newSalary &gt;= 0.0) { salary = newSalary; } } public double computePay() { System.out.println(&quot;计算工资，付给：&quot; + getName()); return salary/52; } } 现在我们仔细阅读下面的代码，尝试给出它的输出结果： /* 文件名 : VirtualDemo.java */ public class VirtualDemo { public static void main(String [] args) { Salary s = new Salary(&quot;员工 A&quot;, &quot;北京&quot;, 3, 3600.00); Employee e = new Salary(&quot;员工 B&quot;, &quot;上海&quot;, 2, 2400.00); System.out.println(&quot;使用 Salary 的引用调用 mailCheck -- &quot;); s.mailCheck(); System.out.println(&quot;\n使用 Employee 的引用调用 mailCheck--&quot;); e.mailCheck(); } } 以上实例编译运行结果如下： Employee 构造函数 Employee 构造函数 使用 Salary 的引用调用 mailCheck -- Salary 类的 mailCheck 方法 邮寄支票给：员工 A ，工资为：3600.0 使用 Employee 的引用调用 mailCheck-- Salary 类的 mailCheck 方法 邮寄支票给：员工 B ，工资为：2400.0 例子解析 实例中，实例化了两个 Salary 对象：一个使用 Salary 引用 s，另一个使用 Employee 引用 e。当调用 s.mailCheck() 时，编译器在编译时会在 Salary 类中找到 mailCheck()，执行过程 JVM 就调用 Salary 类的 mailCheck()。因为 e 是 Employee 的引用，所以调用 e 的 mailCheck() 方法时，编译器会去 Employee 类查找 mailCheck() 方法 。在编译的时候，编译器使用 Employee 类中的 mailCheck() 方法验证该语句， 但是在运行的时候，Java虚拟机(JVM)调用的是 Salary 类中的 mailCheck() 方法。以上整个过程被称为虚拟方法调用，该方法被称为虚拟方法。Java中所有的方法都能以这种方式表现，因此，重写的方法能在运行时调用，不管编译的时候源代码中引用变量是什么数据类型。 多态的实现方式： 方式一：重写：这个内容已经在上一章节详细讲过，就不再阐述，详细可访问：Java 重写(Override)与重载(Overload)。方式二：接口 生活中的接口最具代表性的就是插座，例如一个三接头的插头都能接在三孔插座中，因为这个是每个国家都有各自规定的接口规则，有可能到国外就不行，那是因为国外自己定义的接口类型。 java中的接口类似于生活中的接口，就是一些方法特征的集合，但没有方法的实现。具体可以看 java接口 这一章节的内容。方式三：抽象类和抽象方法]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javaSE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ajax技术浅谈]]></title>
    <url>%2F2018%2F03%2F31%2Fjava-ajax%2F</url>
    <content type="text"><![CDATA[什么是Ajax？AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML），是指一种创建交互式网页应用的网页开发技术。 AJAX 是一种用于创建快速动态网页的技术。 通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用 AJAX）如果需要更新内容，必须重载整个网页页面。AJAX 技术的广泛使用，对B/S模式应用慢慢取代了桌面软件起到了很大的推动作用。 同步and异步？异步传输是面向字符的传输，它的单位是字符；而同步传输是面向比特的传输，它的单位是桢，它传输的时候要求接受方和发送方的时钟是保持一致的。 异步传输具体来说，异步传输是将比特分成小组来进行传送。一般每个小组是一个8位字符，在每个小组的头部和尾部都有一个开始位和一个停止位，它在传送过程中接收方和发送方的时钟不要求一致，也就是说，发送方可以在任何时刻发送这些小组，而接收方并不知道它什么时候到达。 一个最明显的例子就是计算机键盘和主机的通信，按下一个键的同时向主机发送一个8比特位的ASCII代 码，键盘可以在任何时刻发送代码，这取决于用户的输入速度，内部的硬件必须能够在任何时刻接收一个键入的字符。这是一个典型的异步传输过程。 异步传输存在 一个潜在的问题，即接收方并不知道数据会在什么时候到达。在它检测到数据并做出响应之前，第一个比特已经过去了。这就像有人出乎意料地从后面走上来跟你说 话，而你没来得及反应过来，漏掉了最前面的几个词。因此，每次异步传输的信息都以一个起始位开头，它通知接收方数据已经到达了，这就给了接收方响应、接收 和缓存数据比特的时间；在传输结束时，一个停止位表示该次传输信息的终止。按照惯例，空闲（没有传送数据）的线路实际携带着一个代表二进制1的信号。步传输的开始位使信号变成0，其他的比特位使信号随传输的数据信息而变化。最后，停止位使信号重新变回1，该信号一直保持到下一个开始位到达。例如在键盘上数字“1”，按照8比特位的扩展ASCII编码，将发送“00110001”，同时需要在8比特位的前面加一个起始位，后面一个停止位。 同步传输同步传输的比特分组要大得多。它不是独立地发送每个字符，每个字符都有自己的开始位和停止位，而是把它们组合起来一起发送。我们将这些组合称为数据帧，或简称为帧。 数据帧的第一部分包含一组同步字符，它是一个独特的比特组合，类似于前面提到的起始位，用于通知接收方一个帧已经到达，但它同时还能确保接收方的采样速度和比特的到达速度保持一致，使收发双方进入同步。 帧的最后一部分是一个帧结束标记。与同步字符一样，它也是一个独特的比特串，类似于前面提到的停止位，用于表示在下一帧开始之前没有别的即将到达的数据了。 同步传输通常要比异步传输快速得多。接收方不必对每个字符进行开始和停止的操作。一旦检测到帧同步字符，它就在接下来的数据到达时接收它们。另外，同步传输的开销也比较少。例如，一个典型的帧可能有500字节（即4000比特）的数据，其中可能只包含100比特的开销。这时，增加的比特位使传输的比特总数增加2.5%，这与异步传输中25 %的增值要小得多。随着数据帧中实际数据比特位的增加，开销比特所占的百分比将相应地减少。但是，数据比特位越长，缓存数据所需要的缓冲区也越大，这就限制了一个帧的大小。另外，帧越大，它占据传输媒体的连续时间也越长。在极端的情况下，这将导致其他用户等得太久。 了解了同步和异步的概念之后，大家应该对ajax为什么可以提升用户体验应该比较清晰了，它是利用异步请求方式的。打个比方，如果现在你家里所在的小区因 某种情况而面临停水，现在有关部门公布了两种方案，一是完全停水8个小时，在这8个小时内完全停水，8个小时后恢复正常。二是不完全停水10 个小时，在这10个小时内水没有完全断，只是流量比原来小了很多，在10个小时后恢复正常流量，那么，如果是你你会选择哪种方式呢？显然是后者。 AJAX 所包含的技术大家都知道ajax并非一种新的技术，而是几种原有技术的结合体。它由下列技术组合而成。 1.使用CSS和XHTML来表示。2.使用DOM模型来交互和动态显示。3.使用XMLHttpRequest来和服务器进行异步通信。4.使用javascript来绑定和调用。 在上面几中技术中，除了XmlHttpRequest对象以外，其它所有的技术都是基于web标准并且已经得到了广泛使用的，XMLHttpRequest虽然目前还没有被W3C所采纳，但是它已经是一个事实的标准，因为目前几乎所有的主流浏览器都支持它。 XMLHttpRequest 对象Ajax的原理简单来说通过XmlHttpRequest对象来向服务器发异步请求，从服务器获得数据，然后用javascript来操作DOM而更新页面。这其中最关键的一步就是从服务器获得请求数据。要清楚这个过程和原理，我们必须对 XMLHttpRequest有所了解。 XMLHttpRequest是ajax的核心机制，它是在IE5中首先引入的，是一种支持异步请求的技术。简单的说，也就是javascript可以及时向服务器提出请求和处理响应，而不阻塞用户。达到无刷新的效果。所以我们先从XMLHttpRequest讲起，来看看它的工作原理。首先，我们先来看看XMLHttpRequest这个对象的属性。它的属性有： onreadystatechange 每次状态改变所触发事件的事件处理程序。responseText 从服务器进程返回数据的字符串形式。responseXML 从服务器进程返回的DOM兼容的文档数据对象。status 从服务器返回的数字代码，比如常见的404（未找到）和200（已就绪）status Text 伴随状态码的字符串信息readyState 对象状态值 0 (未初始化) 对象已建立，但是尚未初始化（尚未调用open方法） 1 (初始化) 对象已建立，尚未调用send方法 2 (发送数据) send方法已调用，但是当前的状态及http头未知 3 (数据传送中) 已接收部分数据，因为响应及http头不全，这时通过responseBody和responseText获取部分数据会出现错误， 4 (完成) 数据接收完毕,此时可以通过通过responseXml和responseText获取完整的回应数据通常都是用readystate==4这个状态码判断传输是否完成。 如何使用Ajax技术呢？这里通常我们开发WEB项目若是用到Ajax技术，首先先要创建XMLHttpRequest这个对象，以为这是一个公共代码，我们可以直接在单独的js脚本文件中创建这个对象，以便所有的html5中都可以使用这个对象，只需要调用这个js脚本即可。 //获得ajax对象 function getXhr(){ var xhr=null; if(window.XMLHttpRequest){ //非ie浏览器 xhr=new XMLHttpRequest(); }else{ //ie浏览器 xhr=new ActiveXObject( &quot;Microsoft.XMLHttp&quot;); } return xhr; } 因为IE和其他浏览器的纷争，这里是W3C制定的统一规范，本来是微软发明的，但是微软不肯将自己的技术交给W3C，因此出现了IE浏览器要和其他浏览器做一个if判定。（中间微软和W3C关于ajax技术的所有权可以去看下相关报道） &lt;script type=&quot;text/javascript&quot;&gt; function check_adminCode(){ //step1:先获得ajax对象 //在ajax.js文件中，外部引入 var xhr=getXhr(); //弹出这个xhr对象信息,测试js代码是否正确 //当触发事件的时候会调用这个函数 //alert(xhr); //step2:发送请求 //a.准备工作(请求方式,发送地址?数据,是否异步) //因为是get方式发送 //所以路径后面是要传递的数据 //$F是取得根据该节点id取得对应的值 var uri=&apos;check_admin.do?adminCode=&apos; + $F(&apos;adminCode&apos;); //encodeURI是javascript函数， xhr.open(&apos;get&apos;,encodeURI(uri),true); //b.绑定事件处理函数 //用于处理服务器返回数据并用于展现 //因为事件处理函数也需要xhr对象进行判断 //所以这里的事件处理函数直接用匿名函数来写就好 xhr.onreadystatechange=function(){ //step3是编写服务器端的相关带代码 //step4编写事件处理函数 if(xhr.readyState==4 &amp;&amp; xhr.status==200){ //通信状态为4，表示其完全收到服务端传来的数据 //status为200，即表示成功 var res=xhr.responseText;//返回的是文本 //并将数据进行展示 // alert(res);弹出警告的方式不友好 //先找到要显示的位置span标签 //innerHTML是输出文本信息 $(&apos;adminCode_msg&apos;).innerHTML=res; } }; //c.发送 //get方式的参数是null，post则是数据 xhr.send(null); } &lt;/script&gt; 以上这些代码是根据传统的模式，调用XMLHttpRequest对象的属性去赋值，显然太过麻烦。 一般是采用和Jquery框架结合的方法：通过jQuery内置的$.ajax({“attribute”:”value”…});的写法即可完成异步传输。 function quoto(){ //利用jQuery提供的方法来向服务器发送请求 $.ajax({ //url发送请求，然后服务器就会返回数据 &quot;url&quot;:&quot;quoto.do&quot;, &quot;type&quot;:&quot;post&quot;, &quot;dataType&quot;:&quot;json&quot;, &quot;success&quot;:function(stocks){ //事件处理函数 //dataType会自动将json字符串转换为js对象 //success指定的这个匿名函数中的参数 //就是经过jquery转换的那个javascript对象 //追加数据之前要先清空之前的表格内容tbody //不是remove而是empty。数据清空 $(&apos;#tb1&apos;).empty(); //因为服务器传来的数据是一个数组 //所以需要遍历输出 for(i=0;i&lt;stocks.length;i++){ //注意js中没有int类型，统统都用var //所以用for循环的时候要注意 var s=stocks[i]; //更新表格 //往tbody标签中插入数据,循环插入 $(&apos;#tb1&apos;).append( &apos;&lt;tr&gt;&lt;td&gt;&apos; + s.code + &apos;&lt;/td&gt;&lt;td&gt;&apos; + s.name + &apos;&lt;/td&gt;&lt;td&gt;&apos; + s.price + &apos;&lt;/td&gt;&lt;/tr&gt;&apos;); } } }); } AJAX 的缺点AJAX的优点不言而喻。 下面所阐述的ajax的缺陷都是它先天所产生的。1、ajax干掉了back按钮，即对浏览器后退机制的破坏。后退按钮是一个标准的web站点的重要功能，但是它没法和js进行很好的合作。这是ajax所带来的一个比较严重的问题，因为用户往往是希望能够通过后退来取消前一次操作的。那么对于这个问题有没有办法？答案是肯定的，用过Gmail的知道，Gmail下面采用的ajax技术解决了这个问题，在Gmail下面是可以后退的，但是，它也并不能改变ajax的机制，它只是采用的一个比较笨但是有效的办法，即用户单击后退按钮访问历史记录时，通过创建或使用一个隐藏的IFRAME来重现页面上的变更。（例如，当用户在Google Maps中单击后退时，它在一个隐藏的IFRAME中进行搜索，然后将搜索结果反映到Ajax元素上，以便将应用程序状态恢复到当时的状态。）但是，虽然说这个问题是可以解决的，但是它所带来的开发成本是非常高的，和ajax框架所要求的快速开发是相背离的。这是ajax所带来的一个非常严重的问题。 2、安全问题技术同时也对IT企业带来了新的安全威胁，ajax技术就如同对企业数据建立了一个直接通道。这使得开发者在不经意间会暴露比以前更多的数据和服务器逻辑。ajax的逻辑可以对客户端的安全扫描技术隐藏起来，允许黑客从远端服务器上建立新的攻击。还有ajax也难以避免一些已知的安全弱点，诸如跨站点脚步攻击、SQL注入攻击和基于credentials的安全漏洞等。 3、对搜索引擎的支持比较弱。 4、破坏了程序的异常机制。至少从目前看来，像ajax.dll，ajaxpro.dll这些ajax框架是会破坏程序的异常机制的。关于这个问题，我曾经在开发过程中遇到过，但是查了一下网上几乎没有相关的介绍。后来我自己做了一次试验，分别采用ajax和传统的form提交的模式来删除一条数据……给我们的调试带来了很大的困难。 5、另外，像其他方面的一些问题，比如说违背了url和资源定位的初衷。例如，我给你一个url地址，如果采用了ajax技术，也许你在该url地址下面看到的和我在这个url地址下看到的内容是不同的。这个和资源定位的初衷是相背离的。 6、一些手持设备（如手机、PDA等）现在还不能很好的支持ajax，比如说我们在手机的浏览器上打开采用ajax技术的网站时，它目前是不支持的，当然，这个问题和我们没太多关系。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javascript</tag>
        <tag>ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈SAAS]]></title>
    <url>%2F2018%2F03%2F31%2FSAAS%2F</url>
    <content type="text"><![CDATA[SaasSaaS是Software-as-a-Service（软件即服务）的简称，随着互联网技术的发展和应用软件的成熟， 在21世纪开始兴起的一种完全创新的软件应用模式。它与“on-demand software”（按需软件)，the application service provider(ASP，应用服务提供商)，hosted software(托管软件)所具有相似的含义。它是一种通过Internet提供软件的模式，厂商将应用软件统一部署在自己的服务器上，客户可以根据自己实际需求，通过互联网向厂商定购所需的应用软件服务，按定购的服务多少和时间长短向厂商支付费用，并通过互联网获得厂商提供的服务。 用户不用再购买软件，而改用向提供商租用基于Web的软件，来管理企业经营活动，且无需对软件进行维护，服务提供商会全权管理和维护软件，软件厂商在向客户提供互联网应用的同时，也提供软件的离线操作和本地数据存储，让用户随时随地都可以使用其定购的软件和服务。对于许多小型企业来说，SaaS是采用先进技术的最好途径，它消除了企业购买、构建和维护基础设施和应用程序的需要。 SaaS 应用软件的价格通常为“全包”费用，囊括了通常的应用软件许可证费、软件维护费以及技术支持费，将其统一为每个用户的月度租用费。 对于广大中小型企业来说，SaaS是采用先进技术实施信息化的最好途径。但SaaS绝不仅仅适用于中小型企业，所有规模的企业都可以从SaaS中获利。 SaaS有什么特别之处呢？其实在云计算还没有盛行的时代，我们已经接触到了一些SaaS的应用，通过浏览器我们可以使用Google、百度等搜索系统，可以使用E-mail，我们不需要在自己的电脑中安装搜索系统或者邮箱系统。典型的例子，我们在电脑上使用的Word、Excel、PowerPoint等办公软件，这些都是需要在本地安装才能使用的；而在GoogleDocs（DOC、XLS、ODT、ODS、RTF、CSV和PPT等）、MicrosoftOfficeOnline（WordOnline、ExcelOnline、PowerPointOnline和OneNoteOnline）网站上，无需在本机安装，打开浏览器，注册帐号，可以随时随地通过网络来使用这些软件编辑、保存、阅读自己的文档。对于用户只需要自由自在地使用，不需要自己去升级软件、维护软件等操作。由用户自主维护转化为公司为这些项目服务提供全套托管，用户不用担心升级软件所需要的东西，无需维护产品的安装包，全部操作都在WEB端进行。用户购买软件产品转为在网站租用公司的产品服务费用，改成租赁的形式。 关于这些数据的安全性，SaaS提供商通过有效的技术措施，可以保证每家企业数据的安全性和保密性。 SaaS采用灵活租赁的收费方式。一方面，企业可以按需增减使用帐号；另一方面，企业按实际使用账户和实际使用时间（以月/年计）付费。由于降低了成本，SaaS的租赁费用较之传统软件许可模式更加低廉。企业采用SaaS模式在效果上与企业自建信息系统基本没有区别，但节省了大量资金，从而大幅度降低了企业信息化的门槛与风险。 这就是软件即服务。由以前去买软件，到现在去买服务。 CRM即客户关系管理（client relation message）ERP系统是企业资源计划(Enterprise Resource Planning )的简称，企业全面化管理平台EHR即电子人力资源管理系统（electric-human resources）SCM即软件配置管理（software configuration Message）OA即办公自动化平台（Office Automation） 应该都可以Saas化 a、实际上saas主要在CRM软件领域应用广泛。b、另外，进销存，物流软件等也是一种应用。C、更广义的是工具化SaaS，比如视频会议租用等，企业邮箱等成为SaaS应用的主要应用。 优点对企业来说，SaaS的优点在于：⒈ 从技术方面来看：SaaS是简单的部署，不需要购买任何硬件，刚开始只需要简单注册即可。企业无需再配备IT方面的专业技术人员，同时又能得到最新的技术应用，满足企业对信息管理的需求。⒉ 从投资方面来看：企业只以相对低廉的“月费”方式投资，不用一次性投资到位，不占用过多的营运资金，从而缓解企业资金不足的压力；不用考虑成本折旧问题，并能及时获得最新硬件平台及最佳解决方案。⒊ 从维护和管理方面来看：由于企业采取租用的方式来进行物流业务管理，不需要专门的维护和管理人员，也不需要为维护和管理人员支付额外费用。很大程度上缓解企业在人力、财力上的压力，使其能够集中资金对核心业务进行有效的运营；SaaS能使用户在世界上都是一个完全独立的系统。如果您连接到网络，就可以访问系统。 缺点1.安全性：企业，尤其是大型企业，很不情愿使用SaaS正是因为安全问题，他们要保护他们的核心数据，不希望这些核心数据由第三方来负责。2.标准化：SaaS解决方案缺乏标准化。这个行业刚刚起步，没有明确的解决办法，一家公司可以设计建立一个解决方案。鉴于复杂和高度可定制的ERP产品，这是一个冒险的建议。 关于Saas的数据安全软件即服务已成为了流行的趋势，整个SaaS的范畴涵盖了广泛的用户可以获取并利用的应用，而SaaS的普及也代表着在未来随着互联网的发展，用户不必再投资于任何服务器或是自己的设备上安装任何软件。从包含了在线Office应用程序的GoogleApps到Adobe的Buzzword服务，以及通过LiveOffice和Hotmail提供的电子邮件及即时消息服务都是很好的SaaS的例证。同时，你还会发现大量的在线备份和数据保护服务，无论是IronMoutain还是AmeriVault，当然，其中还包括一些规模较大的供应商，如EMC、IBM、HP，也加入到了这个市场中来，正在日益将其发展方向转向服务以扩大他们的市场。通过提供这些软件，企业们提供了SaaS服务或是将你的数据存放在他的服务器上，以及获取捏计算机系统，所以，引伸出一个问题：用户使用这些服务的安全性到底如何？“中小型企业必须非常谨慎的挑选供应商以存储他们宝贵的数据。”分析机构IDC的分析师Laura DuBois表示，这位分析师一直关注在线存储服务以及SaaS领域的发展动向，曾在一篇文章中表示，由于在线存储服务来势汹汹，IDC甚至没有为其准备好一个相应的分类方法。很明显，可取的做法是尽可能多的了解该公司是如何提供SaaS服务的，他们为了您的信息的安全做了什么？如果你需要恢复数据，需要多久才能收到？该公司是否能够在低迷而又不稳定的市场中长久生存下去？这些都是你应该问问自己的关键问题–只有做出满意的答案才能够任何选择SaaS供应商的决定。SaaS能够节省用户在部署应用时捆绑的软件许可、硬件以及管理成本，但是这并不意味着SaaS就是每一个人都是使用的。当打算选择一家SaaS供应商时，你应该深入了解这家供应商到底能够提供多少实质性内容，反面的典型就是不愿意向用户提供详细的参考资料或是只有很低用户口碑度。“在SaaS的世界里，留住用户的数字是一个非常重要的宣传。”LiveOffice公司的总裁Matt Smith这样认为，他的公司提供电子邮件、即时消息以及其它SaaS产品，”一个可靠的公司的客户保持率应该至少在98%。”如果这是一家刚刚成立的没有太多用户听说过的初创厂商，你就需要进行更加彻底的调查，以核实其原有的一些用户是否成功交付了。从另一个角度来看，评价一个SaaS提供商还要看用户的支持度，也许有些供应商的设备看起来是豪华的，但是却可能是华而不实的并不中用，尤其是可能会很薄弱的售后支持，虽然在某些情况下，熟练的服务人员和专业的顶尖的技术支持可能与其高昂的价格相比并不值得。“这实际上取决于公司想要什么，”Iron Mountain公司Digital Record Center for Images服务的总经理Tom Meyer认为，”一些供应商并不具备高度安全的内容管理系统，所以他们提供的在线存储空间价格低廉而且简单易行，但是这确实可能会被罚款的。”很清楚的一件事是，安全应该是供应商在选择SaaS标准之前就应考虑的问题并且应该一直放在核心位置，这些在线服务提供商的一个重要的工作就是如何保持其数据的安全，并且确保保护这些数据的保障系统的安全，以免使其遭受灾难。“小型企业的拥有者应该问问供应商如何存储他们的数据，”Smith认为，”一个好的供应商应该有多个镜像数据中心，这也就意味着客户端的数据备份在多个地点和多个时间内总是可以用的。” SaaS厂商利用各种方式来保障他们的数据，他们其中的一些喜欢使用提供了数据加密功能的磁盘阵列，另外一些供应商的方法更加机械化，他们将数据存放在一个大的仓库中，并给予起一个孤立但是安全的位置。Iron Mountain公司提供了一项名为Digital Record Center for Images的服务，这项服务为用户提供了数据加密传输、用户访问路径控制以及确保位于地下200英尺的数据中心的安全的服务。备份和存储SaaS提供商Elephant Drive通过将数据存储在多个基于硬盘的存储池并进行复制的方法来保证用户数据的安全，数据复制保护功能被集成到其产品系统中，所有的数据都可以让用户在位于至少两个不同地点的独立站点进行访问。AmeriVault也是一家在线备份服务提供商，其帮助用户在三个地点保存用户的备份数据，每个用户的数据都存放在两个不同的磁盘系统中，第三份备份则放置在1000公里之外的保证业务连续性的站点中。在线备份提供商DS3则使用EMCClariion作为主存储设备，为了保证备份方便，他们将备份的数据保存在其他的高端磁盘系统中，在DS3的三个数据中心中，有一个数据中心专门用于保存用户的信息的备份。“任何一家有个良好信誉的SaaS供应商都应该采取必要合适的措施确保他们服务器的安全，并且为每个用户都展现出所有的操作。”Smith表示。 服务满意度 服务级别协议是我们通常用来判断一个SaaS服务是否令用户满意的工具，SLA是一项针对提供某种程度上的稳定性的厂商的合同义务，Smith认为，当前使用SLA协议的用户达到了99%以上。此外，SLA协议还包括如果合同到期的话，SaaS服务提供商应该如何处理用户数据的条款，在这种情况下，用户应该确保拥有这些信息的所有权，并且确认是受到法律保护的。例如，Prince Street Capital Management公司采用了由Data Storage公司提供的备份服务，这项服务可以对企业的电子邮件系统实施保护，并对离线数据存储池进行保护，确保远程存储安全以及信息的快速恢复，SLA协议在其中也是一个重要的组成部分。该公司的首席财务官Peter McKown表示，”在你寻找一款适合的备份和恢复解决方案时，对Microsoft Exchange的快速恢复是一个重要的考查标准，在选择了Data Storage服务作为我们的备份和恢复服务管理合作伙伴之后，我们的业务获得了充分的满足，服务水平超过了我们的想象。” 用户质疑SaaS是很正常的，但是从多个方面来看，在十几年前业界关于电子商务的不休争论时，这些质疑就已经存在了。SaaS服务模式与传统许可模式软件有很大的不同，它是未来管理软件的发展趋势。相比较传统服务方式而言SaaS具有很多独特的特征：SaaS不仅减少了或取消了传统的软件授权费用，而且厂商将应用软件部署在统一的服务器上，免除了最终用户的服务器硬件、网络安全设备和软件升级维护的支出，客户不需要除了个人电脑和互联网连接之外的其它IT投资就可以通过互联网获得所需要软件和服务。此外，大量的新技术，如Web Service，提供了更简单、更灵活、更实用的SaaS。 另外，SaaS供应商通常是按照客户所租用的软件模块来进行收费的，因此用户可以根据需求按需订购软件应用服务，而且SaaS的供应商会负责系统的部署、升级和维护。而传统管理软件通常是买家需要一次支付一笔可观的费用才能正式启动。 ERP这样的企业应用软件，软件的部署和实施比软件本身的功能、性能更为重要，万一部署失败，那所有的投入几乎全部白费，这样的风险是每个企业用户都希望避免的。通常的ERP、CRM项目的部署周期至少需要一两年甚至更久的时间，而SaaS模式的软件项目部署通常只占五分之一时间，而且用户无需在软件许可证和硬件方面进行投资。传统软件在使用方式上受空间和地点的限制，必须在固定的设备上使用，而SaaS模式的软件项目可以在任何可接入互联网的地方与时间使用。相对于传统软件而言，SaaS模式在软件的升级、服务、数据安全传输等各个方面都有很大的优势。 SaaS已成为软件产业的一个重要力量。只要SaaS的品质和可信度能继续得到证实，它的魅力就不会消退。例如中企云软基于excel平台和excel服务器，使这一服务云端化，支持在线定制，在线服务，在线使用，让用户无需自建服务器即可轻松拥有saas+paas的平台。而协达软件的渡云SAAS则通过“微商务”的方式让用户低成本使用简洁易用的微型SAAS应用功能，从而逐步升级到更贴身的应用功能上。 过去，很多中小企业对于数据安全都有所顾虑，他们不知道是不是可以信任那些初创厂商，或是不太确定电子商务是一个稳定的业务模式，但是在10年之后，似乎每个人都多多少少和电子商务有所联系，不过，要是想让企业也接受这个全新的技术还要等一段时间。 同样的，SaaS服务也需要经历这样的循环，赢得人们的信任是SaaS服务提供商们不得不面对的一项日产共工作，但是对于那些只有几个技术人员或是根本没有IT部门的中小企业来说，SaaS确实有很重要的作用，能够为企业提供他们必须要完成的工作。 同时，如果你是Prince Street公司的话，或许你需要和多个厂商合作，DuBois认为，在判断究竟哪一个供应商才是可信的时候，用户需要问自己三个问题：谁是技术提供商？谁是管理他们数据的供应商？谁负责建设数据中心和他们的基本数据架构？ 她认为：”在很多情况下，这些问题的答案指向不同的三个厂商，因此每个层次都会有危险存在，在任何情况下，用户要认真的了解隐私性、加密、可用性、恢复时间、SLA协议、成本以及合同期限等细节情况。” 总之，安全问题不容小觑，解决安全问题是SaaS模式继续存在并发展的前提，而周全的考虑各方面的安全性则是中小企业在选择SaaS服务商时必须注意的问题。 如何保证数据安全性除了选择优秀的Saas提供商以外，我们还需要注意：如何辨别具体的一种SaaS是否安全，需要把握以下几点： 1、传输协议加密首先，要看SaaS产品提供使用的协议，是https://还是一般的http://，别小看这个s，这表明所有的数据在传输过程中都是加密的。如果不加密，网上可能有很多“嗅探器”软件能够轻松的获得您的数据，甚至是您的用户名和密码；实际上网上很多聊天软件帐号被盗大多数都是遭到“嗅探器”的“招”了。其次，传输协议加密还要看是否全程加密，即软件的各个部分都是https://协议访问的，有部分软件只做了登录部分，这是远远不够的。比如Salesforce、XToolsCRM都是采取全程加密的。2、服务器安全证书服务器安全证书是用户识别服务器身份的重要标示，有些不正规的服务厂商并没有使用全球认证的服务器安全证书。用户对服务器安全证书的确认，表示服务器确实是用户访问的服务器，此时可以放心的输入用户名和密码，彻底避免“钓鱼”型网站，大多数银行卡密码泄漏都是被“钓鱼”站钓上的。3、URL数据访问安全码技术对于一般用户来说，复杂的URL看起来只是一串没有意义的字符而已。但是对于一些IT高手来说，这些字符串中可能隐藏着一些有关于数据访问的秘密，通过修改URL，很多黑客可以通过诸如SQL注入等方式攻入系统，获取用户数据。4、数据的管理和备份机制SaaS服务商的数据备份应该是完善的，用户必须了解自己服务商为您提供了什么样的数据备份机制，一旦出现重大问题，如何恢复数据等。服务商在内部管理上如何保证用户数据不被服务商所泄露，也是需要用户和服务商沟通的。5、运营服务系统的安全在评估SaaS产品安全度的时侯，最重要的是看公司对于服务器格局的设置，只有这样的格局才是可以信任的，包括：运营服务器与网站服务器分离。服务器的专用是服务器安全最重要的保证。试想，如果一台服务器安装了SaaS系统，但同时又安装了网站系统、邮件系统、论坛系统……，他还能安全吗?在黑客角度来说，越多的系统就意味着越多的漏洞，况且大多数网站使用的网站系统、邮件系统和论坛系统都是在网上能够找到源代码的免费产品，有了源代码，黑客就可以很容易攻入。很多网站被攻入都是因为论坛系统的漏洞。因此，一个优秀的软件SaaS运营商，运营服务器和网站服务器应该完全隔离的，甚至域名也应该分开。 总而言之，SaaS 最大特色是虽是软件在线使用，数据却能本地存储，保证数据安全.（就是服务和数据是分离开的，数据在网络上传输，处理，然后汇集到本地的数据库里来，数据的储存虽然是本地储存，但是由于数据的交互全都是在网络上执行和传输，所以存在相当的风险。） “云”取代“SaaS”成为新的热点 ———更新线——— 云和Saas又有什么不同云计算是Grid计算和（广义的基于SOA的）SaaS技术和理念融合、提升、和发展后的产物。 SaaS不是云计算，云计算也不等于SaaS。SaaS是云计算上的应用表现，云计算是SaaS的后端基础服务保障。 云计算将弱化SaaS门槛，促进SaaS发展。云计算应用直接剥离出去，将平台留下，做平台的始终做平台，做云计算资源的人专心做好资深的调度和服务。SaaS服务商只需要关注自己的软件功能表现，无需投入大量资金到后端基础系统建设。 因为SOA就是如此设计的，面向服务的架构，但是现在又有新的技术出来替代这个SOA设计模式了，那就是微服务架构，将软件开发完全的组件化。当然云计算也是可以和微服务结合的。 云计算系统建立起来之后SaaS将获得跨越式的发展，云计算将大力推动SaaS发展。 根据NIST的权威定义，云计算有SPI， 即SaaS、PaaS和IaaS三大服务模式。这是目前被业界最广泛认同的划分。PaaS和IaaS源于SaaS理念。 1.SaaS：提供给客户的服务是运营商运行在云计算基础设施上的应用程序，用户可以在各种设备上通过搜客户端界面访问，如浏览器。消费者不需要管理或控制任何云计算基础设施，包括网络、服务器、操作系统、存储等等；NIST云计算划分NIST云计算划分2.PaaS：提供给消费者的服务是把客户采用提供的开发语言和工具（例如Java，python, .Net等）开发的或收购的应用程序部署到供应商的云计算基础设施上去。客户不需要管理或控制底层的云基础设施，包括网络、服务器、操作系统、存储等，但客户能控制部署的应用程序，也可能控制运行应用程序的托管环境配置；3。IaaS: 提供给消费者的服务是对所有设施的利用，包括处理器、存储、网络和其它基本的计算资源，用户能够部署和运行任意软件，包括操作系统和应用程序。消费者不管理或控制任何云计算基础设施，但能控制操作系统的选择、储存空间、部署的应用，也有可能获得有限制的网络组件（例如，防火墙，负载均衡器等）的控制。 用云计算还是Saas？主要还是看需求。 一些网络声音总结： 云计算是一种新兴的共享基础架构的方法，可以将巨大的系统池连接在一起以提供各种基于互联网的IT服务。而IaaS、PaaS、SaaS是云计算最主要的三种落地方式。其中IaaS是第一层：基础设施即服务，代表公司：阿里云、腾讯云等；PaaS是第二层：平台即服务，代表公司：数人云；SaaS是第三层：软件即服务，代表公司：用友财务软件、微软office. 云计算（cloud computing）是基于互联网的相关服务的增加、使用和交付模式，通常涉及通过互联网来提供动态易扩展且经常是虚拟化的资源。 SaaS：提供给客户的服务是运营商运行在云计算基础设施上的应用程序，用户可以在各种设备上通过客户端界面访问，如浏览器，消费者不需要管理或控制任何云计算基础设施，包括网络、服务器、操作系统、存储等等 。SaaS带来的是商业模式的转变，云计算带来的是技术的变革。云计算不仅继承了SAAS的所有优点，而且在此基础上还创造了属于自己的特点。 云计算的出现，恰好解决了SaaS发展过程中面临的一些问题，当SaaS提供商的客户快速增加到一定程度，客户所消耗的巨大资源将迫使SaaS供应商提供更多的硬件资源，但由于成本的问题，SaaS又不想花费大量资金购买硬件或带宽资源的时候，云计算无疑是个不错的选择。根据通常的概念，云计算处于SaaS的更底层，而SaaS位于云计算和最终客户之间，如果SaaS在最初开发的时候是基于云计算架构的，那么就很容易利用云计算架构来获取海量的资源，并提供给最终用户。这就一劳永逸的解决SaaS发展的瓶颈问题。 等等。来自知乎 与物联网的合作物联网的两种业务模式 MAI（M2M Application Integration), 内部MaaS MaaS（M2M As A Service), MMO, Multi-Tenants(多租户模型）随着物联网业务量的增加，对数据存储和计算量的需求将带来对“云计算”能力的要求：云计算：从计算中心到数据中心在物联网的初级阶段，PoP即可满足需求在物联网高级阶段，可能出现MVNO/MMO营运商（国外已存在多年），需要虚拟化云计算技术，SOA等技术的结合实现物联网的泛在服务： TaaS （everyTHING As A Service)。]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>软件架构</tag>
        <tag>云计算</tag>
        <tag>Saas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM-GC机制]]></title>
    <url>%2F2018%2F03%2F30%2FJVM-GC%2F</url>
    <content type="text"><![CDATA[为什么需要GC应用程序对资源操作，通常简单分为以下几个步骤： 1、为对应的资源分配内存 2、初始化内存 3、使用资源 4、清理资源 5、释放内存 应用程序对资源（内存使用）管理的方式，常见的一般有如下几种： 1、手动管理：C,C++ 2、计数管理：COM 3、自动管理：.NET,Java,PHP,GO… 但是，手动管理和计数管理的复杂性很容易产生以下典型问题： 1.程序员忘记去释放内存 2.应用程序访问已经释放的内存 产生的后果很严重，常见的如内存泄露、数据内容乱码，而且大部分时候，程序的行为会变得怪异而不可预测，还有Access Violation（访问违例）等。 .NET、Java等给出的解决方案，就是通过自动垃圾回收机制GC进行内存管理。这样，问题1自然得到解决，问题2也没有存在的基础。 总结：无法自动化的内存管理方式极容易产生bug，影响系统稳定性，尤其是线上多服务器的集群环境，程序出现执行时bug必须定位到某台服务器然后dump内存再分析bug所在，极其打击开发人员编程积极性，而且源源不断的类似bug让人厌恶。因此自动化的GC机制就很重要。 GC的工作流程主要分为如下几个步骤： 1、标记(Mark) 2、计划(Plan) 3、清理(Sweep) 4、引用更新(Relocate) 5、压缩(Compact) （一）、标记 目标：找出所有引用不为0(live)的实例 方法：找到所有的GC的根结点(GC Root), 将他们放到队列里，然后依次递归地遍历所有的根结点以及引用的所有子节点和子子节点，将所有被遍历到的结点标记成live。弱引用不会被考虑在内 （二）、计划和清理 1、计划 目标：判断是否需要压缩 方法：遍历当前所有的generation上所有的标记(Live),根据特定算法作出决策 2、清理 目标：回收所有的free空间 方法：遍历当前所有的generation上所有的标记(Live or Dead),把所有处在Live实例中间的内存块加入到可用内存链表中去 （三）、引用更新和压缩 1、引用更新 目标： 将所有引用的地址进行更新 方法：计算出压缩后每个实例对应的新地址，找到所有的GC的根结点(GC Root), 将他们放到队列里，然后依次递归地遍历所有的根结点以及引用的所有子节点和子子节点，将所有被遍历到的结点中引用的地址进行更新，包括弱引用。 2、压缩 目标：减少内存碎片 方法：根据计算出来的新地址，把实例移动到相应的位置。 GC的根节点也即GC Root是个什么东西呢？ 每个应用程序都包含一组根（root）。每个根都是一个存储位置，其中包含指向引用类型对象的一个指针。该指针要么引用托管堆中的一个对象，要么为null。 在应用程序中，只要某对象变得不可达，也就是没有根（root）引用该对象，这个对象就会成为垃圾回收器的目标。 用一句简洁的英文描述就是:GC roots are not objects in themselves but are instead references to objects.而且，Any object referenced by a GC root will automatically survive the next garbage collection. .NET中可以当作GC Root的对象有如下几种： 1、全局变量 2、静态变量 3、栈上的所有局部变量(JIT) 4、栈上传入的参数变量 5、寄存器中的变量 注意，只有引用类型的变量才被认为是根，值类型的变量永远不被认为是根。只有深刻理解引用类型和值类型的内存分配和管理的不同，才能知道为什么root只能是引用类型。 顺带提一下JAVA，在Java中，可以当做GC Root的对象有以下几种： 1、虚拟机（JVM）栈中的引用的对象 2、方法区中的类静态属性引用的对象 3、方法区中的常量引用的对象（主要指声明为final的常量值） 4、本地方法栈中JNI的引用的对象 GC什么时候发生1、当应用程序分配新的对象，GC的代的预算大小已经达到阈值，比如GC的第0代已满 2、代码主动显式调用System.GC.Collect()，System.gc()；当然这只是建议GC去处理垃圾，而不是立刻执行，主动权完全在JVM上。 3、其他特殊情况，比如，windows报告内存不足、CLR卸载AppDomain、CLR关闭，甚至某些极端情况下系统参数设置改变也可能导致GC回收 GC中的代代（Generation)引入的原因主要是为了提高性能（Performance),以避免收集整个堆（Heap）。一个基于代的垃圾回收器做出了如下几点假设：在JAV中也叫新生代和老年代。1、对象越新，生存期越短 2、对象越老，生存期越长 3、回收堆的一部分，速度快于回收整个堆 .NET的垃圾收集器将对象分为三代（Generation0,Generation1,Generation2）。不同的代里面的内容如下： 1、G0 小对象(Size&lt;85000Byte) 2、G1:在GC中幸存下来的G0对象 3、G2:大对象(Size&gt;=85000Byte);在GC中幸存下来的G1对象 object o = new Byte[85000]; //large objectConsole.WriteLine(GC.GetGeneration(o)); //output is 2,not 0这里必须知道，CLR要求所有的资源都从托管堆（managed heap）分配，CLR会管理两种类型的堆，小对象堆（small object heap，SOH）和大对象堆（large object heap，LOH），其中所有大于85000byte的内存分配都会在LOH上进行。一个有趣的问题是为什么是85000字节？ 代收集规则：当一个代N被收集以后，在这个代里的幸存下来的对象会被标记为N+1代的对象。GC对不同代的对象执行不同的检查策略以优化性能。每个GC周期都会检查第0代对象。大约1/10的GC周期检查第0代和第1代对象。大约1/100的GC周期检查所有的对象。 使用GC（1）除了释放不再被引用的对象，垃圾收集器还要处理堆碎块。请求分配新对象时可能不得不增大堆空间的大小，虽然可以使用的空闲空间是足够的，但是堆中没有没有连续的空间放得下新对象。可能会导致虚拟机产生不必要的”内存不足“错误。 （2）使用垃圾收集堆，有一个潜在的缺陷就是加大程序的负担，可能影响程序的性能。因为虚拟机需要追踪哪些对象被正在执行的程序引用，还要动态释放垃圾对象。 （3）程序可以调用System.gc()建议jvm去收集垃圾， 但是不能为垃圾回收机制指定某个对象是不是垃圾。即便调用了gc（），并不会马上进行垃圾回收，甚至不一定会执行垃圾回收。所有的内存分配和回收权限都在jvm，不在开发人员手里。 Example： public class RubbishRelease { // 类的finalize方法，可以告诉垃圾回收器应该执行的操作，该方法从Object类继承而来。 // 在从堆中永久删除对象之前，垃圾回收器调用该对象的finalize方法。 public void finalize() { System.out.println(&quot;the Object is going...&quot;); } public static void main(String[] args) { for (int i = 0; i &lt; 100; i++) { // 下面不断创建对象，但是这些对象都没有被引用 new RubbishRelease(); new RubbishRelease(); new RubbishRelease(); System.gc(); } System.out.println(&quot;The program is over!&quot;); } } 运行结果： （4）垃圾收集算法有很多，但任何垃圾收集算法都必须做两件事情。首先，它必须检测出垃圾对象。其次，它必须回收垃圾对象所使用的堆空间并还给程序。 区分活动对象和垃圾的两个基本方法是引用计数和跟踪引用计数是垃圾收集的早期策略。在这种方法中，堆中每一个对象都有一个引用计数。一个对象被创建了，并且指向该对象的引用被分配给一个变量，这个对象的引用计数被置为1。当任何其他变量被赋值为对这个对象的引用时，计数加1。当一个对象的引用超过了生存期或者被设置一个新的值时，对象的引用计数减1。任何引用计数为0的对象可以被当作垃圾收集。当一个对象被垃圾收集的时候，它引用的任何对象计数值减1。这种方法的好处是，引用计数收集器可以很快地执行，交织在程序的运行之中。这个特性对于程序不能被长时间打断的实时环境很有利。坏处就是，引用计数无法检测出循环(即两个或者更多的对象互相引用)。 引用计数算法（Reference Counting） java不用介绍：给对象添加一个引用计数器，每当一个地方引用它时，数据器加1；当引用失效时，计数器减1；计数器为0的即可被回收。 优点：实现简单，判断效率高 缺点：很难解决对象之间的相互循环引用（objA.instance = objB; objB.instance = objA）的问题，所以java语言并没有选用引用计数法管理内存 跟踪收集器追踪从根节点开始的对象引用图。给追踪过程中遇到对象以某种方式打上标记。追踪结束时，未被标记的对象就是无法触及的，从而被收集。基本的追踪算法被称作“标记并清除”，这个名字指出垃圾收集过程的两个阶段。 根搜索算法（GC Root Tracing） Java和C#都是使用根搜索算法来判断对象是否存活。通过一系列的名为“GC Root”的对象作为起始点，从这些节点开始向下搜索，搜索所有走过的路径称为引用链（Reference Chain）,当一个对象到GC Root没有任何引用链相连时（用图论来说就是GC Root到这个对象不可达时），证明该对象是可以被回收的。 在Java中哪些对象可以成为GC Root?（在上面提过了，再写一遍） 虚拟机栈（栈帧中的本地变量表）中的引用对象方法区中的类静态属性引用的对象方法区中的常量引用对象本地方法栈中JNI（即Native方法）的引用对象 Java虚拟机的垃圾收集器可能有对付堆碎块的策略。标记并清除收集器通常使用的两种策略是压缩和拷贝。这两种方法都是快速地移动对象来减少堆碎块。 压缩收集器把活动的对象越过空闲区滑动到堆的一端，在这个过程中，堆的另一端出现一个大的连续空闲区。所有被移动的对象的引用也被更新，指向新的位置。 拷贝收集器把所有的活动的对象移动到一个新的区域。在拷贝过程中，被紧挨着布置，这样可以消除原本它们在旧区域的空隙。即空闲区。一般的拷贝收集器算法被称为“停止并拷贝”。此方案中，堆被分成两个区域，任何时候都使用一个区域。对象在同一个区域中分配直到被耗尽。此时，程序执行被中止，堆被遍历，遍历时遇到活动的对象被拷贝到另个区域。当停止和拷贝过程结束时，程序恢复执行。依次往复，对于指定大小的堆来说需要两倍大小的内存，由于任何时候都只使用其中的一半，这就是该方法带来的代价。 复制算法（Copying） 将内存划分为大小相等的两块，每次只使用其中的一块。当这块内存用完了，就将还存活的对象复制到另一块内存上，然后把已使用过的内存空间一次清理掉。 优点：每次只对其中一块进行GC,不用考虑内存碎片的问题，并且实现简单，运行高效 缺点：内存缩小了一半 注：现在的商业虚拟机都是用这种收集算法回收新生代。内存分为一块较大的Eden空间和两块较小的Survior空间，每次使用Eden和其中的一块Survior.当回收时，将Eden和Survior中还存活的对象一次性拷贝到另外一块Survior空间上，最后清理Eden和刚才用过的Survior空间。 按代收集：根据对象的存活周期（一次垃圾收集为一个周期）的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用拷贝算法，只需要付出少量存活对象的拷贝成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，可以使用“标记并清除”算法。 据对象的存活周期的不同将内存划分为几块，一般就分为新生代和老年代，根据各个年代的特点采用不同的收集算法。新生代（少量存活）用复制算法，老年代（对象存活率高）“标记-清理”算法 补充：分代划分内存介绍 整个JVM内存总共划分为三代：年轻代（Young Generation）、年老代（Old Generation）、持久代（Permanent Generation） 1、年轻代：所有新生成的对象首先都放在年轻代内存中。年轻代的目标就是尽可能快速的手机掉那些生命周期短的对象。年轻代内存分为一块较大的Eden空间和两块较小的Survior空间，每次使用Eden和其中的一块Survior.当回收时，将Eden和Survior中还存活的对象一次性拷贝到另外一块Survior空间上，最后清理Eden和刚才用过的Survior空间。 2、年老代：在年轻代经历了N次GC后，仍然存活的对象，就会被放在老年代中。因此可以认为老年代存放的都是一些生命周期较长的对象。 3、持久代：基本固定不变，用于存放静态文件，例如Java类和方法。持久代对GC没有显著的影响。持久代可以通过-XX:MaxPermSize=进行设置。 终结方法（finalize），这个在上面第3点也有提到：这个方法是垃圾收集器在释放对象前必须运行。这个可能存在的终结方法使得任何Java虚拟机的垃圾收集器要完成的工作更加复杂。因为终结方法可能“复活”了某些不再被引用的对象（本身或者其他对象）。 堆中的每一个对象都有三种状态之一：可触及的、可复活的以及不可触及的。可触及状态好理解。关于可复活状态：它在从根节点开始的追踪图中不可触及，但是又可能在垃圾收集器执行某些终结方法时触及。不仅仅是那些声明了finalize方法的对象，而是所有的对象都要经过可复活状态。而不可触及状态标志着不但对象不再被触及，而且也不可能通过任何终结方法复活。不可触及的对象不再对程序的执行产生影响，可自由地回收它们占据的内存。 一些其他算法总结： 标记-清除算法（Mark-Sweep） 首先标记出需要回收的对象，在标记完成后统一回收掉所有的被标记对象。 缺点：效率问题和空间问题（标记清除后会产生大量的不连续内存碎片，内存碎片过多可能会导致程序需要分配较大对象时找不到足够大的连续内存空间而不得不提前触发另一次垃圾回收动作） 标记-整理算法（Mark-Compact） 让所有存活对象都向一端移动，然后直接清理掉端边界以外的所有内存。 对象的强，软，弱，虚引用。强引用：如果一个对象具有强引用，垃圾回收器绝不会回收它。当内存空间不足，JVM宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会考随意回收具有强引用的对象来解决内存不足的问题。 软引用：如果一个对象具有软引用。如果内存空间足够。垃圾回收器不会回收它。如果内存不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 弱引用：如果一个对象具有弱引用。当垃圾回收器发现只具有弱引用对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现只具有弱引用的对象。 虚引用：虚引用不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。 谨慎显式调用GCGC的开销通常很大，而且它的运行具有不确定性，微软的编程规范里是强烈建议你不要显式调用GC。但你的代码中还是可以使用framework中GC的某些方法进行手动回收，前提是你必须要深刻理解GC的回收原理，否则手动调用GC在特定场景下很容易干扰到GC的正常回收甚至引入不可预知的错误。 比如如下代码： void SomeMethod() { object o1 = new Object(); object o2 = new Object(); o1.ToString(); System.gc(); // this forces o2 into Gen1, because it&apos;s still referenced o2.ToString(); } 如果没有GC.Collect()，o1和o2都将在下一次垃圾自动回收中进入Gen0，但是加上GC.Collect()，o2将被标记为Gen1，也就是0代回收没有释放o2占据的内存 还有的情况是编程不规范可能导致死锁，比如流传很广的一段代码： var instance = new MyClass(); Monitor.Enter(instance); instance = null; GC.Collect(); GC.WaitForPendingFinalizers(); Console.WriteLine(&quot;instance is gabage collected&quot;); 上述代码将会导致死锁。原因分析如下： 1、客户端主线程调用代码Monitor.Enter(instance)代码段lock住了instance实例 2、接着手动执行GC回收，主（Finalizer)线程会执行MyClass析构函数 3、在MyClass析构函数内部，使用了lock (this)代码，而主（Finalizer)线程还没有释放instance（也即这里的this），此时主线程只能等待 虽然严格来说，上述代码并不是GC的错，和多线程操作似乎也无关，而是Lock使用不正确造成的。 同时请注意，GC的某些行为在Debug和Release模式下完全不同（Jeffrey Richter在&lt;&gt;举过一个Timer的例子说明这个问题）。比如上述代码，在Debug模式下你可能发现它是正常运行的，而Release模式下则会死锁。 当GC遇到多线程前面讨论的垃圾回收算法有一个很大的前提就是：只在一个线程运行。而在现实开发中，经常会出现多个线程同时访问托管堆的情况，或至少会有多个线程同时操作堆中的对象。一个线程引发垃圾回收时，其它线程绝对不能访问任何线程，因为垃圾回收器可能移动这些对象，更改它们的内存位置。CLR想要进行垃圾回收时，会立即挂起执行托管代码中的所有线程，正在执行非托管代码的线程不会挂起。然后，CLR检查每个线程的指令指针，判断线程指向到哪里。接着，指令指针与JIT生成的表进行比较，判断线程正在执行什么代码。 如果线程的指令指针恰好在一个表中标记好的偏移位置，就说明该线程抵达了一个安全点。线程可在安全点安全地挂起，直至垃圾回收结束。如果线程指令指针不在表中标记的偏移位置，则表明该线程不在安全点，CLR也就不会开始垃圾回收。在这种情况下，CLR就会劫持该线程。也就是说，CLR会修改该线程栈，使该线程指向一个CLR内部的一个特殊函数。然后，线程恢复执行。当前的方法执行完后，他就会执行这个特殊函数，这个特殊函数会将该线程安全地挂起。然而，线程有时长时间执行当前所在方法。所以，当线程恢复执行后，大约有250毫秒的时间尝试劫持线程。过了这个时间，CLR会再次挂起线程，并检查该线程的指令指针。如果线程已抵达一个安全点，垃圾回收就可以开始了。但是，如果线程还没有抵达一个安全点，CLR就检查是否调用了另一个方法。如果是，CLR再一次修改线程栈，以便从最近执行的一个方法返回之后劫持线程。然后，CLR恢复线程，进行下一次劫持尝试。所有线程都抵达安全点或被劫持之后，垃圾回收才能使用。垃圾回收完之后，所有线程都会恢复，应用程序继续运行，被劫持的线程返回最初调用它们的方法。 实际应用中，CLR大多数时候都是通过劫持线程来挂起线程，而不是根据JIT生成的表来判断线程是否到达了一个安全点。之所以如此，原因是JIT生成表需要大量内存，会增大工作集，进而严重影响性能。 这里再说一个真实案例。某web应用程序中大量使用Task，后在生产环境发生莫名其妙的现象，程序时灵时不灵，根据数据库日志（其实还可以根据Windows事件跟踪（ETW）、IIS日志以及dump文件），发现了Task执行过程中有不规律的未处理的异常，分析后怀疑是CLR垃圾回收导致，当然这种情况也只有在高并发条件下才会暴露出来。 开发中的一些建议和意见由于GC的代价很大，平时开发中注意一些良好的编程习惯有可能对GC有积极正面的影响，否则有可能产生不良效果。 1、尽量不要new很大的object，大对象（&gt;=85000Byte）直接归为G2代，GC回收算法从来不对大对象堆（LOH）进行内存压缩整理，因为在堆中下移85000字节或更大的内存块会浪费太多CPU时间 2、不要频繁的new生命周期很短object，这样频繁垃圾回收频繁压缩有可能会导致很多内存碎片，可以使用设计良好稳定运行的对象池（ObjectPool）技术来规避这种问题 3、使用更好的编程技巧，比如更好的算法、更优的数据结构、更佳的解决策略等等 GC线程和Finalizer线程GC在一个独立的线程中运行来删除不再被引用的内存。 Finalizer则由另一个独立（高优先级CLR)线程来执行Finalizer的对象的内存回收。 对象的Finalizer被执行的时间是在对象不再被引用后的某个不确定的时间，并非和C++中一样在对象超出生命周期时立即执行析构函数。 GC把每一个需要执行Finalizer的对象放到一个队列(从终结列表移至freachable队列）中去，然后启动另一个线程而不是在GC执行的线程来执行所有这些Finalizer，GC线程继续去删除其他待回收的对象。 在下一个GC周期，这些执行完Finalizer的对象的内存才会被回收。也就是说一个实现了Finalize方法的对象必需等两次GC才能被完全释放。这也表明有Finalize的方法（Object默认的不算）的对象会在GC中自动“延长”生存周期。因为Finalize是在GC回收对象前调用，所以在类中若是写了Finalize方法，那么GC在收集这个对象时会先执行这个方法，因为优先级较高，然后跑去收集别的了，等到最后再回来收集这个对象，可以说是变相的延长了对象的寿命。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>底层原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java书单]]></title>
    <url>%2F2018%2F03%2F30%2Fjava%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[多数都是在知乎和ImportNew上收集的，作为自己的知识储备。作为一个java开发工程师，若想在职业这条道路上走的更远，不仅要学会底层的知识，学习各种虚拟机，明白各种设计模式，web微服务开发框架，半个DBA，大数据方向作知识储备，hadoop和spark集群演变和原理实现，爬虫与反爬，各种最新的框架和API，跟上java版本更新，保持学习动力。 《Effective Java中文版》 人手一本？谷歌出品，必属精品？ 《Effective Java中文版》的作者是Joshua Bloch，这个人就很厉害了，他是谷歌的首席架构师，属于超级技术大牛级别了吧，呵呵。由于没有看过这本书，所以我不好发表评论，但是从这本书的知名度以及其作者的来头来看（多提一句，这本书也是Java之父James Gosling博士推崇的一本书），我相信这一定是一本值得一看的好书。 好的代码是每个Java程序员都应该去追求的，不是说我今天写一段好代码相比写一段烂代码对性能会有多大的提升，更多的应该是提升了代码的可读性以及可以规避许多潜在的、未知的问题，避免代码上线之后出问题而花时间去维护—-无论从时间成本、人力成本还是风险成本来说，这都是非常高的。 《深入分析Java Web技术内幕》，作者许令波，淘宝工程师。 这本书我用一个字概括就是：全。真的非常全，HTTP、DNS、CDN、静态化、Jetty、Tomcat、Servlet、Spring、MyBatis等等，什么都有，涉及知识面非常广，但又不像专门精讲某个知识点的书籍一样讲得非常深入，感觉这本书就是尽量去用短的篇幅讲清楚一些Java Web使用到的技术的内幕，让读者对这些知识点的技术内幕有一个理性的认识。 不过，尽管每个知识点的篇幅都不多，但是重点都基本讲到了，是一本让人真正有收获的书。如果想进一步了解这些技术的技术内幕，就要自己去买相关书籍或者自己上网查资料了，有种抛砖引玉，或者说师傅领进门、修行在个人的感觉。 《大型网站技术架构 核心原理与案例分析》的作者是李智慧，原阿里巴巴技术专家 Java的大多数应用都是用在Web上的，现在只要稍微大型一点的Web应用，都一定是一个分布式系统，那么一个分布式系统用到了哪些技术？一个大型网站是如何从一个小型网站成长起来的？如何保证你的网站安全？分布式系统使用到了缓存，有哪些缓存？缓存的使用有哪些值得注意的事项？ 关于分布式的知识点，都在这本书里面有体现，只有你想不到，没有他写不到，而且写得非常易懂，基本属于看一两遍，再记一些笔记就知道是怎么一回事儿了。多看几遍，对分布式的理解一定会加深不少。而且里面不仅仅是分布式的知识，还非常接地气地写了如何做一个好的架构师，其实我认为这不仅仅是写给想做架构师的读者看的，就是给读者一些建议，如何更好地提出意见、如何更让别人关注你的声音、如何看到他人的优点，入木三分，让人获益匪浅。 《大型网站系统与Java中间件实践》作者曾宪杰，是淘宝的技术总监，算起来应该在阿里有至少P8的级别了吧。 中间件读物，如果产品开发中涉及到中间件，可以一读。 这本书的部分内容和上面一本李智慧的《大型网站技术架构 核心原理与案例分析》有所重合，像分布式系统的演化、CDN、CAP理论和BASE理论等等，这也更说明这些都是分布式系统或者说是一个大型网站重点关注的内容，当作一次再学习也不错。 本书要突出的重点是中间件三个字，中间件是分布式系统中一个非常重要的东西，其最重要的作用应该就是解耦，降低模块与模块之间的强依赖，不同的模块之间的依赖度降低，便可以各自独立地开发自己的功能，这也可以说是软件工程发展的目标和驱动力。 因此，本书有一部分的内容就是基于中间件，详细讲解了中间件与JMS的各种知识，适合对分布式系统比较熟悉并且想要往中间件方面有一定研究的读者。]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>书单</tag>
        <tag>架构</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA-线程重读]]></title>
    <url>%2F2018%2F03%2F29%2Fjava-thread%2F</url>
    <content type="text"><![CDATA[传统线程及并发处理关于同步及互斥，对象锁与类锁，wait(),notify(),notifyAll()这三种方法怎么用这是最关键的问题。 wait，notify，notifyAll 是定义在Object类的实例方法，用于控制线程状态。 三个方法都必须在synchronized 同步关键字所限定的作用域中调用，否则会报错java.lang.IllegalMonitorStateException ，意思是因为没有同步，所以线程对对象锁的状态是不确定的，不能调用这些方法。 wait 表示持有对象锁的线程A准备释放对象锁权限，释放cpu资源并进入等待。notify 表示持有对象锁的线程A准备释放对象锁权限，通知jvm唤醒某个竞争该对象锁的线程X。线程A synchronized 代码作用域结束后，线程X直接获得对象锁权限，其他竞争线程继续等待(即使线程X同步完毕，释放对象锁，其他竞争线程仍然等待，直至有新的notify ,notifyAll被调用)。notifyAll 表示持有对象锁的线程A准备释放对象锁权限，通知jvm唤醒所有竞争该对象锁的线程，线程A synchronized 代码作用域结束后，jvm通过算法将对象锁权限指派给某个线程X，所有被唤醒的线程不再等待。线程X synchronized 代码作用域结束后，之前所有被唤醒的线程都有可能获得该对象锁权限，这个由JVM算法决定。wait有三个重载方法，同时必须捕获非运行时异常InterruptedException。 wait() 进入等待，需要notify ,notifyAll才能唤醒wait(long timeout) 进入等待，经过timeout 超时后，若未被唤醒，则自动唤醒wait(timeout, nanos) 进入等待，经过timeout 超时后，若未被唤醒，则自动唤醒。相对wait(long timeout) 更加精确时间。 那么对象锁又是什么？类锁又是啥？ synchronized关键字 synchronized关键字有如下两种用法： 1、 在需要同步的方法的方法签名中加入synchronized关键字。在非静态方法中加入synchronized关键字是对象级别的 synchronized public void getValue() { System.out.println(&quot;getValue method thread name=&quot; + Thread.currentThread().getName() + &quot; username=&quot; + username + &quot; password=&quot; + password); } 上面的代码修饰的synchronized是非静态方法，如果修饰的是静态方法（static）含义是完全不一样的。具体不一样在哪里，后面会详细说清楚。而在静态方法中加入synchronized关键字是类级别的（静态方法是类直接调用的） synchronized static public void getValue() { System.out.println(“getValue method thread name=” + Thread.currentThread().getName() + &quot; username=&quot; + username + &quot; password=&quot; + password); } 2、使用synchronized块对需要进行同步的代码段进行同步。因为同步是对系统开销很大的一种操作，若是要执行高并发就必须要用到同步，因此为了尽量减少同步的内容要用到同步块，对方法里的一部分进行加锁。同步块所锁住的参数也是对应不同的级别同步代码块的synchronized (this)用法和synchronized (非this对象)的用法锁的是对象同步代码块的synchronized (类.class)用法锁的是类包括上面所声明的在方法前加关键字，syncronized总共有5种方法 public void serviceMethod() { try { synchronized (this) { System.out.println(&quot;begin time=&quot; + System.currentTimeMillis()); Thread.sleep(2000); System.out.println(&quot;end end=&quot; + System.currentTimeMillis()); } } catch (InterruptedException e) { e.printStackTrace(); } } 上面的代码块是synchronized (this)用法，还有synchronized (非this对象)以及synchronized (类.class)这两种用法，这些使用方式的含义也是有根本的区别的。我们先带着这些问题继续往下看。 1.一段synchronized的代码被一个线程执行之前，他要先拿到执行这段代码的权限；（执行代码的权限就是对象锁）2.在Java里边就是拿到某个同步对象的锁（一个对象只有一把锁）；3.如果这个时候同步对象的锁被其他线程拿走了，他（这个线程）就只能等了（线程阻塞在锁池等待队列中）。4.取到锁后，他就开始执行同步代码(被synchronized修饰的代码）；5.线程执行完同步代码后马上就把锁还给同步对象，其他在锁池中等待的某个线程就可以拿到锁执行同步代码了。6.这样就保证了同步代码在统一时刻只有一个线程在执行。 上面提到锁，这里先引出锁的概念。先来看看下面这些啰嗦而必不可少的文字。 多线程的线程同步机制实际上是靠锁的概念来控制的。 在Java程序运行时环境中，JVM需要对两类线程共享的数据进行协调：1）保存在堆中的实例变量（对象信息放在堆中Heap）2）保存在方法区中的类变量（类信息放在方法区中Method Area） 这两类数据是被所有线程共享的。（程序不需要协调保存在Java 栈当中的数据。因为这些数据是属于拥有该栈的线程所私有的。） 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 栈：在Java中，JVM中的栈记录了线程的方法调用。每个线程拥有一个栈。在某个线程的运行过程中，如果有新的方法调用，那么该线程对应的栈就会增加一个存储单元，即帧(frame)。在frame中，保存有该方法调用的参数、局部变量和返回地址。 堆是JVM中一块可自由分配给对象的区域。当我们谈论垃圾回收(garbage collection)时，我们主要回收堆(heap)的空间。Java的普通对象存活在堆中。与栈不同，堆的空间不会随着方法调用结束而清空。因此，在某个方法中创建的对象，可以在方法调用结束之后，继续存在于堆中。这带来的一个问题是，如果我们不断的创建新的对象，内存空间将最终消耗殆尽。 在java虚拟机中，每个对象和类在逻辑上都是和一个监视器相关联的。对于对象来说，相关联的监视器保护对象的实例变量。 对于类来说，监视器保护类的类变量。 （如果一个对象没有实例变量，或者一个类没有变量，相关联的监视器就什么也不监视。）为了实现监视器的排他性监视能力，java虚拟机为每一个对象和类都关联一个锁。代表任何时候只允许一个线程拥有的特权。线程访问实例变量或者类变量不需锁。 但是如果线程获取了锁，那么在它释放这个锁之前，就没有其他线程可以获取同样数据的锁了。（锁住一个对象就是获取对象相关联的监视器） 类锁实际上用对象锁来实现。当虚拟机装载一个class文件的时候，它就会创建一个java.lang.Class类的实例。当锁住一个对象的时候，实际上锁住的是那个类的Class对象。 一个线程可以多次对同一个对象上锁。对于每一个对象，java虚拟机维护一个加锁计数器，线程每获得一次该对象，计数器就加1，每释放一次，计数器就减 1，当计数器值为0时，锁就被完全释放了。 java编程人员不需要自己动手加锁，对象锁是java虚拟机内部使用的。 在java程序中，只需要使用synchronized块或者synchronized方法就可以标志一个监视区域。当每次进入一个监视区域时，java 虚拟机都会自动锁上对象或者类。 参考这篇文章 ：https://blog.csdn.net/u013142781/article/details/51697672 在这篇文章中你将会学到如何使用 wait、notify 和 notifyAll 来实现线程间的通信，从而解决生产者消费者问题。如果你想要更深入地学习Java中的多线程同步问题，我强烈推荐阅读Brian Goetz所著的《Java Concurrency in Practice | Java 并发实践》，不读这本书你的 Java 多线程征程就不完整哦！这是我最向Java开发者推荐的书之一。 回到wait(),notify(),notifyAll()这三个方法中来 这里用药店窗口取药的模型可以很好的解释这一点。。。 wait( )，notify( )，notifyAll( )都不属于Thread类，而是属于Object基础类，也就是每个对象都有wait( )，notify( )，notifyAll( ) 的功能，因为每个对象都有锁，锁是每个对象的基础，当然操作锁的方法也是最基础了。 当需要调用以上的方法的时候，一定要对竞争资源进行加锁，如果不加锁的话，则会报 IllegalMonitorStateException 异常 当想要调用wait( )进行线程等待时，必须要取得这个锁对象的控制权（对象监视器），一般是放到synchronized(obj)代码中。 在while循环里而不是if语句下使用wait，这样，会在线程暂停恢复后都检查wait的条件，并在条件实际上并未改变的情况下处理唤醒通知 调用obj.wait( )释放了obj的锁，否则其他线程也无法获得obj的锁，也就无法在synchronized(obj){ obj.notify() } 代码段内唤醒A。 notify( )方法只会通知等待队列中的第一个相关线程（不会通知优先级比较高的线程） notifyAll( )通知所有等待该竞争资源的线程（也不会按照线程的优先级来执行） 假设有三个线程执行了obj.wait( )，那么obj.notifyAll( )则能全部唤醒tread1，thread2，thread3，但是要继续执行obj.wait（）的下一条语句，必须获得obj锁，因此，tread1，thread2，thread3只有一个有机会获得锁继续执行，例如tread1，其余的需要等待thread1释放obj锁之后才能继续执行。 当调用obj.notify/notifyAll后，调用线程依旧持有obj锁，因此，thread1，thread2，thread3虽被唤醒，但是仍无法获得obj锁。直到调用线程退出synchronized块，释放obj锁后，thread1，thread2，thread3中的一个才有机会获得锁继续执行。 wait()与sleep()的区别： 1.首先sleep()是Thread()类的方法，而wait()是Object类的方法，包括notify()，notifyAll()都是Object类的方法 2.sleep()方法是休眠，阻塞线程的同时仍然会持有锁，也就是说它休眠期间其他线程仍然无法获得锁，同时sleep()休眠时自动醒 的；而调用wait()方法时，则自动释放锁，也就是其他线程可以获得锁，而且wait()是无法自动醒的，只有通过notify()或 notifyAll() 才行。如果不设置wait自动醒的时间，那么wait将会一直等下去直至notify来唤醒，进入锁池去获取对象锁。 notify()与notifyAll()的区别 notify()一次只能激活一个对这个对象进行wait()的线程，当多个线程都对此对象wait()时，是随机挑一个notify()，而notifyAll()是一次 性激活所以对此对象进行wait()的线程。 接下来说说利用wait()和notify()来实现生产者和消费者并发问题： 显然要保证生产者和消费者并发运行不出乱，主要要解决：当生产者线程的缓存区为满的时候，就应该调用wait()来停止生产者继续生产，而当生产者满的缓冲区被消费者消费掉一块时，则应该调用notify()唤醒生产者，通知他可以继续生产；同样，对于消费者，当消费者线程的缓存区为空的时候，就应该调用wait()停掉消费者线程继续消费，而当生产者又生产了一个时就应该调用notify()来唤醒消费者线程通知他可以继续消费了。 当然我们必须在wait()和notify()的时候锁住我们所要操作的对象,这里即缓存区，下面是一个使用wait()的notify()的规范代码模板：synchronized的背景下和那个被多线程共享的对象上调用 synchronized (sharedObject) { //锁住操作对象，锁的是对象 while (condition) { //当某个条件下 sharedObject.wait(); //进入wait，这个shareObject就是所有线程共享的对象，在生产者-消费者模型里面这个对象就是缓冲区队列 } // 做了什么事，就可以激活，注意在while循环外 shareObject.notify(); } wait, notify 和 notifyAll，这些在多线程中被经常用到的保留关键字，在实际开发的时候很多时候却并没有被大家重视。本文对这些关键字的使用进行了描述。 在 Java 中可以用 wait、notify 和 notifyAll 来实现线程间的通信。。举个例子，如果你的Java程序中有两个线程——即生产者和消费者，那么生产者可以通知消费者，让消费者开始消耗数据，因为队列缓冲区中有内容待消费（不为空）。相应的，消费者可以通知生产者可以开始生成更多的数据，因为当它消耗掉某些数据后缓冲区不再为满。 我们可以利用wait()来让一个线程在某些条件下暂停运行。例如，在生产者消费者模型中，生产者线程在缓冲区为满的时候，消费者在缓冲区为空的时候，都应该暂停运行。如果某些线程在等待某些条件触发，那当那些条件为真时，你可以用 notify 和 notifyAll 来通知那些等待中的线程重新开始运行。不同之处在于，notify 仅仅通知一个线程，并且我们不知道哪个线程会收到通知，然而 notifyAll 会通知所有等待中的线程。换言之，如果只有一个线程在等待一个信号灯，notify和notifyAll都会通知到这个线程。但如果多个线程在等待这个信号灯，那么notify只会通知到其中一个，而其它线程并不会收到任何通知，而notifyAll会唤醒所有等待中的线程。 如何使用Wait 尽管关于wait和notify的概念很基础，它们也都是Object类的函数，但用它们来写代码却并不简单。如果你在面试中让应聘者来手写代码，用wait和notify解决生产者消费者问题，我几乎可以肯定他们中的大多数都会无所适从或者犯下一些错误，例如在错误的地方使用 synchronized 关键词，没有对正确的对象使用wait，或者没有遵循规范的代码方法。说实话，这个问题对于不常使用它们的程序员来说确实令人感觉比较头疼。 第一个问题就是，我们怎么在代码里使用wait()呢？因为wait()并不是Thread类下的函数，我们并不能使用Thread.call()。事实上很多Java程序员都喜欢这么写，因为它们习惯了使用Thread.sleep()，所以他们会试图使用wait() 来达成相同的目的，但很快他们就会发现这并不能顺利解决问题。正确的方法是对在多线程间共享的那个Object来使用wait。在生产者消费者问题中，这个共享的Object就是那个缓冲区队列。 第二个问题是，既然我们应该在synchronized的函数或是对象里调用wait，那哪个对象应该被synchronized呢？答案是，那个你希望上锁的对象就应该被synchronized，即那个在多个线程间被共享的对象。在生产者消费者问题中，应该被synchronized的就是那个缓冲区队列。（我觉得这里是英文原文有问题……本来那个句末就不应该是问号不然不太通……） 永远在循环（loop）里调用 wait 和 notify，不是在 If 语句，常用的是while循环 现在你知道wait应该永远在被synchronized的背景下和那个被多线程共享的对象上调用，下一个一定要记住的问题就是，你应该永远在while循环，而不是if语句中调用wait。因为线程是在某些条件下等待的——在我们的例子里，即“如果缓冲区队列是满的话，那么生产者线程应该等待”，你可能直觉就会写一个if语句。但if语句存在一些微妙的小问题，导致即使条件没被满足，你的线程你也有可能被错误地唤醒。所以如果你不在线程被唤醒后再次使用while循环检查唤醒条件是否被满足，你的程序就有可能会出错——例如在缓冲区为满的时候生产者继续生成数据，或者缓冲区为空的时候消费者开始消耗数据。所以记住，永远在while循环而不是if语句中使用wait！我会推荐阅读《Effective Java》，这是关于如何正确使用wait和notify的最好的参考资料。 就像我之前说的一样，在while循环里使用wait的目的，是在线程被唤醒的前后都持续检查条件是否被满足。如果条件并未改变，wait被调用之前notify的唤醒通知就来了，那么这个线程并不能保证被唤醒，有可能会导致死锁问题。 下面我们提供一个使用wait和notify的范例程序。在这个程序里，我们使用了上文所述的一些代码规范。 我们有两个线程，分别名为PRODUCER（生产者）和CONSUMER（消费者），他们分别继承了了Producer和Consumer类，而Producer和Consumer都继承了Thread类。Producer和Consumer想要实现的代码逻辑都在run()函数内。 Main线程开始了生产者和消费者线程，并声明了一个LinkedList作为缓冲区队列（在Java中，LinkedList实现了队列的接口）。生产者在无限循环中持续往LinkedList里插入随机整数直到LinkedList满。我们在while(queue.size == maxSize)循环语句中检查这个条件。请注意到我们在做这个检查条件之前已经在队列对象上使用了synchronized关键词，因而其它线程不能在我们检查条件时改变这个队列。如果队列满了，那么PRODUCER线程会在CONSUMER线程消耗掉队列里的任意一个整数，并用notify来通知PRODUCER线程之前持续等待。在我们的例子中，wait和notify都是使用在同一个共享对象上的。 这一套程序如果能在面试中写出来那就给劲了，其实逻辑并不难，只需要按照套路来即可。 import java.util.LinkedList; import java.util.Queue; import java.util.Random; /** * Simple Java program to demonstrate How to use wait, notify and notifyAll() * method in Java by solving producer consumer problem. * * @author Javin Paul */ public class ProducerConsumerInJava { public static void main(String args[]) { System.out.println(&quot;How to use wait and notify method in Java&quot;); System.out.println(&quot;Solving Producer Consumper Problem&quot;); Queue&lt;Integer&gt; buffer = new LinkedList&lt;&gt;(); int maxSize = 10; Thread producer = new Producer(buffer, maxSize, &quot;PRODUCER&quot;); Thread consumer = new Consumer(buffer, maxSize, &quot;CONSUMER&quot;); producer.start(); consumer.start(); } } /** * Producer Thread will keep producing values for Consumer * to consumer. It will use wait() method when Queue is full * and use notify() method to send notification to Consumer * Thread. * * @author WINDOWS 8 * */ class Producer extends Thread { private Queue&lt;Integer&gt; queue; private int maxSize; public Producer(Queue&lt;Integer&gt; queue, int maxSize, String name){ super(name); this.queue = queue; this.maxSize = maxSize; } @Override public void run() { while (true) { synchronized (queue) { while (queue.size() == maxSize) { //queue满了，那么Producer就要被wait try { System.out .println(&quot;Queue is full, &quot; + &quot;Producer thread waiting for &quot; + &quot;consumer to take something from queue&quot;); queue.wait(); } catch (Exception ex) { ex.printStackTrace(); } } Random random = new Random(); int i = random.nextInt(); System.out.println(&quot;Producing value : &quot; + i); queue.add(i); queue.notifyAll(); } } } } /** * Consumer Thread will consumer values form shared queue. * It will also use wait() method to wait if queue is * empty. It will also use notify method to send * notification to producer thread after consuming values * from queue. * * @author WINDOWS 8 * */ class Consumer extends Thread { private Queue&lt;Integer&gt; queue; private int maxSize; public Consumer(Queue&lt;Integer&gt; queue, int maxSize, String name){ super(name); this.queue = queue; this.maxSize = maxSize; } @Override public void run() { while (true) { synchronized (queue) { while (queue.isEmpty()) { //queue空的时候，Consumer要被wait System.out.println(&quot;Queue is empty,&quot; + &quot;Consumer thread is waiting&quot; + &quot; for producer thread to put something in queue&quot;); try { queue.wait(); } catch (Exception ex) { ex.printStackTrace(); } } System.out.println(&quot;Consuming value : &quot; + queue.remove()); queue.notifyAll(); } } } } 参考资料：http://www.importnew.com/16453.html如何在 Java 中正确使用 wait, notify 和 notifyAll – 以生产者消费者模型为例 为了更好地理解这个程序，我建议你在debug模式里跑这个程序。一旦你在debug模式下启动程序，它会停止在PRODUCER或者CONSUMER线程上，取决于哪个线程占据了CPU。因为两个线程都有wait()的条件，它们一定会停止，然后你就可以跑这个程序然后看发生什么了（很有可能它就会输出我们以上展示的内容）。你也可以使用Eclipse里的Step into和Step over按钮来更好地理解多线程间发生的事情。 你可以使用wait和notify函数来实现线程间通信。你可以用它们来实现多线程（&gt;3）之间的通信。 永远在synchronized的函数或对象里使用wait、notify和notifyAll，不然Java虚拟机会生成 IllegalMonitorStateException。 永远在while循环里而不是if语句下使用wait。这样，循环会在线程睡眠前后都检查wait的条件，并在条件实际上并未改变的情况下处理唤醒通知。 永远在多线程间共享的对象（在生产者消费者模型里即缓冲区队列）上使用wait。 基于前文提及的理由，更倾向用 notifyAll()，而不是 notify()。 来点新东西吧在JAVA1.8或1.9中，并发工具的改变 Java 8 中 Concurrent package的改变 java.util.concurrent中新的类和接口 增加了两个新接口和4个新类: 接口 CompletableFuture.AsynchronousCompletionTask接口 CompletionStage类 CompletableFuture类 ConcurrentHashMap.KeySetView类 CountedCompleter类 CompletionExceptionjava.util.concurrent.ConcurrentHashMap的新方法 集合框架 在Java 8中做了修订，基于 stream 和 lambda表达式 添加了很多聚合方法。因此 ConcurrentHashMap 也引入了30几个新方法，包括各种 foreach 方法(forEach , forEachKey , forEachValue , 和 forEachEntry )、搜索方法( search , searchKeys , searchValues , 和 searchEntries )和reduction方法( reduce ,reduceToDouble , reduceToLong 等)。 也添加了一些其它方法，比如 mappingCount 和 newKeySet 。并且当前版本的 ConcurrentHashMap 的更适合做cache，因为增加了当键值不存在的时候的检查方法。 java.util.concurrent.atomic中的新类 为了并发计算count、sum， 新引入了 DoubleAccumulator , DoubleAdder , LongAccumulator , LongAdder 类，比Atomic提供更高的吞吐率。 java.util.concurrent.ForkJoinPool的新方法 静态的 commonPool() 新加入，可以为ForkJoinTask提供通用池。 两个方法 getCommonPoolParallelism() 和 commonPool() 提供不同的配置。 新类 java.util.concurrent.locks.StampedLock 新类 StampedLock 提供三种模式(写，读，乐观读)，用来提高性能。 Java 9 中 Concurrent package的改变 主要是 JEP 266: More Concurrency Updates , 包括publish-subscribe, CompletableFuture 接口的加强等。 支持Reactive Streams publish-subscribe框架，四个接口 Processor 、 Publisher 、 Subscriber 、 Subscription ，容器类 java.util.concurrent.Flow 、java.util.concurrent.SubmissionPublisherCompletableFuture类加强，支持delays, timeout, subclassing 以及其它方法调优以及修改javadoc 可以参考：http://www.importnew.com/28319.html 普及JAVA内存管理机制因为线程调度跟内存分配有着很大的关系。 转载内容，觉得这篇是我看过讲得最好的：https://blog.csdn.net/u013142781/article/details/50830754 请注意上图的这个： 我们再来复习下进程与线程吧： 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源分配和调度的一个独立单位。 线程是进程的一个实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源(如程序计数器，一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。 似乎现在更好理解了一些： 方法区和堆是分配给进程的，也就是所有线程共享的。 而栈和程序计数器，则是分配给每个独立线程的，是运行过程中必不可少的资源。 下面我们逐个看下栈、堆、方法区和程序计数器。 1、方法区（Method Area） 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 2、程序计数器（Program Counter Register） 程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 下面重点解下Java内存管理中的栈和堆。 3、栈（Stacks） 在Java中，JVM中的栈记录了线程的方法调用。每个线程拥有一个栈。在某个线程的运行过程中，如果有新的方法调用，那么该线程对应的栈就会增加一个存储单元，即帧(frame)。在frame中，保存有该方法调用的参数、局部变量和返回地址。 Java的参数和局部变量只能是基本类型的变量(比如int)，或者对象的引用(reference)。因此，在栈中，只保存有基本类型的变量和对象引用。引用所指向的对象保存在堆中。(引用可能为Null值，即不指向任何对象)。 当被调用方法运行结束时，该方法对应的帧将被删除，参数和局部变量所占据的空间也随之释放。线程回到原方法，继续执行。当所有的栈都清空时，程序也随之运行结束。 本地方法栈与虚拟机栈的区别： 本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 4、堆（Heap） 堆是JVM中一块可自由分配给对象的区域。当我们谈论垃圾回收(garbage collection)时，我们主要回收堆(heap)的空间。 Java的普通对象存活在堆中。与栈不同，堆的空间不会随着方法调用结束而清空。因此，在某个方法中创建的对象，可以在方法调用结束之后，继续存在于堆中。这带来的一个问题是，如果我们不断的创建新的对象，内存空间将最终消耗殆尽。 垃圾回收（Garbage Collection，GC） 垃圾回收(garbage collection，简称GC)可以自动清空堆中不再使用的对象。垃圾回收机制最早出现于1959年，被用于解决Lisp语言中的问题。垃圾回收是Java的一大特征。并不是所有的语言都有垃圾回收功能。比如在C/C++中，并没有垃圾回收的机制。程序员需要手动释放堆中的内存。 由于不需要手动释放内存，程序员在编程中也可以减少犯错的机会。利用垃圾回收，程序员可以避免一些指针和内存泄露相关的bug(这一类bug通常很隐蔽)。但另一方面，垃圾回收需要耗费更多的计算时间。垃圾回收实际上是将原本属于程序员的责任转移给计算机。使用垃圾回收的程序需要更长的运行时间。 在Java中，对象的是通过引用使用的(把对象相像成致命的毒物，引用就像是用于提取毒物的镊子)。如果不再有引用指向对象，那么我们就再也无从调用或者处理该对象。这样的对象将不可到达(unreachable)。垃圾回收用于释放不可到达对象所占据的内存。这是垃圾回收的基本原则。 早期的垃圾回收采用引用计数(reference counting)的机制。每个对象包含一个计数器。当有新的指向该对象的引用时，计数器加1。当引用移除时，计数器减1。当计数器为0时，认为该对象可以进行垃圾回收。 然而，一个可能的问题是，如果有两个对象循环引用(cyclic reference)，比如两个对象互相引用，而且此时没有其它(指向A或者指向B)的引用，我们实际上根本无法通过引用到达这两个对象。 因此，我们以栈和static数据为根(root)，从根出发，跟随所有的引用，就可以找到所有的可到达对象。也就是说，一个可到达对象，一定被根引用，或者被其他可到达对象引用。 5、再整理下 通常我们定义一个基本数据类型的变量，一个对象的引用，还有就是函数调用的现场保存都使用内存中的栈空间； 而通过new关键字和构造器创建的对象放在堆空间； 程序中的字面量（literal）如直接书写的100、”hello”和常量都是放在静态区中； 栈空间操作起来最快但是栈很小，通常大量的对象都是放在堆空间，理论上整个内存没有被其他进程使用的空间甚至硬盘上的虚拟内存都可以被当成堆空间来使用。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA I/O流详解]]></title>
    <url>%2F2018%2F03%2F28%2Fjava-io%2F</url>
    <content type="text"><![CDATA[流的概念和作用流是一组有顺序的，有起点和终点的字节集合，是对数据传输的总称或抽象。即数据在两设备间的传输称为流，流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。集合框架相当于数据结构，那么IO框架就相当于数据传输。 IO流的分类根据处理数据类型的不同分为：字符流和字节流根据数据流向不同分为：输入流和输出流 对输入流只能进行读操作，对输出流只能进行写操作，程序中需要根据待传输数据的不同特性而使用不同的流。 java.io包中提供了大量的流类，其中所有的输入流都是InputStream抽象类或抽象类Reader的子类，而所有的输出流都是outputStream抽象类或Writer的抽象类，所有继承自InputStream与outputStream的流都是字节流，而所有继承自Reader与Writer的流都是字符流。 要知道所有的高级流都需要借助低级流来操作文件，字节流属于低级流，而字符流属于改进只针对于文字的高级流，但是字符流中也有方法可以直接操作文件不需要借助低级流，但是一般的高级流操作都需要以低级流作为基础。 其中IO流具体的类或接口有哪些；File 文件类RandomAccessFile 随机存取文件类InputStream 字节输入流OutputStream 字节输出流Reader 字符输入流writer 字符输出流 首先先从文件的路径解释下： 文件和目录路径名的抽象表示形式。 用户界面和操作系统使用与系统相关的路径名字符串 来命名文件和目录。此类呈现分层路径名的一个抽象的、与系统无关的视图。抽象路径名 有两个组件： 一个可选的与系统有关的前缀 字符串，比如盘符，”/“ 表示 UNIX 中的根目录，”\\“ 表示 Microsoft Windows UNC 路径名。零个或更多字符串名称 的序列。抽象路径名中的第一个名称是目录名，对于 Microsoft Windows UNC 路径名则是主机名。抽象路径名中第一个名称之后的每个名称表示一个目录；最后一个名称既可以表示目录，也可以表示文件。空 抽象路径名没有前缀和名称序列。路径名字符串与抽象路径名之间的转换与系统有关。将抽象路径名转换为路径名字符串时，每个名称与下一个名称之间用一个默认分隔符 隔开。默认名称分隔符由系统属性 file.separator 定义，可通过此类的公共静态字段 separator 和 separatorChar 使其可用。将路径名字符串转换为抽象路径名时，可以使用默认名称分隔符或者底层系统支持的任何其他名称分隔符来分隔其中的名称。 无论是抽象路径名还是路径名字符串，都可以是绝对 路径名或相对 路径名。绝对路径名是完整的路径名，不需要任何其他信息就可以定位它所表示的文件。相反，相对路径名必须使用取自其他路径名的信息进行解释。默认情况下，java.io 包中的类总是根据当前用户目录来解析相对路径名。此目录由系统属性 user.dir 指定，通常是 Java 虚拟机的调用目录。 调用此类的 getParent() 方法可以获取抽象路径名的父 路径名，它由路径名前缀以及路径名名称序列中的每个名称（最后一个除外）组成。对于任何具有绝对抽象路径名的 File 对象，如果其绝对抽象路径名以某个目录的绝对路径名开头，那么该目录的绝对路径名是该 File 对象的祖先。例如，抽象路径名 “/usr” 表示的目录是路径名 “/usr/local/bin” 所表示目录的一个祖先。 在处理 UNIX 平台的根目录，以及 Microsoft Windows 平台的盘符、根目录和 UNC 路径名时，将用到前缀这一概念。如下所示： 对于 UNIX 平台，绝对路径名的前缀始终是 “/“。相对路径名没有前缀。表示根目录的绝对路径名的前缀为 “/“ 且名称序列为空。对于 Microsoft Windows 平台，包含盘符的路径名前缀由驱动器号和一个 “:” 组成。如果路径名是绝对路径名，还可能后跟 “\“。UNC 路径名的前缀是 “\\“；主机名和共享名是名称序列中的前两个名称。没有指定驱动器的相对路径名没有前缀。此类的实例可能表示（也可能不表示）实际文件系统对象，如文件或目录。如果它表示这种对象，那么该对象驻留在一个分区 中。分区是文件系统特定于操作系统的存储分区。一个存储设备（例如，物理磁盘驱动器、闪存、CD-ROM）可以包含多个分区。对象（如果有）将驻留在此路径名（绝对形式）某个祖先指定的分区上。 文件系统可以实现对实际文件系统对象上的某些操作（比如，读、写、执行）进行限制。这些限制统称为访问权限。文件系统可以对一个对象设置多个访问权限。例如，一个设置可能适用于对象的所有者，另一个设置则可能适用于所有其他用户。对象上的访问权限可能导致此类的某些方法执行失败。 File最基础的文件流，java 处理文件的类 File,java提供了十分详细的文件处理方法主要是创建文件，（不存在则新建，存在则替代），主要是封装了所有文件的属性和元数据。 File类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 File 类的实例是不可变的；也就是说，一旦创建，File 对象表示的抽象路径名将永不改变。 这个类中的方法有常用几个： boolean mkdir()创建此抽象路径名指定的目录。boolean mkdirs()创建此抽象路径名指定的目录，包括所有必需但不存在的父目录。boolean renameTo(File dest)重新命名此抽象路径名表示的文件。long length()返回由此抽象路径名表示的文件的长度。boolean isDirectory()测试此抽象路径名表示的文件是否是一个目录。boolean isFile()测试此抽象路径名表示的文件是否是一个标准文件。boolean isHidden()测试此抽象路径名指定的文件是否是一个隐藏文件。long lastModified()返回此抽象路径名表示的文件最后一次被修改的时间。String getName()返回由此抽象路径名表示的文件或目录的名称。String getParent()返回此抽象路径名父目录的路径名字符串；如果此路径名没有指定父目录，则返回 null。File getParentFile()返回此抽象路径名父目录的抽象路径名；如果此路径名没有指定父目录，则返回 null。boolean delete()删除此抽象路径名表示的文件或目录。boolean exists()测试此抽象路径名表示的文件或目录是否存在。boolean createNewFile()当且仅当不存在具有此抽象路径名指定名称的文件时，不可分地创建一个新的空文件。 public class FileExample{ public static void main(String[] args) { createFile(); } /** * 文件处理示例 */ public static void createFile() { File f=new File(&quot;E:/电脑桌面/jar/files/create.txt&quot;); try{ f.createNewFile(); //当且仅当不存在具有此抽象路径名指定名称的文件时，不可分地创建一个新的空文件。 System.out.println(&quot;该分区大小&quot;+f.getTotalSpace()/(1024*1024*1024)+&quot;G&quot;); //返回由此抽象路径名表示的文件或目录的名称。 f.mkdirs(); //创建此抽象路径名指定的目录，包括所有必需但不存在的父目录。 // f.delete(); // 删除此抽象路径名表示的文件或目录 System.out.println(&quot;文件名 &quot;+f.getName()); // 返回由此抽象路径名表示的文件或目录的名称。 System.out.println(&quot;文件父目录字符串 &quot;+f.getParent());// 返回此抽象路径名父目录的路径名字符串；如果此路径名没有指定父目录，则返回 null。 }catch (Exception e) { e.printStackTrace(); } } } RamdomAccessFile该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点： 该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw)，r是只读不能写，rw是读写模式。注意：该对象在实例化时，如果要操作的文件不存在，会自动创建；如果文件存在，写数据未指定位置，会从头开始写，即覆盖原有的内容。 可以用于多线程下载或多个线程同时写数据到文件。 此类的实例支持对随机访问文件的读取和写入。随机访问文件的行为类似存储在文件系统中的一个大型 byte 数组。存在指向该隐含数组的光标或索引，称为文件指针；输入操作从文件指针开始读取字节，并随着对字节的读取而前移此文件指针。如果随机访问文件以读取/写入模式创建，则输出操作也可用；输出操作从文件指针开始写入字节，并随着对字节的写入而前移此文件指针。写入隐含数组的当前末尾之后的输出操作导致该数组扩展。该文件指针可以通过 getFilePointer 方法读取，并通过 seek 方法设置。 通常，如果此类中的所有读取例程在读取所需数量的字节之前已到达文件末尾，则抛出 EOFException（是一种 IOException）。如果由于某些原因无法读取任何字节，而不是在读取所需数量的字节之前已到达文件末尾，则抛出 IOException，而不是 EOFException。需要特别指出的是，如果流已被关闭，则可能抛出 IOException。 常用方法： void close()关闭此随机访问文件流并释放与该流关联的所有系统资源。long getFilePointer()返回此文件中的当前偏移量。long length()返回此文件的长度。int read()从此文件中读取一个数据字节。int read(byte[] b)将最多 b.length 个数据字节从此文件读入 byte 数组。int read(byte[] b, int off, int len)将最多 len 个数据字节从此文件读入 byte 数组。void seek(long pos)设置到此文件开头测量到的文件指针偏移量，在该位置发生下一个读取或写入操作。void setLength(long newLength)设置此文件的长度。void write(int b)向此文件写入指定的字节。 InputStreamInputStream 是所有的输入字节流的父类，它是一个抽象类。ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。PipedInputStream 是从与其它线程共用的管道中读取数据，与Piped 相关的知识后续单独介绍。ObjectInputStream 和所有FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 常用方法：void close()关闭此输入流并释放与该流关联的所有系统资源。void mark(int readlimit)在此输入流中标记当前的位置。boolean markSupported()测试此输入流是否支持 mark 和 reset 方法。abstract int read()从输入流中读取数据的下一个字节。int read(byte[] b)从输入流中读取一定数量的字节，并将其存储在缓冲区数组 b 中。int read(byte[] b, int off, int len)将输入流中最多 len 个数据字节读入 byte 数组。void reset()将此流重新定位到最后一次对此输入流调用 mark 方法时的位置。long skip(long n)跳过和丢弃此输入流中数据的 n 个字节。 有哪些常用的子类：FileInputStream，ObjectInputStream等 分别实例其用法，方便快速回顾： 1.FileInputStream（文件字节输入流） package io; import java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.io.InputStream; public class ByteInput { public static void main(String[] args) throws IOException { //1、定义要使用的文件 File file = new File(&quot;F:&quot; + File.separator + &quot;byteInput.txt&quot;); file.createNewFile(); //文件存在的时候不会执行，不存在的时候会执行 //2、定义字节输入流指定为文件输入流 InputStream input = new FileInputStream(file); byte[] b = new byte[(int) file.length()]; // file.length()获取文件的长度返回long类型 int len = input.read(b); input.close(); //3、验证输入结果 System.out.println(&quot;文件的内容长度为 : &quot; + len); System.out.println(&quot;文件的内容为: &quot; + new String(b)); } } 2.ObjectInputStream（对象输入流）本例需要对象实现序列化接口，实现对文件内容的逐个对象处理先定义一个实现Serializable接口的pojo实体类 package io; import java.io.Serializable; public class StudentInfo implements Serializable{ private String stuno; private String name; private Integer age; public StudentInfo() { } public StudentInfo(String stuno, String name, Integer age) { super(); this.stuno = stuno; this.name = name; this.age = age; } //省略所有get/set方法 } package io; import java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.io.ObjectInputStream; public class ObjectInput { public static void main(String[] args) throws IOException, ClassNotFoundException { File file=new File(&quot;F:&quot;+File.separator+&quot;object.txt&quot;); file.createNewFile(); ObjectInputStream in=new ObjectInputStream(new FileInputStream(file)); StudentInfo stu=(StudentInfo)in.readObject(); in.close(); System.out.println(stu); } } OutputStreamOutputStream 是所有的输出字节流的父类，它是一个抽象类。ByteArrayOutputStream、FileOutputStream 是两种基本的介质流，它们分别向Byte 数组、和本地文件中写入数据。PipedOutputStream 是向与其它线程共用的管道中写入数据，ObjectOutputStream 和所有FilterOutputStream 的子类都是装饰流。 常用方法：void close()关闭此输出流并释放与此流有关的所有系统资源。void flush()刷新此输出流并强制写出所有缓冲的输出字节。void write(byte[] b)将 b.length 个字节从指定的 byte 数组写入此输出流。void write(byte[] b, int off, int len)将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此输出流。abstract void write(int b)将指定的字节写入此输出流。 有哪些常用的子类呢：FileOutputStream，ObjectOutputStream等 分别实例其用法，方便快速回顾： 1.FileOutputStream（文件字节输出流）实现对文件内容的逐字节处理 package io; import java.io.File; import java.io.FileOutputStream; import java.io.IOException; import java.io.OutputStream; public class ByteOutput { public static void main(String[] args) throws IOException{ //1、获取要操作的文件 File file=new File(&quot;F:&quot;+File.separator+&quot;byteOutput.txt&quot;); file.createNewFile(); //2、写入指定的内容 String str=&quot;I Like Java!&quot;; OutputStream output=new FileOutputStream(file); output.write(str.getBytes(), 0, str.length()); //写入字符串 output.close(); } } 2.ObjectOutputStream（对象输出流）本例需要对象实现序列化接口，实现对文件内容的逐个对象处理pojo对象同5例中的StudentInfo对象，测试类如下（用到ObjectInputStream的那个对象） package io; import java.io.File; import java.io.FileOutputStream; import java.io.IOException; import java.io.ObjectOutputStream; public class ObjectOutput { public static void main(String[] args) throws IOException { File file=new File(&quot;F:&quot;+File.separator+&quot;object.txt&quot;); file.createNewFile(); StudentInfo student=new StudentInfo(&quot;10001&quot;,&quot;zhangsan&quot;,20); ObjectOutputStream output=new ObjectOutputStream(new FileOutputStream(file)); output.writeObject(student); output.close(); } } ReaderReader 是所有的输入字符流的父类，它是一个抽象类。CharReader、StringReader 是两种基本的介质流，它们分别将Char 数组、String中读取数据。PipedReader 是从与其它线程共用的管道中读取数据。BufferedReader 很明显就是一个装饰器，它和其子类负责装饰其它Reader 对象。FilterReader 是所有自定义具体装饰流的父类，其子类PushbackReader 对Reader 对象进行装饰，会增加一个行号。InputStreamReader 是一个连接字节流和字符流的桥梁，它将字节流转变为字符流。FileReader 可以说是一个达到此功能、常用的工具类，在其源代码中明显使用了将FileInputStream 转变为Reader 的方法。我们可以从这个类中得到一定的技巧。Reader 中各个类的用途和使用方法基本和InputStream 中的类使用一致。后面会有Reader 与InputStream 的对应关系。 常用子类：FileReader，BufferedReader等 分别实例其用法，方便快速回顾： 1.FileReader（文件字符输入流）实现对文件内容的逐字符处理 package io; import java.io.File; import java.io.FileReader; import java.io.IOException; import java.io.Reader; public class CharInput { public static void main(String[] args) throws IOException { //1、指定要操作的文件 File file=new File(&quot;F:&quot;+File.separator+&quot;charInput.txt&quot;); file.createNewFile(); //2、指定字节输入流 Reader reader=new FileReader(file); char[] c=new char[(int)file.length()]; int len=reader.read(c); reader.close(); //3、验证 System.out.println(&quot;字符流读取文件的长度为: &quot;+len); System.out.println(&quot;字符流读取文件的内容: &quot;+new String(c)); } } 2.BufferedReader（缓存文件输入流）实现对文件内容的逐行处理 package io; import java.io.BufferedReader; import java.io.File; import java.io.FileReader; import java.io.IOException; public class BufferReaderDemo { public static void main(String[] args) throws IOException { //指定文件 File file = new File(&quot;F:&quot; + File.separator + &quot;buffered.txt&quot;); file.createNewFile(); //定义需要验证的变量 int i = 1; String str; StringBuffer buffer = new StringBuffer(); //定义逐行读入的流 BufferedReader br = new BufferedReader(new FileReader(file)); while ((str = br.readLine()) != null) { //逐行读取并验证 System.out.println(&quot;读取的行数: &quot; + (i)); buffer.append(str); System.out.println(&quot;第&quot; + (i++) + &quot;行的内容为: &quot; + str); } br.close(); //打印最终结果 System.out.println(&quot;\n文件中的全部内容为: &quot;+buffer.toString()); } } WriterWriter 是所有的输出字符流的父类，它是一个抽象类。CharArrayWriter、StringWriter 是两种基本的介质流，它们分别向Char 数组、String 中写入数据。PipedWriter 是向与其它线程共用的管道中写入数据，BufferedWriter 是一个装饰器为Writer 提供缓冲功能。PrintWriter 和PrintStream 极其类似，功能和使用也非常相似。OutputStreamWriter 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类（具体可以研究一SourceCode）。功能和使用和OutputStream 极其类似，后面会有它们的对应图。 常用子类：FileWriter，BufferedWriter等 分别实例其用法，方便快速回顾： 1.FileWriter（文件字符输出流）实现对文件内容的逐字符处理 package io; import java.io.File; import java.io.FileWriter; import java.io.IOException; import java.io.Writer; public class CharOutput { public static void main(String[] args) throws IOException { File file = new File(&quot;F:&quot; + File.separator + &quot;charOutput.txt&quot;); file.createNewFile(); Writer writer = new FileWriter(file); writer.write(&quot;I Love Basketball！&quot;, 0, 18); writer.close(); } } 2.BufferedWriter（缓存文件输出流）实现对文件内容的逐行处理 package io; import java.io.BufferedWriter; import java.io.File; import java.io.FileWriter; import java.io.IOException; import java.io.Writer; public class BufferedWriterDemo { public static void main(String[] args) throws IOException{ //指定文件 File file=new File(&quot;F:&quot;+File.separator+&quot;buffered.txt&quot;); file.createNewFile(); //指定 Writer bw=new BufferedWriter(new FileWriter(file,true)); bw.write(&quot;\r\n&quot;); bw.write(&quot;XiaoHuangRen like banana!&quot;); bw.write(&quot;\r\n&quot;); bw.write(&quot;XiaoHuangRen like bana!&quot;); bw.close(); } } 资料参考：https://blog.csdn.net/qq_34207422/article/details/76149026 那么如何使用这4种类型呢？关于字节流和字符流到底用哪个来处理实际的业务需求呢 字符流的由来： 因为数据编码的不同，而有了对字符进行高效操作的流对象。本质其实就是基于字节流读取时，去查了指定的码表。 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。字节流可以处理全部数据类型的传输，但是字符流就只适合读写纯文字的数据。 关于缓冲流（常用）java.io提供了四种数据传输的缓冲技术 BufferedInputStreamBufferedOutputStreamBufferedReaderBufferedWriter 其常用的构造方法： BufferedReader（Reader in）BufferedReader（Reader in,int sz）//sz为自定义缓冲区大小BufferedWriter （Writer out）BufferedWriter （ Writer out, int sz）BufferedInputStream（InputStream in）BufferedInputStream（ InputStream in,int size）BufferedOutputStream （OutputStream out）BufferedOutputStream （OutputStream out,int size）前四个四字符流，后四个是字节流 带缓冲的字节输入流：上面我们知道文件字节输入流的读取时，是直接同字节流中读取的。由于字节流是与硬件（存储介质）进行的读取，所以速度较慢。而CPU需要使用数据时通过read()、read(byte[])读取数据时就要受到硬件IO的慢速度限制。我们又知道，CPU与内存发生的读写速度比硬件IO快10倍不止，所以优化读写的思路就有了：在内存中建立缓存区，先把存储介质中的字节读取到缓存区中。CPU需要数据时直接从缓冲区读就行了，缓冲区要足够大，在被读完后又触发fill()函数自动从存储介质的文件字节内容中读取字节存储到缓冲区数组。 BufferedInputStream 内部有一个缓冲区，默认大小为8M，每次调用read方法的时候，它首先尝试从缓冲区里读取数据，若读取失败（缓冲区无可读数据），则选择从物理数据源 （譬如文件）读取新数据（这里会尝试尽可能读取多的字节）放入到缓冲区中，最后再将缓冲区中的内容返回给用户.由于从缓冲区里读取数据远比直接从存储介质读取速度快，所以BufferedInputStream的效率很高。 转换流（常用）字符流与字节流转换 转换流的特点： 其是字符流和字节流之间的桥梁可对读取到的字节数据经过指定编码转换成字符可对读取到的字符数据经过指定编码转换成字节何时使用转换流？ 当字节和字符之间有转换动作时；流操作的数据需要编码或解码时。具体的对象体现： InputStreamReader:字节到字符的桥梁OutputStreamWriter:字符到字节的桥梁这两个流对象是字符体系中的成员，它们有转换作用，本身又是字符流，所以在构造的时候需要传入字节流对象进来。 InputStreamReader演示： InputStreamReader是字节流通向字符流的桥梁，它使用指定的charset读取字节并将其解码为字符。它拥有一个InputStream类型的变量，并继承了Reader，使用了对象的适配器模式 根据InputStream的实例创建InputStreamReader的方法有4种： InputStreamReader(InputStream in);//根据默认字符集创建InputStreamReader(InputStream in, Charset cs);//使用给定字符集创建InputStreamReader(InputStream in, CharsetDecoder dec);//使用给定字符集解码器创建InputStreamReader(InputStream in, String charsetName);//使用指定字符集创建 后面的3个构造函数都指定了一个字符集，最后一个是最简单的，可以直接指定字符集的名称来创建，例如GB2312等。 每次调用InputStreamReader中的一个read()方法都会导致从底层输入流读取一个或多个字节。要启用从字节到字符的有效转换，可以提前从底层流读取更多的字节，使其超过满足当前读取操作所需的字节。共有3个可用的read()方法： int read();//读取单个字符int read(char[] cbuf, int offset, int length);//将字符读入数组中的某一部分boolean ready();//判断此流是否已经准备好用于读取 InputStreamReader继承自Reader，因此该类的实例可以被各种输入字符流包装。为了达到最高效率，可以考虑在BufferedReader内包装InputStreamReader。我们首先创建了一个FileInputStream类的实例，然后转换为InputStreamReader对象is，最后使用BufferedReader进行包装。这样就可以将字节流转换为带缓冲功能的字符流。 public class TestInputStreamReader { public static void main(String[] args) { try { // 创建输入流 FileInputStream fis = new FileInputStream(&quot;D:/demo/test.txt&quot;); InputStreamReader is = new InputStreamReader(fis); BufferedReader bis = new BufferedReader(is); // 从输入流读取数据 while (bis.ready()) { int c = bis.read(); System.out.print((char)c); } // 关闭输入流 bis.close(); is.close(); fis.close(); } catch (IOException e) { } } } OutputStreamWriter演示： OutputStreamWriter是字符流通向字节流的桥梁，可使用指定的charset将要写入流中的字符编码成字节。因此，它拥有一个OutputStream类型的变量，并继承了Writer，使用了对象的适配器模式 根据OutputStream的实例创建OutputStreamWriter的方法有4种： OutputStreamReader(OutputStream out);//根据默认字符集创建OutputStreamReader(OutputStream out, Charset cs);//使用给定字符集创建OutputStreamReader(OutputStream out, CharsetDecoder dec);//使用给定字符集解码器创建OutputStreamReader(OutputStream out, Stroutg charsetName);//使用指定字符集创建 后面的3个构造函数都制定了一个字符集，最后一个是最简单的，可以直接指定字符集的名称来创建，例如GB2312等。 每次调用write()方法都会导致在给定字符（或字符集）上调用编码转换器。在写入底层输出流之前，得到的这些字节将在缓冲区中累积。可以指定此缓冲区的大小，不过，默认的缓冲区对多数用途来说已足够大。注意，传递给write()方法的字符没有缓冲。共有3个可用的write()方法： void write(char[] cbuf, int off, int len);//写入字符数组的某一部分void write(int c);//写入单个字符void write(String str, int off, int len);//写入字符串的某一部分OutputStreamWriter继承自Writer，因此该类的实例可以被各种输出字符流包装。为了达到最高效率，可以考虑在BufferedWriter内包装OutputStreamWriter。我们首先创建了一个FileOutputStream类的实例，然后转换为OutputStreamReader对象os，最后使用BufferedWriter进行包装。这样就可以将字节流转换为带缓冲功能的字符流。 public class TestOutputStreamWriter { public static void main(String[] args) { try { // 创建输出流 FileOutputStream fos = new FileOutputStream(&quot;D:/demo/test.txt&quot;); OutputStreamWriter os = new OutputStreamWriter(fos); BufferedWriter bos = new BufferedWriter(os); // 写入数组数据 char[] buf = new char[3]; buf[0] = &apos;a&apos;; buf[1] = &apos;b&apos;; buf[2] = &apos;中&apos;; bos.write(buf); // 关闭输出流 bos.close(); os.close(); fos.close(); } catch (IOException e) { } } }]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创业之路--企业技术支持]]></title>
    <url>%2F2018%2F03%2F26%2F%E4%BB%8E%E6%97%A0%E5%88%B0%E6%9C%89%E6%90%AD%E5%BB%BA01%2F</url>
    <content type="text"><![CDATA[中小型互联网企业的技术支持到底有哪些（数据来源于网络爬虫及本人总结），项目包括多终端接口（WEB,IOS,Android），接入微服务架构，服务端软件层面主要语言为JAVA开发，考虑架构稳定性以及可拓展性，灵活性，硬件层面包括完整的企业级应用主机服务器，服务器商业托管服务，数据管理中心，云计算，大数据，OA系统，ERP软件，数据库储存方案等等，企业对接服务包括400电话，企业邮箱，企业公众号等等，关于这些技术我都会做总结。我尽量用最简洁的语言来做阐述，帮助我的合作伙伴解决技术疑惑。 硬件支持上面一堆屁话，让人云里雾里心生恐惧，为什么一个小型项目需要这么多的技术支持！！先解释一下，一个多终端设备并且以网络用户作为企业交互人群的项目公司都可以成为互联网企业，首先做一个略微完整的项目需要的最小项目软硬件架构是什么，首先先做下分析： 首先根据你的产品预估你的访问量和人数大概在什么位置： 网站统计中的PV(访问量)：UV(独立访客)：IP(独立IP)的定义与区别（名词解释） PV(访问量)：即Page View, 即页面浏览量或点击量，用户每次刷新即被计算一次。 UV(独立访客)：即Unique Visitor,访问您网站的一台电脑客户端为一个访客。00:00-24:00内相同的客户端只被计算一次。 IP(独立IP)：即Internet Protocol,指独立IP数。00:00-24:00内相同IP地址之被计算一次。 QPS：每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。 要想分析自己的产品大概需求是什么样的，可以看“日PV”这个数据，就是说网站在各种终端中累计点击多少次，打个比喻，你的产品拥有完整的各终端接口服务，一天有100个手机用户点击你的软件，每人点击数量不一致，可预估取中间值也就是20次刷新页面（考虑到现在的异步更新功能以减少页面刷新提高用户体验的技术，取20次），而手机系统分为IOS和Android，结果乘2,100202=4000次页面访问，web端属于PC端页面服务，考虑到现在移动端使用频繁于WEB端，所以取1000点击，那么你的软件日PV就是5000的访问量级。 目前互联网中如果某个系统的日pv在千万级别以上,他就可能是一个高并发的系统，中小型的互联网企业的日均PV在10w~300w这个级别。我们的项目系统不只有用户在访问，还有企业的访问量，开发的访问量等等，以这个区间作为系统搭建的参数。 下面贴出我去年做的软硬件系统拓展的流程图：可以看下互联网硬件为了应付日益增多的用户应该如何做好技术选型的 如果我们按照中小型互联网的PV参数作为技术选型标准的话，该从什么方面去选择硬件。 服务器首先一个企业的核心是一台交互处理所有信息的主机，也就是服务器，所有移动端接口都对接到这里，数据库也对接到这里，网站也对接这里，处理数据，储存数据，展示数据，页面访问控制，业务需要和所有的功能实现都要在一个服务器里实现。 这里有4个方案：（除了物理服务器是真实可见的，其他三个都升虚拟的） 1.云服务器2.物理服务器（前期转中期）3.VPS4.虚拟主机 区别和解释 什么是云服务器 云服务器是在一组集群服务器上虚拟出多个类似独立服务器的部分，集群中每个服务器上都有云服务器的一个镜像，从而大大提高了虚拟服务器的安全稳定性，除非所有的集群内服务器全部出现问题，云服务器才会无法访问。 云主机是一群服务器做镜像然后分割的虚拟机，价格适中，但是很稳定，基本没有宕机。宕机，指操作系统无法从一个严重系统错误中恢复过来，或系统硬件层面出问题，以致系统长时间无响应，而不得不重新启动计算机的现象。它属于电脑运作的一种正常现象，任何电脑都会出现这种情况。 云服务器就是将多个传统的服务器连接在一起，形成一个大的超级计算机，这个超级计算机里有多个类似独立服务器的部分，可以根据用户的需求提供给予其使用。由于可以按需付费，并且可以弹性伸缩，所以成本低廉，是现在很多企业倾向于选择的一种方式。而且现在做云服务器提供商越来越多，这也是市场的需求和选择，最著名的肯定就是BAT，还有一些专业型的比如小鸟云计算这一类公司，所以个人认为，这应该是一种时代趋势吧。 什么是物理服务器 （也叫独立服务器） 独立服务器是客户拥有整台服务器的软硬件资源，可以自行配置或通过主机管理工具实现web、mail、ftp等多种网络服务。由于整台服务器只有一个用户使用，在服务器硬件资源以及带宽资源上都得到了极大的保障。 优势/适用范围：稳定安全、独享带宽、可绑定多个IP地址、可单独设置防火墙，可扩展硬件等。适用于中高端用户。 服务器就是一台主机，你自己买自己用，没有专业维护人员的话要托管，费用比较高。一般公司不推荐。涉及到专业服务器硬件托管公司。 什么是VPS主机 VPS主机是在强大的 互联网服务器集群上，利用虚拟化及集中存储等技术构建的主机租用产品，每个VPS主机都是一台虚拟独立的服务器，具有完整的服务器功能，并且比同配置的物理服务器更灵活，具有更安全更稳定的性能。 具有完整的物理服务器 功能，同时具有高性价比、高安全性、高灵活性。适用于业务快速成长的的商业运营公司/需要各地分支机构共享内部资源筹建信息化服务平台的大中型行业门户网站。 vps是一台主机虚拟出来的，和人家合用一台机器，见不到物理的机器，但是可以挂自己的网站，价格便宜但是不稳定。 什么是虚拟主机 所谓虚拟主机，也叫“网站空间”，即把一台运行在互联网上的服务器划分成多个具有一定大小的硬盘空间，每个空间都给予相应的FTP权限和Web访问权限，以用于网站发布。 低成本高利用率，是中小企业提高企业竞争力的重要手段。适用于个人网站或中小型网站。 如何选型既然初创产业没那么多资金，物理服务器当然不需要考虑，而且没有这么多当量的数据量，购买或租用独立服务器的成本在创业预算范围之外。另外虚拟主机的局限性也有，只适合做门户网站，对于响应的业务处理没有两者优。 考虑云服务或者VPS。。。1.如果同台服务器的1台VPS被攻击，将直接影响其他的所有VPS无法运行。同等情况下云服务器只影响被攻击云计算服务器，其他同服务器的云计算服务器不受影响。 2.云计算服务器运行前会预先将硬件内存分配好,如果服务器上有4G内存绝不能分配出5G的内存出来,而VPS服务器自生拥有4G内存可以虚拟100G的内存，并分配给100个独立操作系统。 3.在同等性能的前提下,降低你在数据中心消费成本的一半以上 4.vps对运营商来说成本较低,因为1台服务器可以虚拟上数十个VPS主机,云计算服务器对运营商来说相当于独立服务器出租。成本略低于独立服务器出租,需要有相当实力的运营商才能提供。 目前提供云服务的国外亚马逊比较牛叉，国内的话有腾讯云、阿里云、盛大云等大企业。 产品选型建议选择云计算服务器，均衡性能 关于市面上的云计算产品的报价阿里云计算各项产品（报价均衡在每个月200到500不等） 还有阿里的全套产品线 腾讯云计算各项产品 移动端开发一个项目计划若想要加入移动端数据支持，那么需要多少成本来筹建到完成产品呢？ 就大部分的App项目而言，开发一个app标配的项目需要开发一套后台管理系统（CMS） + 安卓客户端 + iOS客户端，大多数项目的开发成本在12 - 25万不等，具体需要根据App的功能复杂度，质量要求，开发哪些平台等因素来确定具体的价格。另外，个人兼职，团队或工作室，或者专业的app开发公司，报价的成本会有比较大的差距，开发出来的质量也会有比较大的差距，一般报价的成本：个人 &lt; 团队 &lt; 公司，而质量也是跟价格成正比：个人兼职 &lt; 团队 &lt; 公司。 接下来分析如果一个初创公司若想实现移动端，无非就是两种开发手段 1.自建团队2.外包项目 首先初创公司想要召集一批人来做移动端实现，那么需要哪些人 开发一个App项目的人员配置比较复杂。开发网站只需要一个端的开发人员即可以了，而开发app需要三个端的开发人员，通常也相对同样功能的网站开发成本的三倍工作量。App项目的人员基本配置有：产品经理，项目经理一名，UI设计师一名，后台开发工程师两名，安卓开发工程师两名，iOS开发工程师两名，测试人员两名。这里是对专业的App开发公司而言，一般个人或团队可能一个人会身兼多职，所以开发出来的项目质量也不能得到保证； App开发的人工成本相对网站要高一些。通常一个有一两年安卓或iOS开发经验的开发人员，人工成本就要达到1万左右（由于当前的市场环境对开发人才的需求较高，使得技术开发的人工成本也较高）。通常一个app项目的基本人工成本达到2-5来万，再加上员工福利，设备，场地，人员管理，营销成本和商务沟通成本，基本上一个app项目的基本投入成本就要去到8-10万左右。这个暂且不计人员的空档期，招聘的成本，项目的风险等因素。这是对于一个已经具备成熟App开发团队的公司所需要付出的价位，对于完全没有App开发团队和相关开发经验的公司而言，这个成本估计至少需要翻一倍，开发周期要拉很长，基本才能达到前者类似的效果。 开发一个app需要多少钱，需要综合评估app的功能需求，质量要求，需要开发哪些平台端，以此评估出需要投入多少的人工设计和开发量，即可基本测算出app的开发成本。 自建团队的优点： 沟通高效：由于都在同个办公室，沟通方便，随时可以面对面交流，可以快速讨论出解决方案并执行； 需求把握更准确：选择开发公司合作，通常开发成本都需要在合同签订前就确定下来，所以设计通常也被预算所限定，一旦设计确定下来后，就不允许频繁地变更需求，除非只是一是很小的调整，或者额外追加开发的成本。而自建团队的话，通常开发过程中有一些不理想或不合理的设计，调整和优化的灵活度会高很多，可以更纯粹地考虑产品项目本身的合理性和用户体验性，弱化开发成本的控制； 可以充分配合实际的项目运营：比如后期项目需要做活动，或者临时有一些额外的需求需要增加，自建团队可以在极短的时间内讨论出方案并执行，而跟第三方技术团队合作，则前期需要沟通需求，评估开发成本，还有安排开发时间等等工作，通常需要好多个来回的沟通，导致浪费了好多时间； 自建团队的缺点： 开发周期长：创业前期需要组建团队，磨合，且人手经常不够用，导致开发周期会被拉长； 需要解决人员招聘，人员流动性等问题，特别是非技术基因的团队，很难招聘或留住技术人才； 需要分散很多的精力放在技术上面，特别是在项目启动前期，需要投入非常多的精力开发系统； 项目管理成本高：由于开发一套系统，在不同的时间点，需要不同的专业技能，且任务有前后置的衔接关系，这会导致经常在某个专业领域缺人，无法执行后续的开发，或者执行完某个任务后，就会空出一些人手暂时用不上，导致管理成本非常高。相对而言，技术公司由于同时会进行多个项目，所以可以最大程度减少人力成本的浪费，减少管理成本； 那么回归到我们这个问题本身：应该采用自建团队开发，还是找技术公司合作的方案呢？我觉得这个问题的核心关键点需要看创始团队的人员配置。 1.偏技术型团队：通常创始团队大都比较擅长技术，并且拥有丰富的开发经验。那么，自建技术团队开发是最省成本，也是最合适的方案。这里面存在两种情况：一是技术方面只有一个核心骨干，或只擅长其中的某一部分（如整套系统涵盖app开发和网站开发，但技术创始人只擅长web端的开发），这时会导致系统前期的开发周期会拉长，可以采取招聘其他的技术人才，或众包的方式；二是技术团队很强大，基本可以开发整套系统，那么基本可以自行搞定开发完整套系统。但通常技术类型的团队前期很容易犯的一个错误是，把大多数的时间投入在技术和系统的开发方面，却忽视了设计，营销推广等领域的积累和学习，导致一些项目开发出来后，却没能运营推广起来，最终导致项目半路夭折（比如我们团队初期就犯过这样一个错误）。 2.偏运营型团队：这种类型的团队比较擅长运营及营销推广这个领域，但不具备技术的基因。通常运营团队的创业项目核心在运营层面，技术相对次要，所以建议还是找一家技术开发公司合作，把技术相关的开发工作交给技术合作公司搞定，团队专攻运营领域。有些创业者由于预算有限，技术公司的开发成本又太高，可能会觉得自己搭建一个技术团队会更省成本，更高效。我觉得这是一个错误的认知。首先，运营型的团队通常不在技术这个圈子，认识的技术人才非常有限，加上创业前期招聘人才本身就比较困难，通常需要很长的时间才能招聘到相应的技术人才，可能还不是优秀的技术人才；其次，开发一个IT系统需要的专业人才比较多，比如开发一个app，按照专业开发公司的配置，需要产品经理，UI设计师，安卓，iOS，后台开发工程师，测试工程师等专业人才，创业型公司不可能会有这么豪华的人才配置，通常都是一人当几个人用，而这样导致的结果是开发出来的系统质量一般；再者，通常系统的启动初期，对技术人才是一个比较大的缺口，但一旦系统开发完成，后期只需要较少量的运维工作，并不需要这么多的技术人员，所以可能会导致人员和成本浪费，这也是一块比较大的成本损失。当然，如果找技术公司合作的话，能否找到靠谱的合作团队，是这个项目成败的一个非常关键的要点。这方面建议多一点慎重的考量，不要为了节省小额的开发成本，而忽视了对项目质量，以及技术团队的要求。 3.综合配置型团队：相对而言，这种类型的创业团队人员配置较为合理，既有技术人才，也有运营和营销专长，各个领域也有相应的人才资源和圈子，所以项目的成活率也会高很多。这种类型的团队可以考虑找技术公司合作，也可以自己招聘技术人才自行开发，两个方案的可行性都比较高。从长远的角度看，如果要真正做好一个项目，最终还是需要建立自己的技术团队，这样才能减少沟通的成本，严格把控好每一个需求点，并打磨好项目的每一个细节。但这个可能需要基于团队有技术基因，或者资金允许的情况下。这需要一个过程，毕竟技术团队的招聘，搭建，磨合，开发流程的优化等等都需要有一段路要走，特别是对于没有技术基因的团队，可能是一个需要用很多精力和时间投入才能去克服的一个问题。除了创始团队的配置外，项目的类型（比如是偏技术型项目，像今日头条，还是偏运营的项目，像大多数自建的电商平台），也是其中的一个重点考量因素。创业者需要根据自身的团队的优劣势，人员的配置情况，项目类型，以及对于各个领域的人才需求情况做一个整体的评估，再决定是否需要自建一支技术团队。术业有专攻，创业初期，务必要把最核心的资源和力量，放在刀刃上面，并懂得借助第三方的资源和力量。因为第三方的资源和技术本身就是为了顺应创业市场的要求而出现的。 市场经验：（我们的项目属于综合配置型团队） 案例一：（有技术合伙人，对软件开发流程有详细认知，可维护，可选择外包后拿回源码自己改，后期迭代升级组建团队） 我公司这边也有不少项目是外包出去的。在初期，为了快速实现，把项目外包出去是一个好的选择，毕竟自建团队需要投入的资金和时间，还有管理都是很庞大的。但是慢慢的，问题也随之而来。对于外包项目，外包公司使用的是最简单最快实现的框架去做，对于各种情况诸如兼容性（此问题手机端尤其突出）、扩展性、软件访问效率等问题都不会考虑，他们只负责交付给你一个能用的产品，而不是可用的产品，并帮你维护一段时间，合约便结束了。而且在维护期内，如果你有什么其他需求或改动，都是需要另收费的，很麻烦，有时候即使你反应了问题，也不会立刻帮你修改，而是要等。更为麻烦的是，在之后，你还是需要找人去接手这个项目，拿回源码自己去做修改自己去做开发。所以，我的建议是，自建团队是最好的。我们公司的初期项目，为了市场推广的需要，先是找了外包公司做了webview，我们自己开发网页端嵌入，这样就能进行快速迭代，满足市场需要，所以页面的变动都是在网页端，而不需要经过外包公司和上架审核。然后在这段时间内自建ios和android团队，拿回外包的源码自己维护，以及自己开发真正的app，因为经过市场反馈，我们app需要的功能也很明确，之前的接口都是可用，所以开发可以很快.然后替换掉webview.. 案例二：（纯外包，没有技术合伙人，闹得很尴尬） 客户A：传统招聘行业，老板准备投几百万嫁装互联网的招聘平台，在朋友推荐下跟一家软件公司合作，以远超行业价格的费用签约，原本按行业水平三个月能开发好的系统，硬是拖了半年才完成，但功能简陋，设计毫无美观，项目因为不达标一直没有上线。一年后，准备投第二笔钱做项目的迭代优化，原先的供应商又报了一个离谱的价格。老板无奈之下重新选择了供应商，由于原开发的项目代码混乱，设计糟糕，把之前的项目推翻从零再次开发，但已错过了互联网项目最佳的推广时期；客户B：法律行业，拿了天使投资准备开发一个互联网法律服务平台，创始团队有律师背景，产品经理，运营人才，只差一个技术合伙人。在没有技术基础的情况下，选择自建技术团队，前前后后招聘技术人才花了不少时间（2014年互联网人才非常紧缺），后经过长达一年的项目开发，把一个一流的设计方案，开发出了三流的山寨效果，项目因为质量原因，迟迟不能上线，也基本把项目的所有开发预算耗完。后请求投资人意见，希望再度投钱重新启动开发项目；客户C：教育行业，由于朋友公司刚好有技术团队有空档，整个项目外包给朋友的技术开发团队，因技术负责人不懂产品设计，不擅沟通，且没有做好项目进度和质量管控，项目严重延期，且质量远远达不到预期。客户C无奈下，选择终止合作，但经过长达半个多月的交涉，才完成项目交接，且双方也闹得不太愉快。 如何找一个给力且满意的技术第三方？ 靠谱的技术服务商需要具备哪些条件？通过哪些渠道可以找到比较高质量的技术服务商？如何在锁定的几家技术服务商中选择一个最适合的团队合作？ 靠谱的技术服务商需要具备哪些条件？ 甲方–我们公司 乙方–第三方技术提供方 1.做事诚信靠谱：这个是首要考虑的条件，做事靠谱是最重要的，不然在后续的合作过程中会出现很多坑。比如价格低开高走，为了追求利润而进行错误的引导，提出的问题拖着不解决，各种不配合，设置系统的后门等等。有很多人咨询过关于合同制订的问题，怕合同里面有什么不合理的条约，我觉得合同这些都是次要的，在中国这样的人情社会，合同对于乙方的约束并不大，而且诉讼流程漫长而繁琐。核心的关键点还是在于乙方的靠谱程度，即使在合同约束范围外的问题，靠谱的服务商也会尽心尽力地去帮甲方解决问题； 2.专业能力强：这方面主要考察专业的深度问题，通常项目开发的成败主要是技术和设计这两方面决定的，设计决定了项目的呈现效果和交互体验，技术决定了项目的最终质量，稳定性和实际体验，所以需要重点对技术和设计的深度做了解。由于对接人一般是业务或产品经理，并不能对这方面有一个深入的了解，一般只能通过公司过往的实际案例，团队的基因，工作经验年限，公司的一些原创文章去做了解和判断。 3.综合能力全面：一个完整的项目开发流程不单单只是技术，还牵涉到设计，管理，测试等环节，如果在某一些环节上面出现严重的短板，务必对项目的最终效果会产生很大的影响。这里牵涉到专业能力的广度问题，涉及到的能力范畴有：商务的对接能力，需求的梳理能力，业务的理解能力，产品的设计能力，UI的设计能力，技术的开发能力，项目的管理能力，质量把控的测试能力。对于任意一家服务商，都有他们擅长的领域，通常创始团队的基因决定了公司擅长的领域，比如业务型的团队擅长营销，技术型的团队技术上面很有优势，而设计出身的团队能产出更好的设计方案。一般都没有面面俱全的公司，需要评估公司的综合能力，避免在某个领域存在致命的短板； 通过哪些渠道可以找到比较高质量的技术服务商？ 找行业内专业的朋友推荐（靠谱指数：5星）：这个是找服务商的首选方式，但朋友一定是专业，并且懂行的人，这个非常重要，因为他会帮你做初步的筛选和甄别，并且会站在客观的立场帮你做分析，推荐合适的公司或开发团队； 通过高质量的文章查找服务商（靠谱指数：4星）：通过知乎，微信，论坛，新媒体等平台，查找一些跟项目相关的高质量文章，如搜索”APP开发”，”微信开发”，”网站开发”之类的业务关键词，然后找到里面的一些高质量高水平的文章（硬广的广告一律pass掉），通常文章里面会留有作者或公司的一些联系方式，尝试跟这个领域的专家勾搭联系，然后让他（她）通过自荐或推荐的方式，找到匹配的技术服务商； 通过搜索引擎查找服务商（靠谱指数：3星）：相对前两种方式，通过这种方式可以快速地找到几百上千家技术服务商，这类公司通常知名度较高，但基本都是非常擅长投放百度竞价或SEO优化的公司，营销和市场能力较强，但技术和设计能力反而不是特别出色。由于业务量和咨询人数较多，做得好的公司有一定人数规模，收费相对而言也比较贵，性价比总体而言不高； 通过众包平台查找服务商（靠谱指数：3星）：比如通过程序员客栈，码市等众包平台寻找接包方。这类平台相对而言，聚焦了一批相对优质的个人开发者，价格相对猪八戒等平台要偏高一些，但相对整包给公司，价格要少一些。优点是质量相对可控，有一定的成本优势，缺点是由于项目的开发人员都通过远程协助开发的方式，且没有经过长期的团队配合，有些开发人员是兼职开发，对于项目周期及开发人员比较难把控，适合有技术合伙人的公司去对接零散的异地开发人员； 通过一些中介平台查找服务商（靠谱指数：2星）：比如猪八戒，一品威客，智城等。这类平台汇集了比较多的低端开发者和公司，价格便宜，但服务和项目质量低，适合一些小项目或对质量要求不高，价格敏感的创业者。 如何在锁定的几家技术服务商中选择一个最适合的团队合作？经过前面一轮初步的筛选和沟通，最终可以锁定了几家技术服务商，并经过细致的沟通后拿到了各个服务商的报价方案。那么，在最终选择服务商的时候，应该怎么选择最合适并且匹配的服务商呢？这里面牵涉到很多专业性的判断和技巧，如果身边有专业的朋友，建议咨询一下朋友的专业意见，从报价的合理性，服务商的技术，设计能力，需求梳理规划的合理性提供一些参考意见。 以下提供一些不需要专业技能进行判断选择的方法： 项目与服务商的匹配度：如果只是做做一个很小的app项目，比如开发一个计算器之类的小应用，找一个兼职的设计师和一个兼职的APP开发人员，或者一个小的开发团队即可完成这个项目；如果你的预算不多，对质量要求也不高，但牵涉到多个端的开发的话，找一个报价低，专业能力一般的小公司开发就足以应付这个项目；如果你项目较大，质量要求高，且牵涉到有一定技术难点的应用，如AR功能，就需要找专业的开发公司来开发了，价格肯定也不会比较贵。有些创业者只有几万的预算，却总想着找一个专业能力强，牛人多，且开发一个复杂的应用，这个时候就只能去购买标准的产品了，而不是走定制的方向，能力越强，规模越大的公司，收费也越贵。如果预算方面比较充裕，请忽略这一点； 价格的比较：相信很多人拿到多份报价方案的时候，一脸茫然，不知道怎么去进行对比，每家公司报的价格出入很大，而且需求描述，功能规划也不尽相同，采用什么技术方案也没有说明，没有一个标准的对比性。这时对于非专业的人来说可能只看最终的价格，这是很大的一个误区。理想的情况下，作为甲方，应该尽可能地把自己的需求描述清楚，最好写一份文字说明文档，并多一些耐心跟每一个服务商沟通清楚，确定大家对于需求理解上的一致。其次，尽可能套出每个供应商的能接受的最低价格。如果大家的理解都是准确的话，拿到报价后对功能进行逐一对比就有了标准需求的参照，这时需要特别留意报价方案上面有没有遗漏的功能点，或者在某些需求描述上面含糊不清，这个对最终的价格也有很大的影响。一般价格选择上面，会更偏向于中等或中等偏上的供应商。 最终的选择：结合上面提到的诚信靠谱度，专业能力，综合能力这三个维度，对每一个供应商进行一个综合的打分。如果供应商有开发过类似行业的应用，或者类似的功能，是一个加分项。最后结合价格做一个最终的权衡，基本就可以得出应该要选哪个供应商了。 当然，权衡利弊，若要自己选择搭建可靠地团队，需要付出的是开发时间成本，但是产品的走向可以根据自己的来，质量完全由自己控制，还有人力资源，因为若想要自己筹建团队来搞这个项目，首先你要招人，这些招聘开销以及人员的工资，让一个项目从零到1开发出来，时间不固定，若是有完整可靠地开发团队想跟你走这个创业的道路当然是最好。若是选择外包，质量永远不可能像自己写的这么好，而且项目一旦在谈产品需求报价确定后，是不可能进行频繁修改的，这其中也有很高的联系成本，产品开发出来是什么样就是什么样了，只能自己维护，当然外包也有好处，就是交由第三方先把总体框架写出来，上线快，可以很快的投入使用，因为外包公司做项目，里面肯定有一些已经现成做好的功能，只需要作为模块添加到你的产品组装即可，开发时间较快。 综合考虑，若是要拿出具体的成品再来谈风投的话，可以选择自己筹钱去谈外包，把外部框架快速做好再取跟风投谈。若是没有成品的前提下要去谈风投的话，则等到风投钱到位后，召集人马自己搭建团队做项目即可。这无疑是最好的办法。 IT外包对于创业公司来说，又爱又恨，不得已而为之者居多。初创公司，有多少是在跑DEMO，因此预算控制，是最重要的一件事。预算有多少，就去做多大的事，量力而行，活在当下，摸着石头过河，是期待问题，而不是技术实现问题。因此，创业公司CEO首先应该做的是根据目前的情况，计算可靠预算，同时，管理自己的期望，做好市场预期规划。而不是，好大喜功，妄图花小钱办大事，世界上没有免费的午餐，即使如此，占得一时便宜，也绝不具有持续性。 IT外包能够最快最大限度实现产品和服务的落地，成本低廉，但是产品质量和后续维护的坑多到难以置信，让你”万万想不到”！ 最常见的坑是： 1.项目需求对接不清，成本差异过大，技术公司要求追加高昂费用，烂尾机率很大 2.因为语言不统一，项目按人工时间成本计算价格，重新学习他人语言逻辑时间成本耗时费力，一般项目难以找到接盘侠，大概率推翻重做，前期投入均为空 3.大外包公司，成本高昂，但服务质量缩水，对创业公司友好度低，有质量得技术基本都在别的大项目组，很可能碰到实习生水平的技术但花着比政府项目还多的钱； 4.小外包公司，人员素质水平差异大，难以判断，后续支持力不足，长期维护难度大，技术实现风险高……………… 既然如此，建立自己的开发团队是最好的也最长久的选择，但是初创公司自己建立团队的难点在哪儿呢？ 1.初创公司，市场预期不明确，项目风险大，项目是否有良好的盈利能力不能准确预期，如此前提，养着一个开发团队造成现金流巨大压力，可能直接导致项目毙命 2.IT团队的cto可遇而不可求，寻找合伙人本身就难度很大，与资源和人脉，自身魅力，公司前景，运气都有极大关系，样样具备，难上加难，一个不合格的合伙人也会直接导致团队溃败…………如此，初创公司在钱不多，在跑demo又急于落地，最佳选择，是不存在的！！！但是我们可以有曲线救国的选项！！！控制成本，项目落地，管理预期风险是最重要的生存保障，那么，策略是：管理预期需求，提升it技术知识经验，在成本控制范围内寻找靠谱的外包公司实现核心需求的落地。随着项目落地，有一定变现能力，寻找投资，搭建技术团队，推倒重建，进行迭代。 我的想法： 若是我们有一定的资金但是没有风投又急于快速占领市场，可以选择快速外包，然后合伙人部分做维护，同时让产品快速进入运营期以补贴亏损，一旦赚到钱或者获得风投，迅速招人，广纳良将，作为初创公司的第一批技术储备力量，从外包公司的源码做升级迭代优化，甚至完全替换。 若是以项目完整度拿到风投资金，则完全不考虑外包或者可以考虑部分外包，只做框架，或者我们招美工去设计，由自己掌控，让产品快速上线。 国内外包移动端服务公司数据收集及评定先列出网络数据源收集的公司，之后我再做系统的综合评定。。。 可以选择知名公司去做（安全稳定，但是报价高），可以选择新外包企业（因为也不排除一些新公司想通过质量去赢得市场），主要是看运气，能不能找到一家质量高又价钱合理的公司，因为做生意总想着找便宜又质量好的东西。 1.广州微匠互联网科技有限公司广州市海珠区新港中路浩蕴大厦1105房（地铁客村D出口，TIT创意园正对面）致力于互联网移动端项目从0到1一站式开发，新公司，案例少，但是质量有可能较高。 2. 资源收集：https://www.zhihu.com/question/31155811]]></content>
      <categories>
        <category>创业之路</category>
      </categories>
      <tags>
        <tag>创业</tag>
        <tag>技术支持</tag>
        <tag>平台搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot之路(四)--Swagger2（RESTful API统一文档）]]></title>
    <url>%2F2018%2F03%2F24%2Fspringboot04%2F</url>
    <content type="text"><![CDATA[现如今的互联网开发，一款成熟的产品往往不只有Web作为前端，有可能要面对多个开发人员或多个开发团队：IOS开发、Android开发或是Web开发等等，现在的多终端时代，有可能我们需要开发所有市面的终端设备。 由于Spring Boot能够快速开发、便捷部署等特性，相信有很大一部分Spring Boot的用户会用来构建RESTful API。而我们构建RESTful API的目的通常都是由于多终端的原因，这些终端会共用很多底层业务逻辑，因此我们会抽象出这样一层来同时服务于多个移动端或者Web前端。 随着互联网技术的发展，现在的网站架构基本都由原来的后端渲染，变成了：前端渲染、先后端分离的形态，而且前端技术和后端技术在各自的道路上越走越远。前端和后端的唯一联系，变成了API接口；API文档变成了前后端开发人员联系的纽带，变得越来越重要，swagger就是一款让你更好的书写API文档的框架。 这样一来，我们的RESTful API就有可能要面对多个开发人员或多个开发团队：IOS开发、Android开发或是Web开发等。为了减少与其他团队平时开发期间的频繁沟通成本，传统做法我们会创建一份RESTful API文档来记录所有接口细节，然而这样的做法有以下几个问题： 1.由于接口众多，并且细节复杂（需要考虑不同的HTTP请求类型、HTTP头部信息、HTTP请求内容等），高质量地创建这份文档本身就是件非常吃力的事，下游的抱怨声不绝于耳。 2.随着时间推移，不断修改接口实现的时候都必须同步修改接口文档，而文档与代码又处于两个不同的媒介，除非有严格的管理机制，不然很容易导致不一致现象。 如何在开发团队中实现一个RESTful API的接口统一文档？ Swagger2使用Swagger之前我们先需要构建一个RESTful API来做准备。 RESTful API具体设计如下： User实体定义： public class User { private Long id; private String name; private Integer age; // 省略setter和getter } 实现对User对象的操作接口： @RestController @RequestMapping(value=&quot;/users&quot;) // 通过这里配置使下面的映射都在/users下 public class UserController { // 创建线程安全的Map //利用Collections工具类中的synchronizedMap方法来构建同步Map //SynchronizedMap类是定义在Collections中的一个静态内部类。 //它实现了Map接口，并对其中的每一个方法实现，通过synchronized关键字进行了同步控制 //这样的好处在于Map不会产生混乱，当有并发请求时这个Map会进行同步锁。 static Map&lt;Long, User&gt; users = Collections.synchronizedMap(new HashMap&lt;Long, User&gt;()); @RequestMapping(value=&quot;/&quot;, method=RequestMethod.GET) public List&lt;User&gt; getUserList() { // 处理&quot;/users/&quot;的GET请求，用来获取用户列表 // 还可以通过@RequestParam从页面中传递参数来进行查询条件或者翻页信息的传递 //users.Values()获取用户信息的所有值并传递给一个List集合，然后通过返回集合呈现过我们 List&lt;User&gt; r = new ArrayList&lt;User&gt;(users.values()); return r; } @RequestMapping(value=&quot;/&quot;, method=RequestMethod.POST) public String postUser(@ModelAttribute User user) { // 处理&quot;/users/&quot;的POST请求，用来创建User // 除了@ModelAttribute绑定参数之外，还可以通过@RequestParam从页面中传递参数 users.put(user.getId(), user); return &quot;success&quot;; } @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.GET) public User getUser(@PathVariable Long id) { // 处理&quot;/users/{id}&quot;的GET请求，用来获取url中id值的User信息 // url中的id可通过@PathVariable绑定到函数的参数中 return users.get(id); } @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.PUT) public String putUser(@PathVariable Long id, @ModelAttribute User user) { // 处理&quot;/users/{id}&quot;的PUT请求，用来更新User信息 User u = users.get(id); u.setName(user.getName()); u.setAge(user.getAge()); users.put(id, u); return &quot;success&quot;; } @RequestMapping(value=&quot;/{id}&quot;, method=RequestMethod.DELETE) public String deleteUser(@PathVariable Long id) { // 处理&quot;/users/{id}&quot;的DELETE请求，用来删除User users.remove(id); return &quot;success&quot;; } } 下面针对该Controller编写测试用例验证正确性，具体如下。当然也可以通过浏览器插件等进行请求提交验证。 @RunWith(SpringJUnit4ClassRunner.class) //@SpringApplicationConfiguration(classes = MockServletContext.class) @WebAppConfiguration public class ApplicationTests { private MockMvc mvc; @Before public void setUp() throws Exception { mvc = MockMvcBuilders.standaloneSetup(new UserController()).build(); } @Test public void testUserController() throws Exception { // 测试UserController RequestBuilder request = null; // 1、get查一下user列表，应该为空 request = get(&quot;/users/&quot;); mvc.perform(request) .andExpect(status().isOk()) .andExpect(content().string(equalTo(&quot;[]&quot;))); // 2、post提交一个user request = post(&quot;/users/&quot;) .param(&quot;id&quot;, &quot;1&quot;) .param(&quot;name&quot;, &quot;测试大师&quot;) .param(&quot;age&quot;, &quot;20&quot;); mvc.perform(request) .andExpect(content().string(equalTo(&quot;success&quot;))); // 3、get获取user列表，应该有刚才插入的数据 request = get(&quot;/users/&quot;); mvc.perform(request) .andExpect(status().isOk()) .andExpect(content().string(equalTo(&quot;[{\&quot;id\&quot;:1,\&quot;name\&quot;:\&quot;测试大师\&quot;,\&quot;age\&quot;:20}]&quot;))); // 4、put修改id为1的user request = put(&quot;/users/1&quot;) .param(&quot;name&quot;, &quot;测试终极大师&quot;) .param(&quot;age&quot;, &quot;30&quot;); mvc.perform(request) .andExpect(content().string(equalTo(&quot;success&quot;))); // 5、get一个id为1的user request = get(&quot;/users/1&quot;); mvc.perform(request) .andExpect(content().string(equalTo(&quot;{\&quot;id\&quot;:1,\&quot;name\&quot;:\&quot;测试终极大师\&quot;,\&quot;age\&quot;:30}&quot;))); // 6、del删除id为1的user request = delete(&quot;/users/1&quot;); mvc.perform(request) .andExpect(content().string(equalTo(&quot;success&quot;))); // 7、get查一下user列表，应该为空 request = get(&quot;/users/&quot;); mvc.perform(request) .andExpect(status().isOk()) .andExpect(content().string(equalTo(&quot;[]&quot;))); } } 测试Console内容： 2018-03-25 23:25:23.827 INFO 7832 --- [ main] com.didispace.ApplicationTests : Starting ApplicationTests on DX-20170223SFUP with PID 7832 (F:\SpringBoot-Learning\Chapter3-1-1\target\test-classes started by Administrator in F:\SpringBoot-Learning\Chapter3-1-1) 2018-03-25 23:25:23.829 INFO 7832 --- [ main] com.didispace.ApplicationTests : No active profile set, falling back to default profiles: default 2018-03-25 23:25:24.217 INFO 7832 --- [ main] o.s.w.c.s.GenericWebApplicationContext : Refreshing org.springframework.web.context.support.GenericWebApplicationContext@6d2a209c: startup date [Sun Mar 25 23:25:24 CST 2018]; root of context hierarchy 2018-03-25 23:25:25.640 INFO 7832 --- [ main] com.didispace.ApplicationTests : Started ApplicationTests in 3.468 seconds (JVM running for 6.448) 2018-03-25 23:25:26.400 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/hello]}&quot; onto public java.lang.String com.didispace.web.HelloController.index() 2018-03-25 23:25:26.431 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/],methods=[GET]}&quot; onto public java.util.List&lt;com.didispace.domain.User&gt; com.didispace.web.UserController.getUserList() 2018-03-25 23:25:26.432 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/],methods=[POST]}&quot; onto public java.lang.String com.didispace.web.UserController.postUser(com.didispace.domain.User) 2018-03-25 23:25:26.432 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/{id}],methods=[GET]}&quot; onto public com.didispace.domain.User com.didispace.web.UserController.getUser(java.lang.Long) 2018-03-25 23:25:26.432 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/{id}],methods=[PUT]}&quot; onto public java.lang.String com.didispace.web.UserController.putUser(java.lang.Long,com.didispace.domain.User) 2018-03-25 23:25:26.433 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/{id}],methods=[DELETE]}&quot; onto public java.lang.String com.didispace.web.UserController.deleteUser(java.lang.Long) 2018-03-25 23:25:27.204 INFO 7832 --- [ main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.test.web.servlet.setup.StubWebApplicationContext@44a7bfbc 2018-03-25 23:25:27.498 INFO 7832 --- [ main] o.s.mock.web.MockServletContext : Initializing Spring FrameworkServlet &apos;&apos; 2018-03-25 23:25:27.499 INFO 7832 --- [ main] o.s.t.web.servlet.TestDispatcherServlet : FrameworkServlet &apos;&apos;: initialization started 2018-03-25 23:25:27.500 INFO 7832 --- [ main] o.s.t.web.servlet.TestDispatcherServlet : FrameworkServlet &apos;&apos;: initialization completed in 1 ms 2018-03-25 23:25:27.890 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/hello]}&quot; onto public java.lang.String com.didispace.web.HelloController.index() 2018-03-25 23:25:27.896 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/],methods=[GET]}&quot; onto public java.util.List&lt;com.didispace.domain.User&gt; com.didispace.web.UserController.getUserList() 2018-03-25 23:25:27.896 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/],methods=[POST]}&quot; onto public java.lang.String com.didispace.web.UserController.postUser(com.didispace.domain.User) 2018-03-25 23:25:27.897 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/{id}],methods=[GET]}&quot; onto public com.didispace.domain.User com.didispace.web.UserController.getUser(java.lang.Long) 2018-03-25 23:25:27.897 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/{id}],methods=[PUT]}&quot; onto public java.lang.String com.didispace.web.UserController.putUser(java.lang.Long,com.didispace.domain.User) 2018-03-25 23:25:27.897 INFO 7832 --- [ main] ilder$StaticRequestMappingHandlerMapping : Mapped &quot;{[/users/{id}],methods=[DELETE]}&quot; onto public java.lang.String com.didispace.web.UserController.deleteUser(java.lang.Long) 2018-03-25 23:25:27.920 INFO 7832 --- [ main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.test.web.servlet.setup.StubWebApplicationContext@61fe30 2018-03-25 23:25:27.927 INFO 7832 --- [ main] o.s.mock.web.MockServletContext : Initializing Spring FrameworkServlet &apos;&apos; 2018-03-25 23:25:27.927 INFO 7832 --- [ main] o.s.t.web.servlet.TestDispatcherServlet : FrameworkServlet &apos;&apos;: initialization started 2018-03-25 23:25:27.927 INFO 7832 --- [ main] o.s.t.web.servlet.TestDispatcherServlet : FrameworkServlet &apos;&apos;: initialization completed in 0 ms 2018-03-25 23:25:27.952 INFO 7832 --- [ Thread-1] o.s.w.c.s.GenericWebApplicationContext : Closing org.springframework.web.context.support.GenericWebApplicationContext@6d2a209c: startup date [Sun Mar 25 23:25:24 CST 2018]; root of context hierarchy 这个是用MockMVC来做数据分析，当然我们也可以直接用浏览器发送请求来实战测试，这就需要浏览器发送GET请求和POST请求那么按照我们常规的方法是编写一个网站来发送POST请求（因为POST请求是不直接表达在URL上的）所以这就需要借助浏览器的一些插件来实现发送POST请求 我们开发的工具是Google的Chrome浏览器，我们通过应用市场下载POSTMAN插件就可以实现浏览器发送POST请求做WEB端开发的人员必须要下载这个插件作为调试页面访问请求的数据显示。 Postman插件概述Postman插件是什么？postman插件是一款chrome插件，是谷歌浏览器的网页调试插件，这款插件可以利用Chrome插件的形式把各种模拟用户HTTP请求的数据发送到服务器，以便开发人员能够及时地作出正确的响应，或者是对产品发布之前的错误信息提前处理，进而保证产品上线之后的稳定性和安全性。Postman是一种网页调试与发送网页http请求的chrome插件。我们可以用来很方便的模拟get或者post或者其他方式的请求来调试接口。 Postman插件功能介绍 当开发人员需要调试一个网页是否运行正常，并不是简简单单地调试网页的HTML、CSS、脚本等信息是否运行正常，更加重要的是网页能够正确是处理各种HTTP请求，毕竟网页的HTTP请求是网站与用户之间进行交互的非常重要的一种方式，在动态网站中，用户的大部分数据都需要通过HTTP请求来与服务器进行交互。可以利用Chrome插件的形式把各种模拟用户HTTP请求的数据发送到服务器，以便开发人员能够及时地作出正确的响应，或者是对产品发布之前的错误信息提前处理，进而保证产品上线之后的稳定性和安全性。 POSTman教程及安装：http://www.cnplugins.com/devtool/postman/ 它可以轻松的整合到Spring Boot中，并与Spring MVC程序配合组织出强大RESTful API文档。它既可以减少我们创建文档的工作量，同时说明内容又整合入实现代码中，让维护文档和修改代码整合为一体，可以让我们在修改代码逻辑的同时方便的修改文档说明。另外Swagger2也提供了强大的页面测试功能来调试每个RESTful API。具体效果如下图所示：]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot之路(三)--Thymeleaf模板引擎]]></title>
    <url>%2F2018%2F03%2F24%2Fspringboot03%2F</url>
    <content type="text"><![CDATA[Thymeleaf在实际的公司开发中，传统的JSP技术由于前段后端结合太过紧密，而且后端人员需要随时测试数据能不能渲染在界面上，这就需要前端与后端部门过于紧耦合，不利于实际开发，反而是模板引擎用的较多（当然也不排除一些老的公司需要维护公司以前的动态JSP网页） thymeleaf最大的优势后缀为html,就是只需要浏览器就可以展现页面了,还有就是thymeleaf可以很好的和spring集成. Thymeleaf是一个XML/XHTML/HTML5模板引擎，可用于Web与非Web环境中的应用开发。它是一个开源的Java库，基于Apache License 2.0许可，由Daniel Fernández创建，该作者还是Java加密库Jasypt的作者。 Thymeleaf提供了一个用于整合Spring MVC的可选模块，在应用开发中，你可以使用Thymeleaf来完全代替JSP或其他模板引擎，如Velocity、FreeMarker等。Thymeleaf的主要目标在于提供一种可被浏览器正确显示的、格式良好的模板创建方式，因此也可以用作静态建模。你可以使用它创建经过验证的XML与HTML模板。相对于编写逻辑或代码，开发者只需将标签属性添加到模板中即可。接下来，这些标签属性就会在DOM（文档对象模型）上执行预先制定好的逻辑。 静态资源访问在我们开发Web应用的时候，需要引用大量的js、css、图片等静态资源。 默认配置Spring Boot默认提供静态资源目录位置需置于classpath下，目录名需符合如下规则： /static /public /resources /META-INF/resources 举例：我们可以在src/main/resources/目录下创建static，在该位置放置一个图片文件。启动程序后，尝试访问http://localhost:8088/D.jpg。如能显示图片，配置成功。 在之前的示例中，我们都是通过@RestController来处理请求，所以返回的内容为json对象。那么如果需要渲染html页面的时候，要如何实现呢？ 在动态HTML实现上Spring Boot依然可以完美胜任，并且提供了多种模板引擎的默认配置支持，所以在推荐的模板引擎下，我们可以很快的上手开发动态网站。 Spring Boot提供了默认配置的模板引擎（抛弃传统的JSP动态网页）主要有以下几种： ThymeleafFreeMarkerVelocityGroovyMustache Spring Boot建议使用这些模板引擎，避免使用JSP，若一定要使用JSP将无法实现Spring Boot的多种特性，具体可见后文：支持JSP的配置 当你使用上述模板引擎中的任何一个，它们默认的模板配置路径为：src/main/resources/templates。当然也可以修改这个路径，具体如何修改，可在后续各模板引擎的配置属性中查询并修改。 示例： &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th th:text=&quot;#{msgs.headers.name}&quot;&gt;Name&lt;/td&gt; &lt;th th:text=&quot;#{msgs.headers.price}&quot;&gt;Price&lt;/td&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr th:each=&quot;prod : ${allProducts}&quot;&gt; &lt;td th:text=&quot;${prod.name}&quot;&gt;Oranges&lt;/td&gt; &lt;td th:text=&quot;${#numbers.formatDecimal(prod.price,1,2)}&quot;&gt;0.99&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; 可以看到Thymeleaf主要以属性的方式加入到html标签中，浏览器在解析html时，当检查到没有的属性时候会忽略，所以Thymeleaf的模板可以通过浏览器直接打开展现，这样非常有利于前后端的分离。 在Spring Boot中使用Thymeleaf，只需要引入下面依赖，并在默认的模板路径src/main/resources/templates下编写模板文件即可完成。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; 在完成配置之后，举一个简单的例子，在快速入门工程的基础上，举一个简单的示例来通过Thymeleaf渲染一个页面。 package com.example.springboot.controller; import org.springframework.stereotype.Controller; import org.springframework.ui.ModelMap; import org.springframework.web.bind.annotation.RequestMapping; @Controller public class IndexController { @RequestMapping(&quot;/index&quot;) public String index(ModelMap map){ map.addAttribute(&quot;hello&quot;, &quot;i love you@com&quot;); return &quot;index&quot;; } } 相应的index.html界面代码为 &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt;&lt;/meta&gt; &lt;title&gt;Insert title here&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1 th:text=&quot;${hello}&quot;&gt;Hello World&lt;/h1&gt; &lt;/body&gt; &lt;/html&gt; 启动程序后，页面输入http://localhost:8088/index，所出现的显示数据是：i love you@com，并没有出现Hello World展现的是IndexController中的hello的属性值，做到了不破坏HTML自身内容的数据逻辑分离。 注意：我们建立模板测试或者是实际开发中，我们都需要严格遵守thymeleaf的HTML5页面规范，不然会报错包括我上面写的标签则需要双标签来自正常结束才可以，否则页面将出现错误，你可以使用严格的标签，也就是每个标签都有结束标签，这种可能比较麻烦。 那么如何在thymeleaf中声明使用非严格的html5规范呢？ maven添加依赖 &lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;version&gt;1.9.22&lt;/version&gt; &lt;/dependency&gt; application.properties属性文件中添加 spring.thymeleaf.mode = LEGACYHTML5 声明thymeleaf使用非严格的html即可。最后更新下maven仓库即可生效。 关于Thymeleaf的默认参数配置如有需要修改默认配置的时候，只需复制下面要修改的属性到application.properties中，并修改成需要的值，如修改模板文件的扩展名，修改默认的模板路径等。 -Enable template caching.spring.thymeleaf.cache=true-Check that the templates location exists.spring.thymeleaf.check-template-location=true-Content-Type value.spring.thymeleaf.content-type=text/html-Enable MVC Thymeleaf view resolution.spring.thymeleaf.enabled=true-Template encoding.spring.thymeleaf.encoding=UTF-8-Comma-separated list of view names that should be excluded from resolution.spring.thymeleaf.excluded-view-names=-Template mode to be applied to templates. See also StandardTemplateModeHandlers.spring.thymeleaf.mode=HTML5-Prefix that gets prepended to view names when building a URL.spring.thymeleaf.prefix=classpath:/templates/-Suffix that gets appended to view names when building a URL.spring.thymeleaf.suffix=.html spring.thymeleaf.template-resolver-order= # Order of the template resolver in the chain. spring.thymeleaf.view-names= # Comma-separated list of view names that can be resolved. 关于Thymeleaf语法命名空间对于Thymeleaf语法来说，很多资料中会写应用Thymeleaf语法需要先在标签中引入在html中引入此命名空间，可避免编辑器出现html验证错误，虽然加不加命名空间对Thymeleaf的功能没有任何影响。 &lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; 才可以使用Thymeleaf语法，但是我在上面的案例中并没有声明html标签的属性也可以用，也让我很费解。（不过为了一些不必要的麻烦还是把这个声明加上） 输出内容例如: Controller: map.addAttribute(&quot;hello&quot;, &quot;i love you@com&quot;); HTML: &lt;h1 th:text=&quot;${hello}&quot;&gt;Hello World&lt;/h1&gt; 1. th:text 用来将内容输出到所在标签的body中。 2. ${hello} 用来引用hello属性值 3. 可以用th:utext 用来显示“unescaped ” 的html内容。(unescaped即非转义字符) 例如： &lt;p th:text=&quot;#{home.welcome}&quot;&gt;Welcome to our grocery store!&lt;/p&gt; `#{home.welcome} 用来引入数据home对象中的 welcome属性。` 标准表达式语法它们分为四类： 1.变量表达式2.选择或星号表达式3.文字国际化表达式4.URL表达式 变量表达式变量表达式即OGNL表达式或Spring EL表达式(在Spring术语中也叫model attributes)。如下所示： ${session.user.name}${hello} 它们将以HTML标签的一个属性来表示： &lt;span th:text=&quot;${book.author.name}&quot;&gt; &lt;li th:each=&quot;book : ${books}&quot;&gt; 选择(星号)表达式选择表达式很像变量表达式，不过它们用一个预先选择的对象来代替上下文变量容器(map)来执行，如下： *{customer.name} 被指定的object由th:object属性定义： &lt;div th:object=&quot;${book}&quot;&gt; ... &lt;span th:text=&quot;*{title}&quot;&gt;...&lt;/span&gt; ... &lt;/div&gt; 文字国际化表达式文字国际化表达式允许我们从一个外部文件获取区域文字信息(.properties)，用Key索引Value，还可以提供一组参数(可选). `#{main.title} ` `#{message.entrycreated(${entryId})} ` 可以在模板文件中找到这样的表达式代码： &lt;table&gt; ... &lt;th th:text=&quot;#{header.address.city}&quot;&gt;...&lt;/th&gt; &lt;th th:text=&quot;#{header.address.country}&quot;&gt;...&lt;/th&gt; ... &lt;/table&gt; URL表达式URL表达式指的是把一个有用的上下文或回话信息添加到URL，这个过程经常被叫做URL重写。@{/order/list} URL还可以设置参数：@{/order/details(id=${orderId})} 相对路径：@{../documents/report} 让我们看这些表达式： &lt;form th:action=&quot;@{/createOrder}&quot;&gt; &lt;a href=&quot;main.html&quot; th:href=&quot;@{/main}&quot;&gt; 变量表达式和星号表达有什么区别吗？ 如果不考虑上下文的情况下，两者没有区别；星号语法评估在选定对象上表达，而不是整个上下文什么是选定对象？就是父标签的值，如下：（相当于星号表达式会去全局中找这个属性，如果找不到则默认不显示） &lt;div th:object=&quot;${session.user}&quot;&gt; &lt;p&gt;Name: &lt;span th:text=&quot;*{firstName}&quot;&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text=&quot;*{lastName}&quot;&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text=&quot;*{nationality}&quot;&gt;Saturn&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt; 完全等价于 &lt;div th:object=&quot;${session.user}&quot;&gt; &lt;p&gt;Name: &lt;span th:text=&quot;${session.user.firstName}&quot;&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text=&quot;${session.user.lastName}&quot;&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text=&quot;${session.user.nationality}&quot;&gt;Saturn&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt; 当然，美元符号和星号语法可以混合使用： &lt;div th:object=&quot;${session.user}&quot;&gt; &lt;p&gt;Name: &lt;span th:text=&quot;*{firstName}&quot;&gt;Sebastian&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Surname: &lt;span th:text=&quot;${session.user.lastName}&quot;&gt;Pepper&lt;/span&gt;.&lt;/p&gt; &lt;p&gt;Nationality: &lt;span th:text=&quot;*{nationality}&quot;&gt;Saturn&lt;/span&gt;.&lt;/p&gt; &lt;/div&gt; 表达式支持的语法 字面（Literals） 文本文字（Text literals）: ‘one text’, ‘Another one!’,…数字文本（Number literals）: 0, 34, 3.0, 12.3,…布尔文本（Boolean literals）: true, false空（Null literal）: null文字标记（Literal tokens）: one, sometext, main,… 文本操作（Text operations） 字符串连接(String concatenation): +文本替换（Literal substitutions）: |The name is ${name}| 算术运算（Arithmetic operations） 二元运算符（Binary operators）: +, -, *, /, %减号（单目运算符）Minus sign (unary operator): - 布尔操作（Boolean operations） 二元运算符（Binary operators）:and, or布尔否定（一元运算符）Boolean negation (unary operator):!, not 比较和等价(Comparisons and equality) 比较（Comparators）: &gt;, &lt;, &gt;=, &lt;= (gt, lt, ge, le)等值运算符（Equality operators）:==, != (eq, ne) 条件运算符（Conditional operators） If-then: (if) ? (then)If-then-else: (if) ? (then) : (else)Default: (value) ?: (defaultvalue) 所有这些特征可以被组合并嵌套： &apos;User is of type &apos; + (${user.isAdmin()} ? &apos;Administrator&apos; : (${user.type} ?: &apos;Unknown&apos;)) 常用th标签都有那些？ 几种常用的使用方法几种Thymeleaf在实际开发中的应用（需要掌握的） 赋值、字符串拼接 description 注意：th:text=””,双引号里面才是正确输出的内容，拼接的字符串要用‘’单引号，也有更加简易的拼接字符串的方法，用||符号 &lt;h1 th:text=&quot;|hello,*{hello}|&quot;&gt;Hello World&lt;/h1&gt; 条件判断 If/Unless Thymeleaf中使用th:if和th:unless属性进行条件判断，下面的例子中，标签只有在th:if中条件成立时才显示： &lt;a th:if=&quot;${myself==&apos;yes&apos;}&quot; &gt;&lt;/a&gt; &lt;a th:unless=${session.user != null} th:href=&quot;@{/login}&quot; &gt;Login&lt;/a&gt; th:unless于th:if恰好相反，只有表达式中的条件不成立，才会显示其内容。 也可以使用 (if) ? (then) : (else) 这种语法来判断显示的内容 for 循环 &lt;tr th:each=&quot;collect,iterStat : ${collects}&quot;&gt; &lt;th scope=&quot;row&quot; th:text=&quot;${collect.id}&quot;&gt;1&lt;/th&gt; &lt;td &gt; &lt;img th:src=&quot;${collect.webLogo}&quot;/&gt; &lt;/td&gt; &lt;td th:text=&quot;${collect.url}&quot;&gt;Mark&lt;/td&gt; &lt;td th:text=&quot;${collect.title}&quot;&gt;Otto&lt;/td&gt; &lt;td th:text=&quot;${collect.description}&quot;&gt;@mdo&lt;/td&gt; &lt;td th:text=&quot;${terStat.index}&quot;&gt;index&lt;/td&gt; &lt;/tr&gt; iterStat称作状态变量，属性有： index:当前迭代对象的index（从0开始计算）count: 当前迭代对象的index(从1开始计算)size:被迭代对象的大小current:当前迭代变量even/odd:布尔值，当前循环是否是偶数/奇数（从0开始计算）first:布尔值，当前循环是否是第一个last:布尔值，当前循环是否是最后一个 URL URL在Web应用模板中占据着十分重要的地位，需要特别注意的是Thymeleaf对于URL的处理是通过语法@{…}来处理的。如果需要Thymeleaf对URL进行渲染，那么务必使用th:href，th:src等属性，下面是一个例子 &lt;!-- Will produce &apos;http://localhost:8080/standard/unread&apos; (plus rewriting) --&gt; &lt;a th:href=&quot;@{/standard/{type}(type=${type})}&quot;&gt;view&lt;/a&gt; &lt;!-- Will produce &apos;/gtvg/order/3/details&apos; (plus rewriting) --&gt; &lt;a href=&quot;details.html&quot; th:href=&quot;@{/order/{orderId}/details(orderId=${o.id})}&quot;&gt;view&lt;/a&gt; 设置背景 &lt;div th:style=&quot;&apos;background:url(&apos; + @{/&lt;path-to-image&gt;} + &apos;);&apos;&quot;&gt;&lt;/div&gt; 根据属性值改变背景 &lt;div class=&quot;media-object resource-card-image&quot; th:style=&quot;&apos;background:url(&apos; + @{(${collect.webLogo}==&apos;&apos; ? &apos;img/favicon.png&apos; : ${collect.webLogo})} + &apos;)&apos;&quot; &gt;&lt;/div&gt; 几点说明： 上例中URL最后的(orderId=${o.id}) 表示将括号内的内容作为URL参数处理，该语法避免使用字符串拼接，大大提高了可读性@{…}表达式中可以通过{orderId}访问Context中的orderId变量@{/order}是Context相关的相对路径，在渲染时会自动添加上当前Web应用的Context名字，假设context名字为app，那么结果应该是/app/order 内联js 内联文本：[[…]]内联文本的表示方式，使用时，必须先用th:inline=”text/javascript/none”激活，th:inline可以在父级标签内使用，甚至作为body的标签。内联文本尽管比th:text的代码少，不利于原型显示。 &lt;script th:inline=&quot;javascript&quot;&gt; /*&lt;![CDATA[*/ ... var username = /*[[${sesion.user.name}]]*/ &apos;Sebastian&apos;; var size = /*[[${size}]]*/ 0; ... /*]]&gt;*/ &lt;/script&gt; js附加代码： /*[+ var msg = &apos;This is a working application&apos;; +]*/ js移除代码： /*[- */ var msg = &apos;This is a non-working template&apos;; /* -]*/ 内嵌变量 为了模板更加易用，Thymeleaf还提供了一系列Utility对象（内置于Context中），可以通过#直接访问： dates ： java.util.Date的功能方法类。calendars : 类似#dates，面向java.util.Calendarnumbers : 格式化数字的功能方法类strings : 字符串对象的功能类，contains,startWiths,prepending/appending等等。objects: 对objects的功能类操作。bools: 对布尔值求值的功能方法。arrays：对数组的功能类方法。lists: 对lists功能类方法setsmaps…下面用一段代码来举例一些常用的方法： 1.dates /* * Format date with the specified pattern * Also works with arrays, lists or sets */ ${#dates.format(date, &apos;dd/MMM/yyyy HH:mm&apos;)} ${#dates.arrayFormat(datesArray, &apos;dd/MMM/yyyy HH:mm&apos;)} ${#dates.listFormat(datesList, &apos;dd/MMM/yyyy HH:mm&apos;)} ${#dates.setFormat(datesSet, &apos;dd/MMM/yyyy HH:mm&apos;)} /* * Create a date (java.util.Date) object for the current date and time */ ${#dates.createNow()} /* * Create a date (java.util.Date) object for the current date (time set to 00:00) */ ${#dates.createToday()} 2.strings /* * Check whether a String is empty (or null). Performs a trim() operation before check * Also works with arrays, lists or sets */ ${#strings.isEmpty(name)} ${#strings.arrayIsEmpty(nameArr)} ${#strings.listIsEmpty(nameList)} ${#strings.setIsEmpty(nameSet)} /* * Check whether a String starts or ends with a fragment * Also works with arrays, lists or sets */ ${#strings.startsWith(name,&apos;Don&apos;)} // also array*, list* and set* ${#strings.endsWith(name,endingFragment)} // also array*, list* and set* /* * Compute length * Also works with arrays, lists or sets */ ${#strings.length(str)} /* * Null-safe comparison and concatenation */ ${#strings.equals(str)} ${#strings.equalsIgnoreCase(str)} ${#strings.concat(str)} ${#strings.concatReplaceNulls(str)} /* * Random */ ${#strings.randomAlphanumeric(count)} 使用thymeleaf布局 使用thymeleaf布局非常的方便 定义代码片段 &lt;footer th:fragment=&quot;copy&quot;&gt; &amp;copy; 2016 &lt;/footer&gt; 在页面任何地方引入： &lt;body&gt; &lt;div th:include=&quot;footer :: copy&quot;&gt;&lt;/div&gt; &lt;div th:replace=&quot;footer :: copy&quot;&gt;&lt;/div&gt; &lt;/body&gt;‘ th:include 和 th:replace区别，include只是加载，replace是替换 返回的HTML如下： &lt;body&gt; &lt;div&gt; &amp;copy; 2016 &lt;/div&gt; &lt;footer&gt;&amp;copy; 2016 &lt;/footer&gt; &lt;/body&gt; 下面是一个常用的后台页面布局，将整个页面分为头部，尾部、菜单栏、隐藏栏，点击菜单只改变content区域的页面 &lt;body class=&quot;layout-fixed&quot;&gt; &lt;div th:fragment=&quot;navbar&quot; class=&quot;wrapper&quot; role=&quot;navigation&quot;&gt; &lt;div th:replace=&quot;fragments/header :: header&quot;&gt;Header&lt;/div&gt; &lt;div th:replace=&quot;fragments/left :: left&quot;&gt;left&lt;/div&gt; &lt;div th:replace=&quot;fragments/sidebar :: sidebar&quot;&gt;sidebar&lt;/div&gt; &lt;div layout:fragment=&quot;content&quot; id=&quot;content&quot; &gt;&lt;/div&gt; &lt;div th:replace=&quot;fragments/footer :: footer&quot;&gt;footer&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; 任何页面想使用这样的布局值只需要替换中见的 content模块即可 &lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot; layout:decorator=&quot;layout&quot;&gt; &lt;body&gt; &lt;section layout:fragment=&quot;content&quot;&gt; ... 也可以在引用模版的时候传参 &lt;head th:include=&quot;layout :: htmlhead&quot; th:with=&quot;title=&apos;Hello&apos;&quot;&gt;&lt;/head&gt; layout 是文件地址，如果有文件夹可以这样写 fileName/layout:htmlheadhtmlhead 是指定义的代码片段 如 th:fragment=”copy” 开源云收藏项目的github源码：https://github.com/cloudfavorites/favorites-web里面用的技术包括springboot集成了tomcat和thymeleaf，源码测试里都是详细的项目功能介绍。 还有就是Springboot虽然不建议使用JSP技术，但是若有相应的要求也可以使用。具体自查。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot之路(二)--Nosql支持]]></title>
    <url>%2F2018%2F03%2F23%2Fspringboot02%2F</url>
    <content type="text"><![CDATA[在SpringBoot中对常用的数据库支持外，对nosql 数据库也进行了封装自动化。 Redis简介REmote DIctionary Server(Redis) 是一个由Salvatore Sanfilippo写的key-value存储系统。Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。它通常被称为数据结构服务器，因为值（value）可以是 字符串(String), 哈希(Map), 列表(list), 集合(sets) 和 有序集合(sorted sets)等类型。 Redis学习手册：http://www.runoob.com/redis/redis-tutorial.html Redis 是完全开源免费的，遵守BSD协议，是一个高性能的key-value数据库。Redis 与其他 key - value 缓存产品有以下三个特点： 1.Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。2.Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。3.Redis支持数据的备份，即master-slave模式的数据备份。 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 什么是BSD协议？BSD开源协议是一个给于使用者很大自由的协议。可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。当你发布使用了BSD协议的代码，或者以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件：如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议。如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。BSD代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销 售，因此是对商业集成很友好的协议。很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者 二次开发。 安装及使用Windows下的安装教程： 下载地址（已在Github上开源）：https://github.com/MSOpenTech/redis/releases Redis 支持 32 位和 64 位。这个需要根据你系统平台的实际情况选择，这里我们下载 Redis-x64-xxx.zip压缩包到 F 盘，解压后，将文件夹重新命名为 redis。 打开一个 cmd 窗口 使用cd命令切换目录到 C:\redis 运行 redis-server.exe redis.windows.conf 。如果想方便的话，可以把 redis 的路径加到系统的环境变量里，这样就省得再输路径了，后面的那个 redis.windows.conf 可以省略，如果省略，会启用默认的。输入之后，会显示如下界面： 这时候另启一个cmd窗口，原来的不要关闭，不然就无法访问服务端了。切换到redis目录下运行 redis-cli.exe -h 127.0.0.1 -p 6379 。设置键值对 set myKey abc取出键值对 get myKey Redis是目前业界使用最广泛的内存数据存储。相比memcached，Redis支持更丰富的数据结构，例如hashes, lists, sets等，同时支持数据持久化。除此之外，Redis还提供一些类数据库的特性，比如事务，HA，主从库。可以说Redis兼具了缓存系统和数据库的一些特性，因此有着丰富的应用场景。本文介绍Redis在Spring Boot中两个典型的应用场景。 如何使用1、引入 spring-boot-starter-redis &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;/dependency&gt; 2、添加配置文件 编辑配置你可以通过修改 redis.conf 文件或使用 CONFIG set 命令来修改配置。 CONFIG SET 命令基本语法：redis 127.0.0.1:6379&gt; CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE 通过修改application.properties中配置Redis连接信息文件修改如下： REDIS (RedisProperties) Redis数据库索引（默认为0）spring.redis.database=0 Redis服务器地址spring.redis.host=192.168.0.58 Redis服务器连接端口spring.redis.port=6379 Redis服务器连接密码（默认为空）spring.redis.password= 连接池最大连接数（使用负值表示没有限制）spring.redis.pool.max-active=8 连接池最大阻塞等待时间（使用负值表示没有限制）spring.redis.pool.max-wait=-1 连接池中的最大空闲连接spring.redis.pool.max-idle=8 连接池中的最小空闲连接spring.redis.pool.min-idle=0 连接超时时间（毫秒）spring.redis.timeout=0 3、修改项目启动类 增加注解@EnableCaching，开启缓存功能，如下： package springboot; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cache.annotation.EnableCaching; import org.springframework.scheduling.annotation.EnableScheduling; @SpringBootApplication @EnableScheduling @EnableCaching public class SpringbootApplication{ public static void main(String[] args) { SpringApplication.run(SpringbootApplication.class, args); } } 4、新建Redis缓存配置类RedisConfig，如下： package springboot.config; import org.springframework.beans.factory.annotation.Value; import org.springframework.cache.CacheManager; import org.springframework.cache.annotation.CachingConfigurerSupport; import org.springframework.cache.annotation.EnableCaching; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.cache.RedisCacheManager; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; /** * Redis缓存配置类 * @author szekinwin * */ @Configuration @EnableCaching public class RedisConfig extends CachingConfigurerSupport{ @Value(&quot;${spring.redis.host}&quot;) private String host; @Value(&quot;${spring.redis.port}&quot;) private int port; @Value(&quot;${spring.redis.timeout}&quot;) private int timeout; //自定义缓存key生成策略 // @Bean // public KeyGenerator keyGenerator() { // return new KeyGenerator(){ // @Override // public Object generate(Object target, java.lang.reflect.Method method, Object... params) { // StringBuffer sb = new StringBuffer(); // sb.append(target.getClass().getName()); // sb.append(method.getName()); // for(Object obj:params){ // sb.append(obj.toString()); // } // return sb.toString(); // } // }; // } //缓存管理器 @Bean public CacheManager cacheManager(@SuppressWarnings(&quot;rawtypes&quot;) RedisTemplate redisTemplate) { RedisCacheManager cacheManager = new RedisCacheManager(redisTemplate); //设置缓存过期时间 cacheManager.setDefaultExpiration(10000); return cacheManager; } @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory factory){ StringRedisTemplate template = new StringRedisTemplate(factory); setSerializer(template);//设置序列化工具 template.afterPropertiesSet(); return template; } private void setSerializer(StringRedisTemplate template){ @SuppressWarnings({ &quot;rawtypes&quot;, &quot;unchecked&quot; }) Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); template.setValueSerializer(jackson2JsonRedisSerializer); } } 5、新建UserMapper package springboot.dao; import org.apache.ibatis.annotations.Delete; import org.apache.ibatis.annotations.Insert; import org.apache.ibatis.annotations.Mapper; import org.apache.ibatis.annotations.Param; import org.apache.ibatis.annotations.Select; import org.apache.ibatis.annotations.Update; import org.springframework.cache.annotation.CacheConfig; import org.springframework.cache.annotation.CacheEvict; import org.springframework.cache.annotation.CachePut; import org.springframework.cache.annotation.Cacheable; import springboot.domain.User; @Mapper @CacheConfig(cacheNames = &quot;users&quot;) public interface UserMapper { @Insert(&quot;insert into user(name,age) values(#{name},#{age})&quot;) int addUser(@Param(&quot;name&quot;)String name,@Param(&quot;age&quot;)String age); @Select(&quot;select * from user where id =#{id}&quot;) @Cacheable(key =&quot;#p0&quot;) User findById(@Param(&quot;id&quot;) String id); @CachePut(key = &quot;#p0&quot;) @Update(&quot;update user set name=#{name} where id=#{id}&quot;) void updataById(@Param(&quot;id&quot;)String id,@Param(&quot;name&quot;)String name); //如果指定为 true，则方法调用后将立即清空所有缓存 @CacheEvict(key =&quot;#p0&quot;,allEntries=true) @Delete(&quot;delete from user where id=#{id}&quot;) void deleteById(@Param(&quot;id&quot;)String id); } @Cacheable将查询结果缓存到redis中，（key=”#p0”）指定传入的第一个参数作为redis的key。 @CachePut，指定key，将更新的结果同步到redis中 @CacheEvict，指定key，删除缓存数据，allEntries=true,方法调用后将立即清除缓存 共享Session-spring-session-data-redis分布式系统中，sessiong共享有很多的解决方案，其中托管到缓存中应该是最常用的方案之一， 多台分布式机器共享Session的解决方案 如何使用 引入依赖 org.springframework.session spring-session-data-redis Session配置 @Configuration @EnableRedisHttpSession(maxInactiveIntervalInSeconds = 86400*30) public class SessionConfig { } maxInactiveIntervalInSeconds: 设置Session失效时间，使用Redis Session之后，原Boot的server.session.timeout属性不再生效 测试 添加测试方法获取sessionid @RequestMapping(&quot;/uid&quot;) String uid(HttpSession session) { UUID uid = (UUID) session.getAttribute(&quot;uid&quot;); if (uid == null) { uid = UUID.randomUUID(); } session.setAttribute(&quot;uid&quot;, uid); return session.getId(); } 登录redis 输入 keys ‘sessions‘ xml t&lt;spring:session:sessions:db031986-8ecc-48d6-b471-b137a3ed6bc4 t(spring:session:expirations:1472976480000 其中 1472976480000为失效时间，意思是这个时间后session失效，db031986-8ecc-48d6-b471-b137a3ed6bc4 为sessionId,登录http://localhost:8080/uid 发现会一致，就说明session 已经在redis里面进行有效的管理了。这条session作为数据储存在Redis数据库中。 如何在两台或者多台中共享session其实就是按照上面的步骤在另一个项目中再次配置一次，启动后自动就进行了session共享。 多台机器可以共享这个Redis数据库，里面自然有Session数据。]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot之路(一)--概念介绍及注解配置]]></title>
    <url>%2F2018%2F03%2F22%2Fspringboot%2F</url>
    <content type="text"><![CDATA[Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。也被称为spring的脚手架。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。 spring boot 致力于简洁，让开发者写更少的配置，程序能够更快的运行和启动。它是下一代javaweb框架，并且它是spring cloud（微服务）的基础。(spring cloud的基础就是基于spring boot来进行配置)，spring boot就是为了让开发人员从繁重的配置工作解放出来，快速参与开发。 创建独立的Spring应用程序 嵌入的Tomcat，无需部署WAR文件 简化Maven配置 自动配置Spring 提供生产就绪型功能，如指标，健康检查和外部配置 绝对没有代码生成和对XML没有要求配置（由java配置替代XML配置，也就是零XML配置） 从最根本上来讲，Spring Boot就是一些库的集合，它能够被任意项目的构建系统所使用。简便起见，该框架也提供了命令行界面，它可以用来运行和测试Boot应用。框架的发布版本，包括集成的CLI（命令行界面），可以在Spring仓库中手动下载和安装。一种更为简便的方式是使用Groovy环境管理器（Groovy enVironment Manager，GVM），它会处理Boot版本的安装和管理。Boot及其CLI可以通过GVM的命令行gvm install springboot进行安装。 要进行打包和分发的工程会依赖于像Maven或Gradle这样的构建系统。为了简化依赖图，Boot的功能是模块化的，通过导入Boot所谓的“starter”模块，可以将许多的依赖添加到工程之中。为了更容易地管理依赖版本和使用默认配置，框架提供了一个parent POM，工程可以继承它。 如何spring boot快速开发如何进行spring boot的快速开发和学习，以便于熟悉这个框架所带来的方便。最好的办法就是在项目中去参与开发，体验会所带来的FEEL！！ 首先我们需要一个项目以方便我们了解利用spirng boot框架的项目里面到底有什么？ Test项目下载spring官网为我们提供了可方便快速上手体验的Test项目http://start.spring.io/ 在里面根据自己的选择JDK的版本，项目名和包名等等，点击下方的Switch to the full version可以选择java的版本（还可以引入所依赖的组件，这里我们先下载默认的项目，若是以后有需要的jar包再通过maven导入即可） 我的选择界面如下： 选择好后然后点击Generate Project下载项目压缩包 解压后，使用eclipse，Import -&gt; Existing Maven Projects -&gt; Next -&gt;选择解压后的文件夹-&gt; Finsh，OK done! 项目目录结构分析 可以看到Spring Boot的基础结构共三个文件 src/main/java 程序开发以及主程序入口src/main/resources 配置文件src/test/java 测试程序 这里跟我们平时在eclipse中建立的maven项目目录结构一致 spingboot建议的目录结果如下（根据你的公司的开发格式要求为重）： root package结构：com.example.myproject com +- example +- myproject +- Application.java | +- domain | +- Customer.java | +- CustomerRepository.java | +- service | +- CustomerService.java | +- controller | +- CustomerController.java | 其实也就是分包，按照以往的springmvc的开发模式，Entity中方实体类，也就是数据库与服务端的字段映射POJO，Controller中放控制层（负责页面访问控制），Service中负责处理相关业务逻辑，Dao层负责处理数据库交互，Util负责放入一些工具类，aspect为切面组件，converter为消息转化组件等等。 而spring boot中建议以java配置取代繁重的XML配置（或者用更简化的Groovy来代替），这个项目用java配置演示。 1、Application.java 建议放到跟目录下面,主要用于做一些框架配置（java配置文件） 2、domain 目录主要用于实体（Entity）与数据访问层（Repository） 3、service 层主要是业务类代码 4、controller 负责页面访问控制 采用默认配置可以省去很多配置，当然也可以根据自己的喜欢来进行更改 最后，启动Application main方法，至此一个java项目搭建好了！ 项目POM.XML在项目的POM.xml文件中引入父POM，来初始化一些常用的功能（不用重新去导入其他包，当然也有办法不用这个父POM），项目自动配好了，若是自己编写项目则需要手动配置。 &lt;!-- Inherit defaults from Spring Boot --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;!-- spring-boot-maven-plugin插件 在SpringBoot项目中开启的方式有两种 一种是run java.application 还有一种就是这个插件开启--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;!--支持springboot所需的jar包依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 以上这些就是官网提供项目的POM配置。 还需要引入一个web模块 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; pom.xml文件中默认有两个模块： spring-boot-starter：核心模块，包括自动配置支持、日志和YAML； spring-boot-starter-test：测试模块，包括JUnit、Hamcrest、Mockito。 测试一下页面访问的Controller看看效果 package com.example.springboot.controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; //springboot特有注解，包含@Controller和@ResponseBody //所以这里就相当于控制层组件和输出Json格式字符串，极其方便 @RestController public class HelloController { @RequestMapping(&quot;/hel&quot;) public String index(){ return &quot;Hello World&quot;; } //测试成功！ } 注意：本机的tomcat端口若是自己设置的，需要在application.properties中去设置端口号，否则将访问不到。 我的设置是：server.port=8088 配置完后直接测试 http://localhost:8088/hel 出现结果 @RestController的意思就是controller里面的方法都以json格式输出，不用再写什么jackjson配置的了！跟我们之前所使用的@ResponseBody一样，向浏览器直接输出Json格式的数据 当然在这里我贴出一些可能出现问题的解法包括我自己是怎么解决的 出现的问题及解决方案当我们部署完后可以直接run运行这个Test项目看下是否输入路径能否得到这个json字符串这时候可能会出现Whitelabel Error Page的提示，这时候表示这个项目并没运行成功我去网上找找方法，常见的解决方法是去看看SpringbootApplication.java这个类是否放在项目根目录下（目录结构不对的问题）因为springboot是默认加载这个配置文件下的所有子包，我查看了一下我的包结构 发现并没有问题，SpringbootApplication.java配置也放在根目录下了，controller是其子包没错。 后来发现是springboot项目在运行的时候，eclipse需要你去手动配置springboot的项目以及执行配置的main方法所在类这时候我们可以在所运行的项目下右键–&gt;run Configuations，然后在Main标签上的project中选择你所要执行的springboot项目Main Class位置选择执行配置的main方法所在类，这时候重新运行就生效了，这是因为我之前也有用过springboot框架建了一个项目，这时候以为内原来的配置是上一个项目的，这时候需要我们手动去将项目重新设置。 另外这是我的解决方法，每个人都可能遇到不同的问题，所以这就是需要我们要去融入问题本身，再加上eclipse的debug模式去发现系统报给我们的错误的真正原因。 如何做单元测试利用我们刚刚做的这个Test类，我们要做个单元测试，将服务端和浏览器的交互数据都给打印出来，包括Springmvc各组件的数据交互。打开的src/test/下的测试入口，编写简单的http请求来测试；使用mockmvc进行，利用MockMvcResultHandlers.print()打印出执行结果。 注意：@SpringApplicationConfiguration(classes = Application.class) 报错，注解不能导入。 //springJunit支持，由于引入了Spring-Test框架支持。 @RunWith(SpringJUnit4ClassRunner.class) //这个注解在1.5版本后移除了，所以用两个注解就可以实现测试， //@SpringApplicationConfiguration(classes = MockServletContext.class) //由于是web项目，Junit需要模拟ServletContext，需要给测试类加上@WebAppConfiguration注解 @WebAppConfiguration public class HelloWorldControlerTests { private MockMvc mvc; @Before public void setUp() throws Exception { mvc = MockMvcBuilders.standaloneSetup(new HelloController()).build(); } @Test public void getHello() throws Exception { mvc.perform(MockMvcRequestBuilders.get(&quot;/hel&quot;).accept(MediaType.APPLICATION_JSON)) .andExpect(MockMvcResultMatchers.status().isOk()) .andDo(MockMvcResultHandlers.print()) .andReturn(); } } 热启动在正常开发项目中已经很常见了吧，虽然平时开发web项目过程中，改动项目启重启总是报错；但springBoot对调试支持很好，修改之后可以实时生效，需要添加以下的配置： &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 该模块在完整的打包环境下运行的时候会被禁用。如果你使用java -jar启动应用或者用一个特定的classloader启动，它会认为这是一个“生产环境”。 关于Eclipse的热启动及热部署 Tomcat的热部署（以后就不用重启了）1、 tomcat上的部署问题，有时候也是个麻烦的问题，要是不采用热部署，我们就只能每次对原来的文件做一次改动的时候就要重新部署，而每次重新部署都要关闭tomcat，部署完重起tomcat，可见这是一个多么烦人的事情。现在，我们可以采用热部署了，以后，就不用做凡人的关闭重起工作。 实现方式：编辑Tomcat的server.xml 在host节点内加入 “myapp” 为要部署的应用程序，通常在webapps目录下 元素的属性: path:指定访问该Web应用的URL入口。 docBase:指定Web应用的文件路径，可以给定绝对路径，也可以给定相对于 的appBase属性的相对路径，如果Web应用采用开放目录结构，则指定Web应用的根目录，如果Web应用是个war文件，则指定war文件的路径。 reloadable:如果这个属性设为true，tomcat服务器在运行状态下会监视在WEB-INF/classes和WEB-INF/lib目录下class文件的改动，如果监测到有class文件被更新的，服务器会自动重新加载Web应用。 在开发阶段将reloadable属性设为true，有助于调试servlet和其它的class文件，但这样用加重服务器运行负荷，建议在Web应用的发存阶段将reloadable设为false。 2、双击tomcat 服务器，切换到modules 界面，把项目的auto_reload 设置为Disabled 保存 这种方法只适用于改变类的方法实现，如果当一个类改变结构、或者配置文件修改了，tomcat是没办法热加载的，需要重启tomcat。搞定！ 使用spring boot可以非常方便、快速搭建项目，使我们不用关心框架之间的兼容性，适用版本等各种问题，我们想使用任何东西，仅仅添加一个配置就可以，所以使用sping boot非常适合构建微服务。 ——————更新线—————— 关于SpringBootApplication的各项注解配置 较为常用的注解有以下这些： @SpringBootApplication: 包含@Configuration、@EnableAutoConfiguration、@ComponentScan通常用在主类上。 @Repository:用于标注数据访问组件，即DAO组件。 @Service:用于标注业务层组件。 @RestController:4.0重要的一个新的改进是@RestController注解，它继承自@Controller注解。4.0之前的版本，Spring MVC的组件都使用@Controller来标识当前类是一个控制器servlet。使用这个特性，我们可以开发REST服务的时候不需要使用@Controller而专门的@RestController。当你实现一个RESTful web services的时候，response将一直通过response body发送。为了简化开发，Spring 4.0提供了一个专门版本的controller。用于标注控制层组件(如struts中的action)，包含@Controller和@ResponseBody。 @ResponseBody：表示该方法的返回结果直接写入HTTP response body中一般在异步获取数据时使用，在使用@RequestMapping后，返回值通常解析为跳转路径，加上@responsebody后返回结果不会被解析为跳转路径，而是直接写入HTTP response body中。比如异步获取json数据，加上@responsebody后，会直接返回json数据。 @Controller如果我们需要使用页面开发只要使用 @Controller @Component：泛指组件，当组件不好归类的时候，我们可以使用这个注解进行标注。 @ComponentScan：组件扫描。个人理解相当于context:component-scan，如果扫描到有@Component @Controller @Service等这些注解的类，则把这些类注册为bean。 @Configuration：指出该类是 Bean 配置的信息源，相当于XML中的，一般加在主类上。 @Bean:相当于XML中的,放在方法的上面，而不是类，意思是产生一个bean,并交给spring管理。 @EnableAutoConfiguration：让 Spring Boot 根据应用所声明的依赖来对 Spring 框架进行自动配置，一般加在主类上。 @AutoWired:byType方式。把配置好的Bean拿来用，完成属性、方法的组装，它可以对类成员变量、方法及构造函数进行标注，完成自动装配的工作。当加上（required=false）时，就算找不到bean也不报错。 @Qualifier：当有多个同一类型的Bean时，可以用@Qualifier(“name”)来指定。与@Autowired配合使用 @Resource(name=”name”,type=”type”)：没有括号内内容的话，默认byName。与@Autowired干类似的事。 @RequestMapping：RequestMapping是一个用来处理请求地址映射的注解，可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径。该注解有六个属性：params:指定request中必须包含某些参数值是，才让该方法处理。headers:指定request中必须包含某些指定的header值，才能让该方法处理请求。value:指定请求的实际地址，指定的地址可以是URI Template 模式method:指定请求的method类型， GET、POST、PUT、DELETE等consumes:指定处理请求的提交内容类型（Content-Type），如application/json,text/html;produces:指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回 @RequestParam：用在方法的参数前面。可以有效规避浏览器传参与服务端属性名字不一致的问题，最好每个参数前面都要加@RequestParam String a =request.getParameter(“a”)。 @PathVariable: 路径变量。参数与大括号里的名字一样要相同。 RequestMapping(&quot;user/get/mac/{macAddress}&quot;) public String getByMacAddress(@PathVariable String macAddress){ //do something; } @ProfilesSpring Profiles提供了一种隔离应用程序配置的方式，并让这些配置只能在特定的环境下生效。任何@Component或@Configuration都能被@Profile标记，从而限制加载它的时机。 @Configuration @Profile(&quot;prod&quot;) public class ProductionConfiguration { // ... } @ConfigurationPropertiesSpring Boot将尝试校验外部的配置，默认使用JSR-303（如果在classpath路径中）。你可以轻松的为你的@ConfigurationProperties类添加JSR-303 javax.validation约束注解： @Component @ConfigurationProperties(prefix=&quot;connection&quot;) public class ConnectionSettings { @NotNull private InetAddress remoteAddress; // ... getters and setters } 全局异常处理 @ControllerAdvice：包含@Component。可以被扫描到。统一处理异常。 @ExceptionHandler（Exception.class）：用在方法上面表示遇到这个异常就执行以下方法。 自定义Filter我们常常在项目中会使用filters用于录调用日志、排除有XSS威胁的字符、执行权限验证等等。Spring Boot自动添加了OrderedCharacterEncodingFilter和HiddenHttpMethodFilter，并且我们可以自定义Filter。 两个步骤： 1.实现Filter接口，实现Filter方法2.添加@Configuration 注解，将自定义Filter加入过滤链 @Configuration public class WebConfiguration { @Bean public RemoteIpFilter remoteIpFilter() { return new RemoteIpFilter(); } @Bean public FilterRegistrationBean testFilterRegistration() { FilterRegistrationBean registration = new FilterRegistrationBean(); registration.setFilter(new MyFilter()); registration.addUrlPatterns(&quot;/*&quot;); registration.addInitParameter(&quot;paramName&quot;, &quot;paramValue&quot;); registration.setName(&quot;MyFilter&quot;); registration.setOrder(1); return registration; } public class MyFilter implements Filter { @Override public void destroy() { // TODO Auto-generated method stub } @Override public void doFilter(ServletRequest srequest, ServletResponse sresponse, FilterChain filterChain) throws IOException, ServletException { // TODO Auto-generated method stub HttpServletRequest request = (HttpServletRequest) srequest; System.out.println(&quot;this is MyFilter,url :&quot;+request.getRequestURI()); filterChain.doFilter(srequest, sresponse); } @Override public void init(FilterConfig arg0) throws ServletException { // TODO Auto-generated method stub } } } 自定义Property相信很多人选择Spring Boot主要是考虑到它既能兼顾Spring的强大功能，还能实现快速开发的便捷。我们在Spring Boot使用过程中，最直观的感受就是没有了原来自己整合Spring应用时繁多的XML配置内容，替代它的是在pom.xml中引入模块化的Starter POMs，其中各个模块都有自己的默认配置，所以如果不是特殊应用场景，就只需要在application.properties中完成一些属性配置就能开启各模块的应用。 在web开发的过程中，我经常需要自定义一些配置文件，如何使用呢?让我们来体会参数配置文件的强大！！ -自定义普通参数配置在application.properties中，例如： com.neo.title=路飞 com.neo.description=海贼王 @Component //配置好后，直接在类中用SPEL表达式引用即可，所有的参数配置都是写在这里，都是为了避免将程序写死 public class NeoProperties { @Value(&quot;${com.neo.title}&quot;) private String title; @Value(&quot;${com.neo.description}&quot;) private String description; //省略getter settet方法 } -参数间引用另外，在application.properties中，各参数也可以通过SPEL表达式来调用彼此(参数间引用)，比如： com.neo.title=路飞 com.neo.description=海贼王 com.neo.onepiece=${com.neo.title}是${com.neo.description} -使用随机数 在一些情况下，有些参数我们需要希望它不是一个固定的值，比如密钥、服务端口等。Spring Boot的属性配置文件中可以通过${random}来产生int值、long值或者string字符串，来支持属性的随机值。 // 随机字符串 com.didispace.blog.value=${random.value} // 随机int com.didispace.blog.number=${random.int} // 随机long com.didispace.blog.bignumber=${random.long} // 10以内的随机数 com.didispace.blog.test1=${random.int(10)} // 10-20的随机数 com.didispace.blog.test2=${random.int[10,20]} -通过命令行设置属性值不仅可以通过applecation.properties文件来配置参数，还可以用命令行的方式 相信使用过一段时间Spring Boot的用户，一定知道这条命令：java -jar xxx.jar –server.port=8888，通过使用–server.port属性来设置xxx.jar应用的端口为8888。 在命令行运行时，连续的两个减号–就是对application.properties中的属性值进行赋值的标识。所以，java -jar xxx.jar –server.port=8888命令，等价于我们在application.properties中添加属性server.port=8888，该设置在样例工程中可见，读者可通过删除该值或使用命令行来设置该值来验证。 通过命令行来修改属性值固然提供了不错的便利性，但是通过命令行就能更改应用运行的参数，那岂不是很不安全？是的，所以Spring Boot也贴心的提供了屏蔽命令行访问属性的设置，只需要这句设置就能屏蔽：SpringApplication.setAddCommandLineProperties(false)。 log配置配置输出的地址和输出级别，同样也是在application.properties里面设置参数即可 logging.path=/user/local/log logging.level.com.favorites=DEBUG logging.level.org.springframework.web=INFO logging.level.org.hibernate=ERROR path为本机的log地址，logging.level 后面可以根据包路径配置不同资源的log级别（关于等级说明在java常用类库–log日志 里面可以找到） 将日志输出到指定位置 数据库操作重点讲述mysql、spring data jpa的使用，其中mysql 就不用说了大家很熟悉，jpa是利用Hibernate生成各种自动化的sql，如果只是简单的增删改查，基本上不用手写了，spring内部已经帮大家封装实现了。实现了Mysql与Hibernate的结合，既可以自动生成sql语句简化开发，又可以利用mysql的快速方便不冗余的特点。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; 数据库及jpa配置文件（写在application.properties里） spring.datasource.url=jdbc:mysql://localhost:3306/test spring.datasource.username=root spring.datasource.password=root spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.jpa.properties.hibernate.hbm2ddl.auto=update spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL5InnoDBDialect spring.jpa.show-sql= true 其实这个hibernate.hbm2ddl.auto参数的作用主要用于：自动创建|更新|验证数据库表结构,有四个值： 1.create： 每次加载hibernate时都会删除上一次的生成的表，然后根据你的model类再重新来生成新表，哪怕两次没有任何改变也要这样执行，这就是导致数据库表数据丢失的一个重要原因。 2.create-drop ：每次加载hibernate时根据model类生成表，但是sessionFactory一关闭,表就自动删除。 3.update：最常用的属性，第一次加载hibernate时根据model类会自动建立起表的结构（前提是先建立好数据库），以后加载hibernate时根据 model类自动更新表结构，即使表结构改变了但表中的行仍然存在不会删除以前的行。要注意的是当部署到服务器后，表结构是不会被马上建立起来的，是要等 应用第一次运行起来后才会。 4.validate ：每次加载hibernate时，验证创建数据库表结构，只会和数据库中的表进行比较，不会创建新表，但是会插入新值。 dialect 主要是指定生成表名的存储引擎为InneoDBshow-sql 是否打印出自动生产的SQL，方便调试的时候查看 添加实体类和Dao： @Entity public class User implements Serializable { private static final long serialVersionUID = 1L; @Id @GeneratedValue private Long id; @Column(nullable = false, unique = true) private String userName; @Column(nullable = false) private String passWord; @Column(nullable = false, unique = true) private String email; @Column(nullable = true, unique = true) private String nickName; @Column(nullable = false) private String regTime; //省略getter settet方法、构造方法 } 关于@Column 注解详情 @Column标记表示所持久化属性所映射表中的字段，该注释的属性定义如下： @Target({METHOD, FIELD}) @Retention(RUNTIME) public @interface Column { String name() default &quot;&quot;; boolean unique() default false; boolean nullable() default true; boolean insertable() default true; boolean updatable() default true; String columnDefinition() default &quot;&quot;; String table() default &quot;&quot;; int length() default 255; int precision() default 0; int scale() default 0; } 在使用此@Column标记时，需要注意以下几个问题：此标记可以标注在getter方法或属性前，例如以下的两种标注方法都是正确的： 标注在属性前： @Entity @Table(name = &quot;contact&quot;) public class ContactEO{ @Column(name=&quot; contact_name &quot;) private String name; } 标注在getter方法前： @Entity @Table(name = &quot;contact&quot;) public class ContactEO{ @Column(name=&quot; contact_name &quot;) public String getName() { return name; } } JPA规范中并没有明确指定那种标注方法，只要两种标注方式任选其一都可以。这根据个人的喜好来选择 属性都是什么意思： unique属性表示该字段是否为唯一标识，默认为false。如果表中有一个字段需要唯一标识，则既可以使用该标记，也可以使用@Table标记中的@UniqueConstraint。 nullable属性表示该字段是否可以为null值，默认为true。 insertable属性表示在使用“INSERT”脚本插入数据时，是否需要插入该字段的值。 updatable属性表示在使用“UPDATE”脚本插入数据时，是否需要更新该字段的值。insertable和updatable属性一般多用于只读的属性，例如主键和外键等。这些字段的值通常是自动生成的。 columnDefinition属性表示创建表时，该字段创建的SQL语句，一般用于通过Entity生成表定义时使用。 table属性表示当映射多个表时，指定表的表中的字段。默认值为主表的表名。有关多个表的映射将在本章的5.6小节中详细讲述。 length属性表示字段的长度，当字段的类型为varchar时，该属性才有效，默认为255个字符。 precision属性和scale属性表示精度，当字段类型为double时，precision表示数值的总长度，scale表示小数点所占的位数。 dao只要继承JpaRepository类就可以，几乎可以不用写方法，还有一个特别有尿性的功能非常赞，就是可以根据方法名来自动的生产SQL，比如findByUserName 会自动生产一个以 userName 为参数的查询方法，比如 findAlll 自动会查询表里面的所有数据，比如自动分页等等 Entity中不映射成列的字段得加@Transient 注解，不加注解也会映射成列 package com.example.springboot.domain; import org.springframework.data.jpa.repository.JpaRepository; public interface UserDao extends JpaRepository&lt;User, Long&gt;{ User findByUserName(String userName); User findByUserNameOrEmail(String username, String email); } 测试： @RunWith(SpringJUnit4ClassRunner.class) //@SpringApplicationConfiguration(Application.class) public class UserDaoTest { @Autowired private UserDao userDao; @Test public void test() throws Exception { Date date = new Date(); DateFormat dateFormat = DateFormat.getDateTimeInstance(DateFormat.LONG, DateFormat.LONG); String formattedDate = dateFormat.format(date); userDao.save(new User(&quot;1&quot;,&quot;aaa&quot;,&quot;123456&quot;,&quot;123@com&quot;,&quot;AAA&quot;,formattedDate)); userDao.save(new User(&quot;2&quot;,&quot;bbb&quot;,&quot;123456&quot;,&quot;123@com&quot;,&quot;BBB&quot;,formattedDate)); userDao.save(new User(&quot;3&quot;,&quot;ccc&quot;,&quot;123456&quot;,&quot;123@com&quot;,&quot;CCC&quot;,formattedDate)); Assert.assertEquals(9, userDao.findAll().size()); Assert.assertEquals(&quot;bb&quot;, userDao.findByUserNameOrEmail(&quot;bbb&quot;, &quot;123@com&quot;).getNickName()); userDao.delete(userDao.findByUserName(&quot;aaa&quot;)); } } thymeleaf模板Spring boot 推荐使用来代替jsp Thymeleaf是一款用于渲染XML/XHTML/HTML5内容的模板引擎。类似JSP，Velocity，FreeMaker等，它也可以轻易的与Spring MVC等Web框架进行集成作为Web应用的模板引擎。与其它模板引擎相比，Thymeleaf最大的特点是能够直接在浏览器中打开并正确显示模板页面，而不需要启动整个Web应用。 1.Thymeleaf 在有网络和无网络的环境下皆可运行，即它可以让美工在浏览器查看页面的静态效果，也可以让程序员在服务器查看带数据的动态页面效果。这是由于它支持 html 原型，然后在 html 标签里增加额外的属性来达到模板+数据的展示方式。浏览器解释 html 时会忽略未定义的标签属性，所以 thymeleaf 的模板可以静态地运行；当有数据返回到页面时，Thymeleaf 标签会动态地替换掉静态内容，使页面动态显示。 2.Thymeleaf 开箱即用的特性。它提供标准和spring标准两种方言，可以直接套用模板实现JSTL、 OGNL表达式效果，避免每天套模板、该jstl、改标签的困扰。同时开发人员也可以扩展和创建自定义的方言。 3.Thymeleaf 提供spring标准方言和一个与 SpringMVC 完美集成的可选模块，可以快速的实现表单绑定、属性编辑器、国际化等功能。 我们已经习惯使用了什么 velocity,FreMaker，beetle之类的模版，那么到底好在哪里呢？ 比一比吧 Thymeleaf是与众不同的，因为它使用了自然的模板技术。这意味着Thymeleaf的模板语法并不会破坏文档的结构，模板依旧是有效的XML文档。模板还可以用作工作原型，Thymeleaf会在运行期替换掉静态值。Velocity与FreeMarker则是连续的文本处理器。 下面的代码示例分别使用Velocity、FreeMarker与Thymeleaf打印出一条消息： Velocity: &lt;p&gt;$message&lt;/p&gt; FreeMarker: &lt;p&gt;${message}&lt;/p&gt; Thymeleaf: &lt;p th:text=&quot;${message}&quot;&gt;Hello World!&lt;/p&gt; 注意：由于Thymeleaf使用了XML DOM解析器，因此它并不适合于处理大规模的XML文件。 具体的在组件官方网站有详细介绍：https://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html#introducing-thymeleaf 涵盖了常见的前端操作，比如，判断，循环，引入模板，常用函数（日期格式化，字符串操作）下拉，js和css中使用，基本可以应对一般场景。 引用命名空间 在html中引入此命名空间，可避免编辑器出现html验证错误，虽然加不加命名空间对Thymeleaf的功能没有任何影响。 URL URL在Web应用模板中占据着十分重要的地位，需要特别注意的是Thymeleaf对于URL的处理是通过语法@{…}来处理的。Thymeleaf支持绝对路径URL： &lt;a th:href=&quot;@{http://www.thymeleaf.org}&quot;&gt;Thymeleaf&lt;/a&gt; 条件求值 Login 等等，在开发文档中都有介绍 页面即原型 在Web开发过程中一个绕不开的话题就是前端工程师与后端工程师的写作，在传统Java Web开发过程中，前端工程师和后端工程师一样，也需要安装一套完整的开发环境，然后各类Java IDE中修改模板、静态资源文件，启动/重启/重新加载应用服务器，刷新页面查看最终效果。 但实际上前端工程师的职责更多应该关注于页面本身而非后端，使用JSP，Velocity等传统的Java模板引擎很难做到这一点，因为它们必须在应用服务器中渲染完成后才能在浏览器中看到结果，而Thymeleaf从根本上颠覆了这一过程，通过属性进行模板渲染不会引入任何新的浏览器不能识别的标签。 Thymeleaf的用法也可参考：https://www.cnblogs.com/topwill/p/7434955.html WebJarsWebJars是一个很神奇的东西，可以让大家以jar包的形式来使用前端的各种框架、组件。 什么是WebJars？WebJars是将客户端（浏览器）资源（JavaScript，Css等）打成jar包文件，以对资源进行统一依赖管理。WebJars的jar包部署在Maven中央仓库上。 我们在开发Java web项目的时候会使用像Maven，Gradle等构建工具以实现对jar包版本依赖管理，以及项目的自动化管理，但是对于JavaScript，Css等前端资源包，我们只能采用拷贝到webapp下的方式，这样做就无法对这些资源进行依赖管理。那么WebJars就提供给我们这些前端资源的jar包形势，我们就可以进行依赖管理。 WebJars官网：http://www.webjars.org/WebJars教程整理：https://www.cnblogs.com/liaojie970/p/7852576.html 关于SpringBoot的多环境配置（实际开发）我们在开发Spring Boot应用时，通常同一套程序会被应用和安装到几个不同的环境，比如：开发、测试、生产等。其中每个环境的数据库地址、服务器端口等等配置都会不同，如果在为不同环境打包时都要频繁修改配置文件的话，那必将是个非常繁琐且容易发生错误的事。 对于多环境的配置，各种项目构建工具或是框架的基本思路是一致的，通过配置多份不同环境的配置文件，再通过打包命令指定需要打包的内容之后进行区分打包，Spring Boot也不例外，或者说更加简单。 1、开发环境是指开发时所运行的环境，比较随意，只是为了方便开发调试的，是对程序员开放的；而生产环境就不一样了，2、生产环境是系统开发的最后一个环节了，也就是上线，是要提供对外服务的，是要关闭错误报告的，然后把日志打印出来；还有一个测试环境对吧，那就顾名思义，3、测试环境就是上线前的测试，测试环境都不通过的话，肯定是不能上线的啊，也就是不能发布到生产环境上。系统开发流程及对应环境： 开发（开发环境，随意，重运行）—&gt;测试（测试环境，重功能）—&gt;上线（生产环境，重实现） 在Spring Boot中多环境配置文件名需要满足application-{profile}.properties的格式，其中{profile}对应你的环境标识，比如： application-dev.properties：开发环境application-test.properties：测试环境application-prod.properties：生产环境 至于哪个具体的配置文件会被加载，需要在application.properties文件中通过spring.profiles.active属性来设置，其值对应{profile}值。 如：spring.profiles.active=test就会加载application-test.properties配置文件内容 下面，以不同环境配置不同的服务端口为例，进行样例实验。针对各环境新建不同的配置文件application-dev.properties、application-test.properties、application-prod.properties 在这三个文件均都设置不同的server.port属性，如：dev环境设置为1111，test环境设置为2222，prod环境设置为3333 application.properties中设置spring.profiles.active=dev，就是说默认以dev环境设置 测试不同配置的加载 执行java -jar xxx.jar，可以观察到服务端口被设置为1111，也就是默认的开发环境（dev）执行java -jar xxx.jar –spring.profiles.active=test，可以观察到服务端口被设置为2222，也就是测试环境的配置（test）执行java -jar xxx.jar –spring.profiles.active=prod，可以观察到服务端口被设置为3333，也就是生产环境的配置（prod）按照上面的实验，可以如下总结多环境的配置思路： application.properties中配置通用内容，并设置spring.profiles.active=dev，以开发环境为默认配置application-{profile}.properties中配置各个环境不同的内容通过命令行方式去激活不同环境的配置 关于springboot的AOPAOP即aspect oriented programming面向切面编程，不需要修改原本代码便可将独立功能切入某个层面。通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术。AOP是Spring框架中的一个重要内容，它通过对既有程序定义一个切入点，然后在其前后切入不同的执行内容，比如常见的有：打开数据库连接/关闭数据库连接、打开事务/关闭事务、记录日志等。基于AOP不会破坏原来程序逻辑，因此它可以很好的对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。aop的理解：我们传统的编程方式是垂直化的编程，即A–&gt;B–&gt;C–&gt;D这么下去，一个逻辑完毕之后执行另外一段逻辑。但是AOP提供了另外一种思路，它的作用是在业务逻辑不知情（即业务逻辑不需要做任何的改动）的情况下对业务代码的功能进行增强，这种编程思想的使用场景有很多，例如事务提交、方法执行之前的权限检测、日志打印、方法调用事件等等 其用到的动态代理技术为：本身集成的JDK动态代理和第三方cglib动态代理两者本身各有优缺（不过JDK的原理要满足sun规范，cglib就没有那么多的限制）JDK基于java反射机制，而cglib则是基于asm（java字节码）来动态增加既有类功能的JDK是在生成切面上足够高效，而cglib则是在生成切面后的功能上比较高效，根据实际需求去选择使用哪种动态代理技术。 AOP在实际开发中能做到很多事，比如在原框架上新追加的切面功能实现，流量审计，WEB日志等等。。 下面主要讲两个内容，一个是如何在Spring Boot中引入Aop功能，二是如何使用Aop做切面去统一处理Web请求的日志。 第一步：确保自己的springboot所属工程存在spring-boot-starter-web模块，导入jar包即可 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; 第二步：引入AOP依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; 在完成了引入AOP依赖包后，一般来说并不需要去做其他配置。也许在Spring中使用过注解配置方式的人会问是否需要在程序主类中增加@EnableAspectJAutoProxy来启用，实际并不需要。 可以看下面关于AOP的默认配置属性，其中spring.aop.auto属性默认是开启的，也就是说只要引入了AOP依赖后，默认已经增加了@EnableAspectJAutoProxy。 第三步：在applicaton.properties文件中配置AOP的各项属性配置 spring.aop.auto=true //就相当于在在驱动主类上配置了@EnableAspectAutoProxy注解，切面自动代理spring.aop.proxy-target-class=false //若是不用JDK自动代理，也可以通过这个开启Cglib的自动代理 而当我们需要使用CGLIB来实现AOP的时候，需要配置spring.aop.proxy-target-class=true，不然默认使用的是标准Java的实现。 第四步：确认切点位置 假设切点为：com.didispace.controller包下的控制器，我们需要以这个切点为基础 @RestController public class HelloController { @RequestMapping(&quot;/hello&quot;) public String hello(@RequestParam String name){ return &quot;HelloKoro&quot;; } } 第五步：可以编写切面了，自由度超高，基本可以在任何想要的层级上切入逻辑 实现AOP的切面主要有以下几个要素：（主要是切面和切点） 使用@Aspect注解将一个java类定义为切面类使用@Pointcut定义一个切入点，可以是一个规则表达式，比如下例中某个package下的所有函数，也可以是一个注解等。根据需要在切入点不同位置的切入内容 -使用@Before在切入点开始处切入内容-使用@After在切入点结尾处切入内容-使用@AfterReturning在切入点return内容之后切入内容（可以用来对处理返回值做一些加工处理）-使用@Around在切入点前后切入内容，并自己控制何时执行切入点自身的内容-使用@AfterThrowing用来处理当切入内容部分抛出异常之后的处理逻辑 例如： @Aspect @Component public class WebLogAspect { //声明Logger对象 private Logger logger = Logger.getLogger(getClass()); //通过@Pointcut定义的切入点为com.didispace.web包下的所有函数 //（对web层所有请求处理做切入点） //public *（表示对访问限定符也可以规定）com.didispace.web（web包下）..*.*(..)所有方法，而且参数随意 //声明一个切入点，webLog为切入点名称 @Pointcut(&quot;execution(public * com.didispace.controller..*.*(..))&quot;) public void webLog(){} //用来具体实现webLog()方法 //JoinPoint对象封装了SpringAop中切面方法的信息,在切面方法中添加JoinPoint参数, //就可以获取到封装了该方法信息的JoinPoint对象. //JoinPoint其实就具象化为这个切面类 @Before(&quot;webLog()&quot;) public void doBefore(JoinPoint joinPoint) throws Throwable { // 接收到请求，记录请求内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 记录下请求内容 //获取URL logger.info(&quot;URL : &quot; + request.getRequestURL().toString()); //获取请求方式 logger.info(&quot;HTTP_METHOD : &quot; + request.getMethod()); //获取请求IP logger.info(&quot;IP : &quot; + request.getRemoteAddr()); //Signature getSignature();方法 //获取封装了署名信息的对象,在该对象中可以获取到目标方法名,所属类的Class等信息 //哪个方法触发了就输出哪个方法的方法名 logger.info(&quot;CLASS_METHOD : &quot; + joinPoint.getSignature().getDeclaringTypeName() + &quot;.&quot; + joinPoint.getSignature().getName()); //Object[] getArgs(); //获取传入目标方法的参数对象,也就是请求参数值 logger.info(&quot;ARGS : &quot; + Arrays.toString(joinPoint.getArgs())); } //在切入点return内容之后切入内容（可以用来对处理返回值做一些加工处理） //一旦切点有ruturn函数，则触发这个切面 @AfterReturning(returning = &quot;ret&quot;, pointcut = &quot;webLog()&quot;) public void doAfterReturning(Object ret) throws Throwable { // 处理完请求，返回内容 logger.info(&quot;RESPONSE : &quot; + ret); } //后置异常通知 //有异常则输出这个 @AfterThrowing(&quot;webLog()&quot;) public void throwss(JoinPoint joinPoint){ logger.error(&quot;Exception : &quot;+joinPoint.getTarget()); } //后置最终通知,final增强，不管是抛出异常或者正常退出都会执行 //最后一定执行，属于final级别 @After(&quot;webLog()&quot;) public void after(JoinPoint jp){ System.out.println(&quot;方法最后执行.....&quot;); } //环绕通知,环绕增强，相当于MethodInterceptor //其实我们仅使用环绕通知就可以实现前置通知、后置通知、异常通知、最终通知等的效果。 //ProceedingJoinPoint继承JoinPoint子接口，它新增了两个用于执行连接点方法的方法： //1.Object proceed()通过反射执行目标对象的连接点处的方法 //2.Object proceed(java.lang.Object[] args)通过反射执行目标对象连接点处的方法，不过使用新的参数替换原来的参数。 @Around(&quot;webLog()&quot;) public Object arround(ProceedingJoinPoint pjp) { System.out.println(&quot;方法环绕start.....&quot;); try { //proceed()方法输出的是切入点的方法名 Object o = pjp.proceed(); System.out.println(&quot;方法环绕proceed，结果是 :&quot; + o); return o; } catch (Throwable e) { e.printStackTrace(); return null; } } } 第六步：配置相关日志输出配置 // LOG4J配置 log4j.rootLogger=INFO, stdout, file, errorfile // 动态识别多环境下的日志级别 log4j.category.com.didispace=${logging.level.com.didispace}, didifile log4j.logger.error=errorfile // 控制台输出 log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n // root日志输出 log4j.appender.file=org.apache.log4j.DailyRollingFileAppender log4j.appender.file.file=logs/all.log log4j.appender.file.DatePattern=&apos;.&apos;yyyy-MM-dd log4j.appender.file.layout=org.apache.log4j.PatternLayout log4j.appender.file.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n // error日志输出 log4j.appender.errorfile=org.apache.log4j.DailyRollingFileAppender log4j.appender.errorfile.file=logs/error.log log4j.appender.errorfile.DatePattern=&apos;.&apos;yyyy-MM-dd log4j.appender.errorfile.Threshold = ERROR log4j.appender.errorfile.layout=org.apache.log4j.PatternLayout log4j.appender.errorfile.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L - %m%n // com.didispace下的日志输出 log4j.appender.didifile=org.apache.log4j.DailyRollingFileAppender log4j.appender.didifile.file=logs/my.log log4j.appender.didifile.DatePattern=&apos;.&apos;yyyy-MM-dd log4j.appender.didifile.layout=org.apache.log4j.PatternLayout log4j.appender.didifile.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} %5p %c{1}:%L ---- %m%n 这样就将aop的信息打印到文件中了，我们可以结合springboot+log4j，实现aop的日志记录功能切入到业务逻辑中 关于AOP优化问题AOP切面中的同步问题，也就是有关于性能审计的相关功能，若是我们想记录某个组件运行的时间，我们应该如何设计？传统方式是在WebLogAspect切面中，分别通过doBefore和doAfterReturning两个独立函数实现了切点头部和切点返回后执行的内容，若我们想统计请求的处理时间，就需要在doBefore处记录时间，并在doAfterReturning处通过当前时间与开始处记录的时间计算得到请求处理的消耗时间。 那么我们是否可以在WebLogAspect切面中定义一个成员变量来给doBefore和doAfterReturning一起访问呢？是否会有同步问题呢？ 的确，直接在这里定义基本类型会有同步问题，所以我们可以引入ThreadLocal对象，像下面这样进行记录： @Aspect @Component public class WebLogAspect { private Logger logger = Logger.getLogger(getClass()); ThreadLocal&lt;Long&gt; startTime = new ThreadLocal&lt;&gt;(); @Pointcut(&quot;execution(public * com.didispace.web..*.*(..))&quot;) public void webLog(){} @Before(&quot;webLog()&quot;) public void doBefore(JoinPoint joinPoint) throws Throwable { startTime.set(System.currentTimeMillis()); // 省略日志记录内容 } @AfterReturning(returning = &quot;ret&quot;, pointcut = &quot;webLog()&quot;) public void doAfterReturning(Object ret) throws Throwable { // 处理完请求，返回内容 logger.info(&quot;RESPONSE : &quot; + ret); logger.info(&quot;SPEND TIME : &quot; + (System.currentTimeMillis() - startTime.get())); } } 还有AOP切面的优先级，我们也可以进行控制，若是我们的业务中存在很多的AOP组件，那么如何管理他们的优先级呢？ 由于通过AOP实现，程序得到了很好的解耦，但是也会带来一些问题，比如：我们可能会对Web层做多个切面，校验用户，校验头信息等等，这个时候经常会碰到切面的处理顺序问题。 所以，我们需要定义每个切面的优先级，我们需要@Order(i)注解来标识切面的优先级。i的值越小，优先级越高。假设我们还有一个切面是CheckNameAspect用来校验name必须为didi，我们为其设置@Order(10)，而上文中WebLogAspect设置为@Order(5)，所以WebLogAspect有更高的优先级，这个时候执行顺序是这样的： 在@Before中优先执行@Order(5)的内容，再执行@Order(10)的内容在@After和@AfterReturning中优先执行@Order(10)的内容，再执行@Order(5)的内容所以我们可以这样子总结： 在切入点前的操作，按order的值由小到大执行在切入点后的操作，按order的值由大到小执行 就是一种像洋葱一样的aop结构，order越小就越外层。。。 SpringBoot中的事务管理支持事务管理的功能模块主要用于业务层，因为业务层要调用持久层组件，实际是对数据读写的多步操作的结合。多步读写操作意味着多步调用持久层，期间每一步的读写操作都不能够错，一旦错就需要全部回滚，之前的所有操作都无效。 事务的作用就是为了保证用户的每一个操作都是可靠的，事务中的每一步操作都必须成功执行，只要有发生异常就回退到事务开始未进行操作的状态。 事务管理是Spring框架中最为常用的功能之一，我们在使用Spring Boot开发应用时，大部分情况下也都需要使用事务。（业务层使用） 在SpringBoot中，持久层组件常用mybatis-spring-boot-starter（Mybatis），spring-boot-starter-jdbc（JDBC）或spring-boot-starter-data-jpa（spring-data-jpa，相当于Hibernate的增强版）这三个持久层技术框架。 当我们使用了spring-boot-starter-jdbc或spring-boot-starter-data-jpa依赖的时候，框架会自动默认分别注入DataSourceTransactionManager或JpaTransactionManager。所以我们不需要任何额外配置就可以用@Transactional注解进行事务的使用。 在相应的业务层加上@Transactional即可。 通常我们在测试的时候，会加上@Rollback来保证每一次测试的独立性，即测试完逻辑代码后直接进行回滚，避免数据污染。 这些只是事务管理的默认配置，在实际开发中会出现较大较为复杂的项目代码（几十万行的项目？） 如何学习Spring Boot：https://www.zhihu.com/question/53729800/answer/255785661]]></content>
      <categories>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>spring</tag>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好用的代码规范检查工具--checkstyle]]></title>
    <url>%2F2018%2F03%2F22%2Fcheckstyle%2F</url>
    <content type="text"><![CDATA[CheckStyle是SourceForge下的一个项目，提供了一个帮助Java开发人员遵守某些编码规范的工具。它能够自动化代码规范检查过程，从而使得开发人员从这项重要但枯燥的任务中解脱出来。它可以根据设置好的编码规则来检查代码。比如符合规范的变量命名，方法体的最大行数，重复代码检查等等。 如果你的项目经理开会时说：“我希望我们写出来的代码就象一个人写的!”，那么用Checkstyle绝对是正确选择！ 如何在Eclipse上安装插件在eclipse上安装插件，这里直接以安装checkstyle为例子，以后的插件安装根据这个流程来就可以了。 列举了几个网上的方法（大概有5种方法可以将插件安装到eclipse上） 第一种：直接复制法假设Eclipse的安装目录在C:\eclipse，解压下载的eclipse 插件或者安装eclipse 插件到指定目录文件夹，打开安装文件夹，在安装文件夹里的根目录分别包含两个文件夹features和plugins ，然后把两个文件夹里的文件分别复制到C:\eclipse下所对应的文件夹下的features 和plugins 下，一般的把插件文件直接复制到eclipse目录里是最直接也是最愚蠢的一种方法！因为日后想要删除这些插件会非常的困难（你根本分不清哪个是哪个），不推荐使用。 注意：直接将插件包解压到plugins文件夹下之后，重启eclipse，可能不会加载新的插件。 解决方法是： 1、打开命令行，到当前eclipse的目录下，输入eclipse -clean，重新启动eclipse，这样eclipse就会加上新的插件了。 2、如果插件不能生效，则请将eclipse\configuration\org.eclipse.update目录（此目录会记录你插件等更新历史）删除后再启动eclipse； 你可以在eclipse的菜单”Help”–&gt;”About Eclipse SDK”–&gt;”Feature Details” 和”Plug-in Details”中看到新安装的插件。 第二种：使用link文件法a.假设Eclipse的安装目录在C:\eclipse，在该文件夹下，新建这样的目录结构C:\eclipse\PluginsEclipse\jode\eclipse； b.解压下载的eclipse 插件或者安装eclipse 插件到指定目录BB（如：C:\BB）文件夹，打开BB文件夹，然后把 BB文件夹里的两个文件夹features和plugins复制到刚刚新建好 C:\eclipse\PluginsEclipse\jode\eclipse，这样eclipse中就有了两个插件目录features and plugins下。 c.在C:\eclipse目录中新建links（C:\eclipse\links）目录，在links目录中建立一个以link为扩展名的文本文件如jode.link，内容如下path=C:/eclipse/PluginsEclipse/jode 或者path=C:\eclipse\PluginsEclipse\jode（插件的目录），保存后重启eclipse插件就会安装完成。 注意：link文件中path=插件目录的path路径分隔要用\或是/ /eclipse/ links/ jode.link webtools.link updateManager.link/eclipse/ links/ jode.link webtools.link updateManager.link … … 可以在eclipse的菜单”Help”–&gt;”AboutEclipse SDK”–&gt;”Feature Details” 和”Plug-in Details”中看到新安装的插件。 第三种：使用eclipse自带图形界面安装选择Help &gt; Software Updates &gt; Manager Configuration，再选择Add &gt; Extension Location 找到你要安装插件的目录就可以了。使用eclipse的help-&gt;SoftwareUpdates -&gt;Find and install… search for new features… 输入软件安装地址进行安装强烈推荐这种方法，优点很多比如可以方便的添加删除，也不用自己写link文件！(注意：这种安装你即使没有下载安装包也可以从远程下载，只要知道链接即可，不过国内下载很多插件由于网络原因很慢，或者不能下载下来。) 备注：Eclipse插件的目录结构/eclipse-plugins/ eclipse/ .eclipseextension features/ plugins/ 第2.3种方法所指向的目录都指的是”eclipse”目录，如果用第3种方法，在eclipse这个目录下必须有文件.eclipseextension，如果下载的插件没有这个文件， 那就随便eclipse安装目录下的那个文件拷过去就行，只有有这么个文件就可以了，内容没什么用，主要是一些版本信息。例如： id=org.eclipse.platform name=Eclipse Platform version=3.1.1 id=org.eclipse.platform name=Eclipse Platform version=3.1.1 第四种：使用dropins安装插件从Eclipse3.5开始，安装目录下就多了一个dropins目录。只要将插件解压后拖到该目录即可安装插件。比如安装svn插件subclipse-1.8.16.zip，只需要如下的三步即可： 1、使用winrar等压缩软件将压缩包解压至某一文件夹，比如subclipse-1.8.16 2、将此目录移动/复制至Eclipse安装目录下的dropins目录 3、重启Eclipse。 由于此种安装方式可以将不同的插件安装在不同的目录里，并且不用麻烦地写配置文件，因此管理起来会非常方便，推荐使用。 使用eclipse marketplace自动下载这个是我用的方法，不能离线，需要从网上下载，eclipse market在线安装 在早期的eclipse版本中，market功能不是很好用，经常搜索不到插件或下来。忘记在哪个版本开始，使用market就很方便了。建议采用这种方式来安装，直接搜索，选择要安装的插件内容。无需记录引用插件位置。 打开Help -&gt; eclipse market，搜索checkstyle选择想要的版本 -&gt; install 勾选所想要应用的功能 接受协议，安装完成后，重启eclipse生效。（方便快捷，适用于有网络的环境下） 检查安装是否生效打开eclipse，Window–&gt;Preferences,如果列表中出现了checkstyle的菜单，则表示安装成功了。 注意：如果启动eclipse时，提示ClassNotFoundException等异常信息，则启动时加上一个“-clean”参数启动即可。 cd F:\Program Files\eclipse\eclipse.exe -clean 资源整理CheckStyle插件地址：http://eclipse-cs.sourceforge.net/update github开源插件地址：https://github.com/checkstyle/eclipse-cs eclipse如何安装插件：http://blog.csdn.net/netdevgirl/article/details/54382301 Eclipse中安装和使用CheckStyle： http://blog.csdn.net/qq_36871364/article/details/72472059 Eclipse Marketplace地址：http://marketplace.eclipse.org/ Checkstyle全局配置Window –&gt;Preferences,选择checkstyle菜单，增加项目组统一的规则文件，并设置为默认规则。如下图： 到此全局配置已经完成。 Checkstyle项目配置项目 –&gt; 右键Properties 到此项目配置已经完成。成功的话，checkstyle已经开始工作了。 此次发现项目上有很多红叉叉，说明是代码符合规范造成的。 根据提示信息修改后，则没有有红色的提示。 其它操作在项目右键菜单中，checkstyle还有一些快捷操作，如下图所示： checkStyle 使用选中工程，右键选择checkstyle-&gt;check code withcheckstyle,检查错误即可 Checkstyle的结果输出 序号 输出内容意义1 Type is missing a javadoc commentClass 缺少类型说明2 “{” should be on the previous line “{” 应该位于前一行3 Methos is missing a javadoc comment方法前面缺少javadoc注释4 Expected @throws tag for “Exception”在注释中希望有@throws的说明5 “.” Is preceeded with whitespace “.” 前面不能有空格6 “.” Is followed by whitespace“.” 后面不能有空格7 “=” is not preceeded with whitespace“=” 前面缺少空格8 “=” is not followed with whitespace“=” 后面缺少空格9 “}” should be on the same line“}” 应该与下条语句位于同一行10 Unused @param tag for “unused”没有参数“unused”，不需注释11 Variable “CA” missing javadoc变量“CA”缺少javadoc注释12 Line longer than 80characters行长度超过8013 Line contains a tab character行含有”tab” 字符14 Redundant “Public” modifier冗余的“public” modifier15 Final modifier out of order with the JSL suggestionFinalmodifier的顺序错误16 Avoid using the “.” form of importImport格式避免使用“.”17 Redundant import from the same package从同一个包中Import内容18 Unusedimport-java.util.listImport进来的java.util.list没有被使用19 Duplicate import to line 13重复Import同一个内容20 Import from illegal package从非法包中 Import内容21 “while” construct must use “{}”“while” 语句缺少“{}”22 Variable “sTest1” must be private and have accessormethod变量“sTest1”应该是private的，并且有调用它的方法23 Variable “ABC” must match pattern“^[a-z][a-zA-Z0-9]$”变量“ABC”不符合命名规则“^[a-z][a-zA-Z0-9]$”24 “(” is followed by whitespace“(” 后面不能有空格 25“)” is proceededby whitespace“)” 前面不能有空格]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>软件架构</tag>
        <tag>插件</tag>
        <tag>eclipse</tag>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目自动构建工具--maven和Gradle]]></title>
    <url>%2F2018%2F03%2F22%2Fmaven01%2F</url>
    <content type="text"><![CDATA[主要区别Gradle和Maven都是项目自动构建工具，编译源代码只是整个过程的一个方面，更重要的是，你要把你的软件发布到生产环境中来产生商业价值，所以，你要运行测试，构建分布、分析代码质量、甚至为不同目标环境提供不同版本，然后部署。整个过程进行自动化操作是很有必要的。整个过程可以分成以下几个步骤： 编译源代码 运行单元测试和集成测试 执行静态代码分析、生成分析报告 创建发布版本 部署到目标环境 部署传递过程 执行冒烟测试和自动功能测试 如果你手工去执行每一个步骤无疑效率比较低而且容易出错，有了自动化构建你只需要自定义你的构建逻辑，剩下的事情交给工具去完成。虽然两者都是项目工具，但是maven现在已经是行业标准，Gradle是后起之秀，很多人对他的了解都是从android studio中得到的，Gradle抛弃了Maven的基于XML的繁琐配置，众所周知XML的阅读体验比较差，对于机器来说虽然容易识别，但毕竟是由人去维护的。取而代之的是Gradle采用了领域特定语言Groovy的配置，大大简化了构建代码的行数，比如在Maven中你要引入一个依赖： &lt;properties&gt; &lt;kaptcha.version&gt;2.3&lt;/kaptcha.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.kaptcha&lt;/groupId&gt; &lt;artifactId&gt;kaptcha&lt;/artifactId&gt; &lt;version&gt;${kaptcha.version}&lt;/version&gt; &lt;classifier&gt;jdk15&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 然后我将其转换成Gradle脚本，结果是惊人的： dependencies { compile(‘org.springframework:spring-core:2.5.6’) compile(‘org.springframework:spring-beans:2.5.6’) compile(‘org.springframework:spring-context:2.5.6’) compile(‘com.google.code.kaptcha:kaptcha:2.3:jdk15’) testCompile(‘junit:junit:4.7’) }注意配置从原来的28行缩减至7行！这还不算我省略的一些父POM配置。依赖的groupId、artifactId、 version，scope甚至是classfier，一点都不少。较之于Maven或者Ant的XML配置脚本，Gradle使用的Grovvy脚本杀伤力太大了，爱美之心，人皆有之，相比于七旬老妇松松垮垮的皱纹，大家肯定都喜欢少女紧致的脸蛋，XML就是那老妇的皱纹。Gradle给我最大的有点是两点。其一是简洁，基于Groovy的紧凑脚本实在让人爱不释手，在表述意图方面也没有什么不清晰的地方。其二是灵活，各种在Maven中难以下手的事情，在Gradle就是小菜一碟，比如修改现有的构建生命周期，几行配置就完成了，同样的事情，在Maven中你必须编写一个插件，那对于一个刚入门的用户来说，没个一两天几乎是不可能完成的任务。 gradle基于groovy领域特定语言，而maven通过xml来配置，比较繁琐 Gradle和Maven作为自动构建工具，在项目的构建中有着广泛的应用。他们之间有各自的优缺点，这里我们讨论下他们在项目构建中的一些区别并进行比较。 首先简单介绍下Gradle和Maven。Maven主要服务于基于java平台的项目构建、依赖管理和项目信息管理。无论是小型的开源类库项目，还是大型的企业级应用；无论是传统的瀑布式开发还是流行的敏捷模式，Maven都能大显身手。Gradle是以groovy语言为基础，面向java应用为主，基于DSL语法的自动化构建工具。 虽然两种构建工具有着很多相似处，但是在依赖管理、构建生命周期、加载构建系统组件等许多方面两者有着许多区别。Maven使用XML来定义生成脚本，而 Gradle构建脚本是用Groovy。 用XML的优势在于它可以更方便地定义构建逻辑，但这是比较复杂的步骤。 用Groovy的好处是写起来比XML标签要简洁许多。 不过熟悉的XML的开发人员比groovy的多，并且复杂的逻辑必须由自己编写。类似于Maven的pom.xml文件，每个Gradle项目都需要有一个对应的build.gradle文件，该文件定义一些任务（task）来完成构建工作，当然，每个任务是可配置的，任务之间也可以依赖，用户亦能配置缺省任务。 加载构建系统的组件Maven中每个用于构建的组件（编译/jar等）都作为一个插件， 每个插件都有它自己的版本和依赖关系树。 Gradle的构建系统组件都是分散的。 Maven插件的优点是在于可以独立更新，无需整个系统更新。Gradle的模型的优点是编译需要核心组件以外的组件时才下载。与此同时Gradle给了用户足够的自由去定义自己的任务，Gradle每个任务都有一个描述,可以分配到一个组。Maven中插件和命令可以描述。比如Gradle你可以排除任何运行的任务。在Maven中没有通用的排除机制，必须用插件来实现它。而且Gradle具有高级任务排序的特性，任务之间的依赖关系被建立之后能够得到完全控制，因为Gradle具有强大的语言结构来描述任务之间的执行顺序，即使任务并不取决于对方的输出。Gradle支持动态任务创建，有时你想要一个任务的行为取决于或无限价值的大范围的参数。一个很好的表达方式提供这样的任务是任务规则。并且执行任务时，Gradle 在遇到第一次失败时不停止，执行每一个要执行的任务其中所有的任务依赖关系都要被完成且没有失败。任务可以被分配去完成其他任务类似于java中的终结原则。他们总是在另一个任务执行之后运行，不管这个任务是否失败了。可以发现在一个单一的执行中许多失败任务会被很好地记录成一个错误报告并最终被汇总。 构建生命周期Maven提供有限的构建生命周期访问，插件可以连接到生命周期的特定阶段，而且只有在核心插件执行。而Gradle可以访问生成的一部分并允许用Groovy代码进行处理。Gradle Java Plugin也定义了构建生命周期，包括编译主代码、处理资源、编译测试代码、执行测试、上传归档等等任务. 相对于Maven完全线性的生命周期，Gradle的构建生命周期略微复杂，不过也更为灵活，例如jar这个任务是用来打包的，它不像Maven那样依赖于执行测试的test任务，类似的，从图中可以看到，一个最终的build任务也没有依赖于uploadArchives任务。这个生命周期并没有将用户限制得很死，由于Gradle完全是基于灵活的任务模型，因此很多事情包括覆盖现有任务，跳过任务都非常易于实现。而这些事情，在Maven的世界中，实现起来就比较的麻烦，或者说Maven就不希望用户这么做。 除了以上几个Maven核心内容与Gradle的区别，在面向对象输出模式，GUI操作界面、声明元素等方面Gradle也有良好表现。构建输出是构建用户体验的重要部分。在其他大多数构建工具中默认输出对于一个构建作者试图调试一个问题来说是有关联的。这通常会导致一个非常详细的输出会隐藏重要的警告和消息实际上是相关的开发人员运行构建。Gradle的默认输出是针对开发人员运行构建和只显示消息相关的情况下而不是滥用日志输出作为一种进度，例如在执行测试的时候。构建输出为构建用户体验是非常重要的。如果你与外部工具和库集成他们的控制台输出可能非常冗长。Gradle系统中你可以定义每个外部工具结合的日志级别的输出应该被路由。Gradle提供GUI操作界面，这是一个独立的用户界面，可以启动GUI选项，通过自定义日志模式你可以替换它的日志与自己的UI。Gradle有许多细粒度的声明性元素,如SourceSets或Android Product Flavors。它们的核心Gradle DSL然后让Gradle构建语言更加丰富。他们不断构建简洁、易于使用、维护和理解即使你有复杂的需求。Maven没有细粒声明元素，这是Maven极端顽固的主要原因。在Gradle,每个插件都可以提供自己的粗或细粒声明元素。这使你可以提供一个声明性方法甚至定制域。它还允许其他技术集成在Gradle中,让它被更多人使用。 整体来讲，Gradle给人一种简洁灵活的体验，然而必须掌握groovy也是他的问题，而且由于其灵活性，导致人们更容易破坏约定以至于让构建变得难以理解。但是Gradle确实是Maven理念的优秀实现。如果足够了解Groovy，也理解Maven的配置和构建，Gradle会是绝佳选择，尤其是它几乎能和现有的Maven系统无缝集成，而且你也能享受到简洁带来的极大乐趣，相信Gradle作为后起之秀在今后能够被完善的更好。 关于groovy语言Groovy 是 用于Java虚拟机的一种敏捷的动态语言，它是一种成熟的面向对象编程语言，既可以用于面向对象编程，又可以用作纯粹的脚本语言。使用该种语言不必编写过多的代码，同时又具有闭包和动态语言中的其他特性。 Groovy 是一个基于 JVM 的语言，代码最终编译成字节码（bytecode）,并在 JVM 上运行。它具有类似于 Java 的语法风格，但是语法又比 Java 要灵活和方便，同时具有动态语言（如 ruby 和 Python）的一些特性。 Groovy是JVM的一个替代语言（替代是指可以用 Groovy 在Java平台上进行 Java 编程），使用方式基本与使用 Java代码的方式相同，该语言特别适合与Spring的动态语言支持一起使用，设计时充分考虑了Java集成，这使 Groovy 与 Java 代码的互操作很容易。（注意：不是指Groovy替代java，而是指Groovy和java很好的结合编程。 Groovy 语法与Java 语言的语法很相似，虽然 Groovy 的语法源于Smalltalk和Ruby这类语言的理念，但是可以将它想像成 Java 语言的一种更加简单、表达能力更强的变体。（在这点上，Ruby与 Groovy 不同，因为它的语法与 Java 语法差异很大。） 许多 Java 开发人员喜欢 Groovy 代码和 Java 代码的相似性。从学习的角度看，如果知道如何编写 Java 代码，那就已经了解 Groovy 了。Groovy 和 Java 语言的主要区别是：完成同样的任务所需的 Groovy 代码比 Java 代码更少。 Groovy教程参考： https://www.w3cschool.cn/groovy/ 在现在市场上流行的微服务架构都建议采用Gradle来构建项目而非Maven spring 项目建议使用Gradle进行构建项目，相比maven来讲 Gradle更简洁，而且gradle更时候大型复杂项目的构建。gradle吸收了maven和ant的特点而来，不过目前maven仍然是Java界的主流，大家可以先了解了解。 一个使用gradle配置的项目（在开发人员的角度去看这个会发现比maven构建项目的结构会更加的清晰明了） buildscript { repositories { maven { url &quot;http://repo.spring.io/libs-snapshot&quot; } mavenLocal() } dependencies { classpath(&quot;org.springframework.boot:spring-boot-gradle-plugin:1.3.6.RELEASE&quot;) } } apply plugin: &apos;java&apos; //添加 Java 插件, 表明这是一个 Java 项目 apply plugin: &apos;spring-boot&apos; //添加 Spring-boot支持 apply plugin: &apos;war&apos; //添加 War 插件, 可以导出 War 包 apply plugin: &apos;eclipse&apos; //添加 Eclipse 插件, 添加 Eclipse IDE 支持, Intellij Idea 为 &quot;idea&quot; war { baseName = &apos;favorites&apos; version = &apos;0.1.0&apos; } sourceCompatibility = 1.7 //最低兼容版本 JDK1.7 targetCompatibility = 1.7 //目标兼容版本 JDK1.7 repositories { // Maven 仓库 mavenLocal() //使用本地仓库 mavenCentral() //使用中央仓库 maven { url &quot;http://repo.spring.io/libs-snapshot&quot; } //使用远程仓库 } dependencies { // 各种 依赖的jar包 compile(&quot;org.springframework.boot:spring-boot-starter-web:1.3.6.RELEASE&quot;) compile(&quot;org.springframework.boot:spring-boot-starter-thymeleaf:1.3.6.RELEASE&quot;) compile(&quot;org.springframework.boot:spring-boot-starter-data-jpa:1.3.6.RELEASE&quot;) compile group: &apos;mysql&apos;, name: &apos;mysql-connector-java&apos;, version: &apos;5.1.6&apos; compile group: &apos;org.apache.commons&apos;, name: &apos;commons-lang3&apos;, version: &apos;3.4&apos; compile(&quot;org.springframework.boot:spring-boot-devtools:1.3.6.RELEASE&quot;) compile(&quot;org.springframework.boot:spring-boot-starter-test:1.3.6.RELEASE&quot;) compile &apos;org.webjars.bower:bootstrap:3.3.6&apos; compile &apos;org.webjars.bower:jquery:2.2.4&apos; compile(&quot;org.webjars:vue:1.0.24&quot;) compile &apos;org.webjars.bower:vue-resource:0.7.0&apos; } bootRun { addResources = true } 如何构建eclipse的groovy插件版本集合根据Groovy在github上的开源：https://github.com/groovy/groovy-eclipse/wiki 我们若是想要构建Groovy工程和文件，必须要导入groovy插件 Groovy-Eclipse provides Eclipse and Maven support for the Apache Groovy programming language. Releases The latest Groovy-Eclipse release is available from the following Eclipse update sites. To install, point your Eclipse update manager to the update site appropriate for your Eclipse version. Eclipse Level Release Update Site4.7 (Oxygen) http://dist.springsource.org/release/GRECLIPSE/e4.74.6 (Neon) http://dist.springsource.org/release/GRECLIPSE/e4.64.5 (Mars) http://dist.springsource.org/release/GRECLIPSE/e4.54.4 (Luna) http://dist.springsource.org/release/GRECLIPSE/e4.44.3 (Kepler) http://dist.springsource.org/release/GRECLIPSE/e4.34.3j8 (Kepler with Java 8) http://dist.springsource.org/release/GRECLIPSE/e4.3-j84.2 and 3.8 (Juno) http://dist.springsource.org/release/GRECLIPSE/e4.23.7 (Indigo) http://dist.springsource.org/release/GRECLIPSE/e3.7 根据你的Eclipse版本来选择对应的Groovy版本，如何查看自己的Eclipse版本，help–》about Eclipse 第一步：help–》install new SoftWare第二步：在avaliable software选择add–》填写name和URL，name随意，URL要写成对应的版本URL第三步：pending。。（加载）以后全部勾选，然后默认完成即可第四步：重启机器即可创建Groovy工程 mavenmaven是目前java项目中比较流行的构建工具，特别是它提供的插件，如果使用得当，整个项目研发流程都将会受益，从而提高研发、测试和部署的效率。 Maven工程标准架构 目录 备注 ${basedir} 存放 pom.xml和所有的子目录${basedir}/src/main/resources 项目的资源，如spring配置文件，properties资源文件等${basedir}/src/main/webapps web项目特有${basedir}/src/test/java 项目的测试类，比如说 JUnit代码、TestNg代码${basedir}/src/test/resources 测试代码使用的资源]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>软件架构</tag>
        <tag>构建工具</tag>
        <tag>maven</tag>
        <tag>Gradle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构--Dubbo还是Spring Cloud？]]></title>
    <url>%2F2018%2F03%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F02%2F</url>
    <content type="text"><![CDATA[微服务以及SOA我们在上一篇《微服务架构简析》中已经谈及了，接下来我们来谈谈市面上流行的微服务架构产品有哪些? 微服务架构是互联网很热门的话题，是互联网技术发展的必然结果。它提倡将单一应用程序划分成一组小的服务，服务之间互相协调、互相配合，为用户提供最终价值。 虽然微服务架构没有公认的技术标准和规范或者草案，但业界已经有一些很有影响力的开源微服务架构框架提供了微服务的关键思路，例如 Dubbo 和 Spring Cloud。 DubboDubbo 是阿里巴巴公司一个开源的高性能服务框架，致力于提供高性能和透明化的 RPC 远程服务调用方案，以及 SOA 服务治理方案，使得应用可通过高性能 RPC 实现服务的输出、输入功能和 Spring 框架无缝集成。(淘宝为了应付双11的高流量并发请求所带来的服务器压力而开发的产品)。 Dubbo 包含远程通讯、集群容错和自动发现三个核心部分。它提供透明化的远程方法调用，实现像调用本地方法一样调用远程方法，只需简单配置，没有任何 API 侵入。 同时它具备软负载均衡及容错机制，可在内网替代 F5 等硬件负载均衡器，降低成本，减少单点。 它可以实现服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的 IP 地址，并且能够平滑添加或删除服务提供者。 2011 年末，阿里巴巴在 GitHub 上开源了基于 Java 的分布式服务治理框架 Dubbo，之后它成为了国内该类开源项目的佼佼者，许多开发者对其表示青睐。 同时，先后有不少公司在实践中基于 Dubbo 进行分布式系统架构。目前在 GitHub 上，它的 fork、star 数均已破万。 Dubbo 核心部件（如下图）: Provider：暴露服务的提供方，可以通过 jar 或者容器的方式启动服务。 Consumer：调用远程服务的服务消费方。 Registry：服务注册中心和发现中心。 Monitor：统计服务和调用次数，调用时间监控中心。（Dubbo 的控制台页面中可以显示，目前只有一个简单版本。） Container：服务运行的容器。 Dubbo总体架构 Dubbo 核心功能： 1.远程通讯，提供对多种基于长连接的 NIO 框架抽象封装，包括多种线程模型，序列化，以及“请求-响应”模式的信息交换方式。2.集群容错，提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。3.自动发现，基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器。 关于Dubbo的历史可以到这篇知乎文章中去看，人家已经整理了Dubbo的发展历史。https://zhuanlan.zhihu.com/p/31206177 Spring Cloudspring cloud 为开发人员提供了快速构建分布式系统的一些工具，包括配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等。它运行环境简单，可以在开发人员的电脑上跑。另外说明spring cloud是基于springboot的，所以需要开发中对springboot有一定的了解。 Spring Cloud的子项目，大致可分成两类，一类是对现有成熟框架”Spring Boot化”的封装和抽象，也是数量最多的项目；第二类是开发了一部分分布式系统的基础设施的实现，如Spring Cloud Stream扮演的就是kafka, ActiveMQ这样的角色。对于我们想快速实践微服务的开发者来说，第一类子项目就已经足够使用，如： Spring Cloud Netflix 是对Netflix开发的一套分布式服务框架的封装，包括服务的发现和注册，负载均衡、断路器、REST客户端、请求路由等。 Spring Cloud Config 将配置信息中央化保存, 配置Spring Cloud Bus可以实现动态修改配置文件 Spring Cloud Bus 分布式消息队列，是对Kafka, MQ的封装 Spring Cloud Security 对Spring Security的封装，并能配合Netflix使用 Spring Cloud Zookeeper 对Zookeeper的封装，使之能配置其它Spring Cloud的子项目使用 Spring Cloud Eureka Spring Cloud Eureka 是 Spring Cloud Netflix 微服务套件中的一部分，它基于Netflix Eureka 做了二次封装，主要负责完成微服务架构中的服务治理功能。 Spring Cloud对于中小型互联网公司来说是一种福音，因为这类公司往往没有实力或者没有足够的资金投入去开发自己的分布式系统基础设施，使用Spring Cloud一站式解决方案能在从容应对业务发展的同时大大减少开发成本。同时，随着近几年微服务架构和Docker容器概念的火爆，也会让Spring Cloud在未来越来越“云”化的软件开发风格中立有一席之地，尤其是在目前五花八门的分布式解决方案中提供了标准化的、全站式的技术方案，意义可能会堪比当前Servlet规范的诞生，有效推进服务端软件系统技术水平的进步。 Spring Cloud官方网站：https://projects.spring.io/spring-cloud/ （包括英文DOC）Spring Cloud中文文档地址：https://springcloud.cc/ 学习资源整理： Spring Cloud基础教程：http://blog.didispace.com/Spring-Cloud%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/ 对应Github项目：https://github.com/dyc87112/SpringCloud-Learning方志朋的专栏 Spring Cloud：http://blog.csdn.net/forezp/article/details/70148833 Spring Boot基础教程：http://blog.didispace.com/Spring-Boot%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/ 对应Github项目：https://github.com/dyc87112/SpringBoot-Learning 对应码云：https://gitee.com/didispace/SpringBoot-Learning方志朋的专栏 Spring Boot：http://blog.csdn.net/forezp/article/details/61472783]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>架构</tag>
        <tag>Dubbo</tag>
        <tag>SpringCloud</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主流的Web服务实现方案--REST与SOAP和XML-RPC相比有哪些优势？]]></title>
    <url>%2F2018%2F03%2F20%2Fweb%E9%A3%8E%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[关于RESTful具象状态传输（REST，英文：Representational State Transfer）是Roy Thomas Fielding博士于2000年在他的博士论文中提出来的一种万维网软件架构风格，目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。 目前在三种主流的Web服务实现方案中，因为REST模式与复杂的SOAP和XML-RPC相比更加简洁，越来越多的web服务开始采用REST风格设计和实现。例如，Amazon.com提供接近REST风格的Web服务运行图书查询；雅虎提供的Web服务也是REST风格的。 具象状态传输是设计风格而不是标准。REST通常基于使用HTTP，URI，和XML以及HTML这些现有的广泛流行的协议和标准。一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。 1.资源是由URI来指定。2.对资源的操作包括获取、创建、修改和删除资源，这些操作正好对应HTTP协议提供的GET、POST、PUT和DELETE方法。3.通过操作资源的表现形式来操作资源。4.资源的表现形式则是XML或者HTML，取决于读者是机器还是人，是消费web服务的客户软件还是web浏览器。当然也可以是任何其他的格式。 RESTful特征 统一接口(Uniform Interface)]]></content>
      <categories>
        <category>WEB</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>软件架构</tag>
        <tag>REST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构简析]]></title>
    <url>%2F2018%2F03%2F20%2F%E5%88%86%E5%B8%83%E5%BC%8F01%2F</url>
    <content type="text"><![CDATA[微服务是什么？谈及微服务架构，2016年应该是软件业微服务爆发的元年，微服务到底是什么？观察现有的软件业，使用单独一款开发语言区开发一款较为全面的产品很少，大多数都是各种语言编写的功能模块进行组装成完整的业务模型，各语言效率优势互补。微服务架构所强调的特点就是业务系统需要彻底的组件化与模块分割，原有的单个业务系统分割成多个小应用，每个小应用可以进行独立开发，管理，运行和测试，甚至连每个产品的前端（UI加结构），服务端（控制层，逻辑层和持久化层），数据库都是完全独立的产品，通过微服务架构将每个小应用进行整合，交互与集成，每个小应用不仅能够自己完全独立的功能，还可以运行其他小应用的服务，同时也将自己作为可供利用的服务对象。 采用一组服务的方式来构建一个应用，服务独立部署在不同的进程中，不同服务通过一些轻量级交互机制来通信，例如 RPC、HTTP 等，服务可独立扩展伸缩，每个服务定义了明确的边界，不同的服务甚至可以采用不同的编程语言来实现，由独立的团队来维护。简单的来说，一个系统的不同模块转变成不同的服务！而且服务可以使用不同的技术加以实现。 有了微服务，彻底的将耦合性再次的降低，彼此之间可以高效解耦。 关于微服务到底是个啥，我这里引用了一个知乎的回答，很有意思：作者：dz902链接：https://www.zhihu.com/question/37808426/answer/195479692来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 很久以前的一天，Martin 在跟好友的交流中悟到了一种很棒的架构设计。他总结了一下，然后告诉了好友，好友说，这不是新鲜东西，早有人总结了，叫做 SOA。Martin 很高兴，开始在公司内外推广 SOA。结果，不停有人质疑和挑战他。 你这不是 SOA 吧？SOA 这里应该是如此这般的。对，这里我对 SOA 的理解是这样的。你看，这本 SOA 的书说的和你说的有出入。粒度？SOA 没有谈到这个呀，你这不是 SOA。分层跟 SOA 没有关系，你为什么要说这个呢？ …Martin 没办法，心一横，老子就叫它 Martin’s SOA。老子发明的词，老子就是最高权威，有最终解释权。还有谁不服？同事们一看，这思想本身很不错，值得推广，但叫 Martin’s SOA 太没品了吧？还是要取个好一点的名字，最好还能跟 SOA 有某种暗示传承。干脆就叫 Microservices 好了，又新，又有服务含在其中。 Martin 忿忿地接受了这个建议，心里想着：妈的，明明就是 SOA，一群傻逼非要逼我取个新名字。后来 Martin 发现每次提一个东西就会收到旧恶傻势力对解释权的挑战，所以每次要提一个什么概念，都去发明一个新词，免得一群人在那一边质疑挑战，一边大谈“我的理解”。 这就是微服务、敏捷、精益企业、持续集成、持续交付背后的故事。一个名词产生之后，命名者的解释权就会随着时间而弱化（比如 Cooper 发明的 Persona 就被无数设计师乱用）。敏捷已经有点烂了，等微服务也烂了，我们还会发明新词。实在没辙，都是被逼的啊。 微服务有以下几个特征： 通过服务实现组件化； 按业务能力来划分服务与组织团队； 服务即产品； 智能终端与哑管道； 去中心统一化； 基础设施自动化； Design for failure； 进化设计 上面讲到微服务与SOA的宫斗戏码，又来讲讲什么是SOA？ SOA架构SOA是什么？SOA全英文是Service-Oriented Architecture，中文意思是中文面向服务编程，是一种思想，一种方法论，一种分布式的服务架构 可以肯定的是SOA和微服务的确是一脉相承的，大神Martin Fowler提出来这一概念可以说把SOA的理念继续升华，精进了一步。其核心思想是在应用开发领域，使用一系列微小服务来实现单个应用的方式途径，或者说微服务的目的是有效的拆分应用，实现敏捷开发和部署 ，可以是使用不同的编程语言编写。而SOA可能包含的意义更泛一些，更不准确一些。 用途：SOA解决多服务凌乱问题，SOA架构解决数据服务的复杂程度，同时SOA又有一个名字，叫做服务治理。 架构由中心式转为分布式的演变过程 这里给一个电商服务由传统架构改向分布式架构的流程图 当我们的项目比较小时，我们只有一个系统，并且把他们写到一起，放在一个服务器上，但是随着平台越来越大，数据量越来越大，我们不得不通过分库，把多个模块的数据库分别放在对应得服务器上，每个模块调用自己的子系统即可。 随着我们系统的进一步复杂度的提示，我们不得不进一步对系统的性能进行提升，我们将多个模块分成多个子系统，多个子系统直接互相调用（因为SOA一般用于大型项目，比较复杂，所以一般总系统不会再集成，会拆分多个，分别做成服务，相互调用）。当我们的电商UI进行一个下订单的任务时，多个服务直接互相调用，系统通过数据总线，分别调用对应的子系统即可。 企业数据总线(ESB)：企业数据总线不是对多个子模块的集成，他在这里充当数据通道的作用，数据总线不关心业务，数据总线根据给的地址和协议去调服务，上端不关心服务在哪里是什么，只找数据总线。 上面几个图应该算是比较清楚了，随着业务的深入，我们不得不对系统进行调整，分别是对数据和业务的拆分，最后每个子系统对面提供服务。 还要提的一点就是下面那个图，下面的IP库以及几个子系统是公共服务，分别向上提供功能，也是SOA方法论的一部分。 通过上面的图我们可以看出，多个子系统直接相互交互，相互调用非常凌乱，这样我们就很不爽，所以我们就用到了我们的SOA架构，SOA又叫服务治理，SOA就是帮助我们把服务之间调用的乱七八糟的关系给治理起来，然后提供一个统一的标准，把我们的服务治理成下图所示，以前我们的服务是互相交互，现在是只对数据总线进行交互，这样系统就变得统一起来。 统一标准：各系统的协议、地址、交互方式。 新的交互方式：各个系统分别根据统一标准向数据总线进行注册，各子系统调用其他子系统时，我们并不关心如果找到其他子系统，我们只找数据总线，数据总线再根据统一标准找其他子系统，所以数据总线在这里充当一个只路人的作用。 首先Martin Fowler提出SOA歧义Service Oriented Ambiguity，认为”什么是SOA”是不可能回答，因为不同的人意味着不同的事情，SOA意味服务接口，意味流程整合，意味资源再利用，意味着管制，在下面SOA组件图中，服务和服务消费者(客户端)之间存在多个约束，当一个服务显式暴露后，客户端能够通过绑定定位到该服务，相当于两者签订了合同，规定了合同内容和如何实施，具体合同的描述是通过消息方式进行 由于Java等传统语言主要是以类表达对象，将功能行为封装在类内部，而业务客户一般都是注重软件的功能，包括同行业公司系统之间数据交流也是以功能服务为接口，因此，面向服务的架构SOA更加贴近业务客户，也更适合业务伙伴之间流程整合，各个行业已经诞生自己行业特点的SOA，例如电信联盟的NGOSS，已经成为电信行业业务支撑系统BOSS的标准。SOA不但是技术用语，也是业务销售用语，通过服务这个中间概念，可以实现业务和技术之间的无缝转换，如今SOA已经和REST DDD以及云计算等新技术方法结合。其内部主要概念有SCA ESB JBI等等，涉及工作流 规则引擎 消息总线等多个技术细节方面。 通常，一个架构师进行系统架构顶层设计时，必须考虑使用者的利益，不能单单实现软件的功能，还要考虑到软件的性能Scalable 可用性available/usable 安全性等软件质量，还要借鉴社区的最佳实践和经验形成的模式和反模式，避免重蹈覆辙和陷阱，再大胆采取最新的软件技术(比如用REST替代SOAP等)。 服务的提出其实隐含了两个概念，服务提供者和服务消费者，这两者之间有一个合同约定，这非常类似我们现实生活中签订的服务合同，A单位和B单位分别是服务的提供者和消费者，两者签订了一个服务合同，规定A为B提供某项服务。服务就是提供一些公共需求的设施，通过一个工作过程能提供帮助，使用，让使用者受益。 服务具体有如下：Windows Service:如PC定位者RPC Locator, 事件日志EventLog, DHCP Client,；. 软件服务Software Service，如分布式服务Distribution Service, 警告服务Alert Service 安全服务 Security Service, 日志服务；业务服务Business Service，如 帐号和客户服务，销售服务，订单服务，采购服务。 服务两个重要特点：自治和管制，自治代表服务不能被外部势力牵制，比如如果一个服务内部处理中需要调用外部资源或等待外部流程结束，这种等待不能影响服务本身的调用，如果一个服务分为显式对外和隐式内部两个部分，那么自治是针对隐式内部，意味着我们不能在具体一个服务中直接使用同步代码实现复杂功能。如： public AServiceImpl implements AService{ public void productSale(...){ Product product = productService.getProduct(); int inventory = InventoryService.getInventory(product); int price = priceService.getPrice(product); } } 在AServiceImpl的productSale方法中，我们获得商品的库存和定价，都是通过同步的RPC实现调用的，这样造成productSale方法依赖于InventoryService和priceService，无法实现自身自治。 实现服务真正自治，实际就是解决类之间依赖耦合的问题，消息是一种方式，但是基于消息又有两种通讯方式，基于请求响应和基于事件的EDA。 从服务自治可以看出，为什么要提出服务必须自治，因为服务是受管制的，在实际业务活动中，不同服务是被不同部分管理，比如定价服务归属财务部门系统，库存归属仓库系统，涉及系统之间调用协调不能自己使用同步RPC，而是需要消息。 在实际应用中，很多单位使用SOA主要看中其能够无缝整合新旧系统,称为EAI企业应用整合，下图是苏宁的一种SOA图，使用ESB企业服务总线这样的消息系统整合了新旧各种系统。 ESBESB全称为Enterprise Service Bus，即企业服务总线。它是传统中间件技术与XML、Web服务等技术结合的产物。ESB提供了网络中最基本的连接中枢，是构筑企业神经系统的必要元素。ESB的出现改变了传统的软件架构，可以提供比传统中间件产品更为廉价的解决方案，同时它还可以消除不同应用之间的技术差异，让不同的应用服务器协调运作，实现了不同服务之间的通信与整合。从功能上看，ESB提供了事件驱动和文档导向的处理模式，以及分布式的运行管理机制，它支持基于内容的路由和过滤，具备了复杂数据的传输能力，并可以提供一系列的标准接口。 数据总线是起到调度服务的作用，数据总线不是集成服务，数据总线更新一个调度框架，每个服务需要根据约定向数据总线注册服务，那么如何注册那？其实数据总线就像一个字典结构， 数据总线里面一个key对于一个value，key指的是服务名，value则是服务的调度方式，还有一点需要说明的是，数据总线只是指路人，服务是不经过数据总线的，如上图的黄色线的路径。 企业服务总线（EnterpriseServiceBus，ESB）从面向服务体系架构（Service-OrientedArchitecture，SOA）发展而来，是传统中间件技术与XML、Web服务等技术结合的产物。ESB提供了网络中最基本的连接中枢，是构筑企业神经系统的必要元素。ESB采用了“总线”这样一种模式来管理和简化应用之间的集成拓扑结构，以广为接受的开放标准为基础来支持应用之间在消息、事件和服务级别上动态的互连互通，是一种在松散耦合的服务和应用之间标准的集成方式。它可以作用于：①面向服务的架构—分布式的应用由可重用的服务组成；②面向消息的架构—应用之间通过ESB发送和接受消息；③事件驱动的架构—应用之间异步地产生和接收消息。ESB的出现改变了传统的软件架构，可以提供比传统中间件产品更为低廉的解决方案，同时它还可以消除不同应用之间的技术差异，让不同的应用服务器协调运作，实现了不同服务之间的通信与整合。从功能上看，ESB提供了事件驱动和文档导向的处理模式，以及分布式的运行管理机制，它支持基于内容的路由和过滤，具备了复杂数据的传输能力，并可以提供一系列的标准接口。 ESB 是传统中间件技术与XML、Web服务等技术相互结合的产物，ESB的出现改变了传统的软件架构，可以提供比传统中间件产品更为廉价的解决方案，同时它还可以消除不同应用之间的技术差异，让不同的应用服务器协调运作，实现了不同服务之间的通信与整合。从功能上看，ESB提供了事件驱动和文档导向的处理模式，以及分布式的运行管理机制，它支持基于内容的路由和过滤，具备了复杂数据的传输能力，并可以提供一系列的标准接口。 大规模分布式的企业应用需要相对简单而实用的中间件技术来简化和统一越来越复杂、繁琐的企业级信息系统平台。面向服务体系架构（SOA）是能够将应用程序的不同功能单元通过服务之间定义良好的接口和契约联系起来。SOA使用户可以不受限制地重复使用软件、把各种资源互连起来，只要IT人员选用标准接口包装旧的应用程序、把新的应用程序构建成服务，那么其他应用系统就可以很方便的使用这些功能服务。支撑SOA的关键是其消息传递架构-企业服务总线（ESB）。ESB是传统中间件技术与XML、Web服务等技术相互结合的产物，用于实现企业应用不同消息和信息的准确、高效和安全传递。让不同的应用服务协调运作，实现不同服务之间的通信与整合。ESB在不同领域具有非常广泛的用途: 电信领域：ESB能够在全方位支持电信行业OSS的应用整合概念。是理想的电信级应用软件承载平台。电力领域：ESB能够在全方位支持电力行业EMS的数据整合概念，是理想的SCADA系统数据交换平台。金融领域：ESB能够在全方位支持银企间业务处理平台的流程整合概念，是理想的B2B交易支撑平台。电子政务：ESB能够在全方位支持电子政务应用软件业务基础平台、信息共享交换平台、决策分析支撑平台和政务门户的平台化实现。 使用SOA和ESB能够灵活实现业务流程管理，工作流的管理BPM，如下图，一个订单的产生可能需要几个部门批准才能完成，而且这几个部门经常是变化的，如何灵活实现这种批准流程的定制也成为SOA实现的一部分，如下： 注意图中1 2 3 4 5 6 7 8 9标注的订单处理流程步骤，这种不同服务之间调用处理顺序可通过BPM进行灵活定制。 目前提供SOA全套解决方案和产品的厂商很多，包括IBM SAP和Oracle，国内金蝶用友浪潮软件等等，比如苏宁的SOA是以SAP为主的八国联军组装，既然SOA中间件服务商已经为我们提供了成熟的架构方案和产品，那么作为SOA使用者是否就无需顶层架构设计了呢？当然不是，SOA使用者要根据自己业务进行模块划分，进行领域建模设计，根据DDD领域驱动设计将业务分解为一个上下文模块，然后再用服务作为对外接口，内部封装的是DDD聚合根，而传统SOA作法是内部封装的是数据表的DTO，从而导致SOA服务内部腐烂堵塞，违背SOA自治和可用性等原则约束。具体可见DDD领域驱动设计。 SOA的好处1.松耦合：由于服务自治，有一定封装边界，服务调用交互是通过发布接口。这意味着应用程序不感兴趣的服务如何被实现。2.位置透明：服务的消费者不必关系服务位于什么地方。3.可在异构平台间复用。可以将遗留系统包装成服务。4.便于测试，能并行开发，较高可靠性和良好可伸缩性。5.降低用户成本，用户不需要关心各服务之间是什么语言的、不需要知道如果调用他们，只要通过统一标准找数据总线就可以了。6.程序之间关系服务简单7.识别哪些程序有问题（挂掉） 缺点：提示了系统的复杂程度，性能有相应影响。 从实现方式上，两者都是中立性，语言无关，协议跨平台，相比SOA，微服务框架将能够带来更大的敏捷性，并为你构建应用提供更轻量级、更高效率的开发。而SOA更适合大型企业中的业务过程编排、应用集成。 另外还有微服务甚至是去ESB、去中心化、分布式的，而SOA还是以ESB为核心，大量的WS标准实现。再次，从服务粒度上，既然是微，必然微服务更倡导服务的细粒度，重用组合，甚至是每个操作（或方法）都是独立开发的服务，足够小到不能再进行拆分。而SOA没有这么极致的要求，只需要接口契约的规范化，内部实现可以更粗粒度，微服务更多为了可扩充性、负载均衡以及提高吞吐量而去分解应用，但同时也引发了打破数据模型以及维护一致性的问题。 微服务相比于SOA粒度更细。 最后，从部署方式上，这个是最大的不同，对比Monolithic（有人翻译为单体）的Java EE部署架构，通过展现层打包WARs，业务层划分到JARs最后部署为EAR一个大包，而微服务则打开了这个黑盒子，把应用拆分成为一个一个的单个服务，应用Docker技术，不依赖任何服务器和数据模型，是一个 全栈应用，可以通过自动化方式独立部署，每个服务运行在自己的进程中，通过轻量的通讯机制联系，经常是基于HTTP资源API，这些服务基于业务能力构建，能实现集中化管理（因为服务太多啦，不集中管理就无法DevOps啦）。 引用知乎比喻： 以一个公司为例：有基层员工 有管理层 有老板，最初大家都听老板指挥，谁干什么谁干什么，根据需要，你可能今天干A事情，明天干B事情，后来人越来越多了，事情也越来越多了，做事情的效率越来越多，管理也很混乱，就开始做部门划分（服务化），专门部门做专门事情的，IT部门只做研发，人事部门只做招聘； 这个时候就无法避免的发生跨部门协作（服务器调用）， 但是你怎么知道有这样一个部门可以做这个事情呢，就要依赖行政部门（注册中心），新成立的部门要在行政哪里做一个备案（服务注册），然后公布一下，让其他部门知道了（服务发布），大家就可以在新的工作秩序里面嗨皮的上班了，这个时候依然是在公司的组织架构中运转； 上述就是我理解的SOA的概念微服务没有具体的实施过，通过自己的一些理解尝试解释一下，勿喷！微服务有一定SOA的概念在里面，只是在粒度中，微服务更加细一点，比如说用户业务服务：登录 注册 个人中心 包含3个业务，都有userService 提供的，但是在微服务中，登录会被独立出来一个服务，注册也会被独立出来，相对SOA的粒度更细，业务场景耦合更低；另外微服务强调一个去中心化，上述的公司的组织架构会被打散，没有老板，没有管理层，每一个人都是一个服务，做着自己的事情，虽然没有完全想明白，把自己的理解放出来，大家可以探讨一下。 微服务是SOA的一种实现，也可以说微服务是去ESB的SOA 背后实际上是两种思想的分歧：分布还是集中 当然这里说的不是服务的分布和集中。服务肯定是分布的，这是大前提，是SOA的本质理念之一。分歧在于对服务的治理，是分布还是集中。 微服务所提倡的是完完全全的分布式服务。 微服务架构特征（Characteristics）微服务架构是 Martin Fowler 和 James Lewis 定义的一种架构风格。他们将这种风格描述为”一种使用小型服务构建系统的架构方法，每个服务都在自己的进程中，它们通过轻量型协议进行通信”。 每个服务都是相互独立开发和部署的。每个微服务都专注执行一个它所擅长的相对较小的任务。 微服务架构是产品或服务所有者跟上或超越 IT 行业的快速发展节奏的推动因素之一。 小型且专注于业务领域小并不能充分描述微服务，使用这个词只是为了尝试表明微服务相对于整体式应用程序的大小。在此上下文中，小的定义可能在各个系统中各不相同，而且没有规则来定义服务必须有多小。我的理解是将传统整体式应用服务拆分为可独立运行互不干扰的最小组件，这个组件涵盖软件设计的全部细节，前段后端数据端，开发者必须熟悉小组件设计的全部设计环节，这岂不是以后每个人都必须是全栈式开发人员？ 微服务的另一个重要特征是，每个服务专注负责一项精细的业务。Vaughn Vernon 在他撰写的图书《实现领域驱动设计》中定义了术语业务领域。他将业务领域定义为，”某个组织执行的操作和它执行操作的环境。”他还指定，”每个组织都有自己独特的知识范围和操作方式。这个理解范围和它执行操作的方法就是它的领域。”被分解为微服务的单元实际上就是领域内的业务实体，这意味着每个微服务处理完成一个完整的业务任务。例如：Mortgage Services 是一个业务领域。Loan Origination 是该领域中一个可能的微服务。Loan Refinancing 可能是同一个领域中的另一个微服务。跨服务边界的调用和更改通常很耗资源，必须避免。拥有这些服务的团队成为相应业务领域或子领域的专家，而不是任意技术领域的专家。 微服务通常负责一个精细的工作单元，所以它在规模上相对较小。一条著名的指导原则是”两块披萨的团队规模”方案，意思是说，如果两块披萨不能将整个团队喂饱，那么这个开发微服务的团队可能太大了。微服务必须足够小，使得团队中的每个人都能理解该服务的整体设计和实现。另外，它的大小必须足以让团队在必要时轻松地维护或重写服务。 与技术中立开发微服务的团队必须使用他们熟悉的技术。不要规定开发团队应该使用何种编程语言。让开发人员自由选择对任务最有意义的技术和执行任务的人。这种工作方式能够充分利用团队成员拥有的最佳技术和技能。微服务架构仍需要技术决策；举例而言，使用具象状态传输 (REST) 应用编程接口 (API) 访问更好一些（即RESTful API），还是使用某种类型的排队来访问更好一些？但是，一般而言，您可以为微服务架构选择广泛范围内的任何技术。 具象状态传输 (REST)具象状态传输（REST，英文：Representational State Transfer）是Roy Thomas Fielding博士于2000年在他的博士论文中提出来的一种万维网软件架构风格，目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。目前在三种主流的Web服务实现方案中，因为REST模式与复杂的SOAP和XML-RPC相比更加简洁，越来越多的web服务开始采用REST风格设计和实现。例如，Amazon.com提供接近REST风格的Web服务运行图书查询；雅虎提供的Web服务也是REST风格的。这个后期会将互联网WEB架构单独开篇讲解。 松散耦合松散耦合对基于微服务的系统至关重要。每个微服务都必须采用使其与其他服务的关联很小的方式来设计接口。这样，在更改一个服务并部署它时，就无需更改和重新部署系统的其他部分。（完全可独立运行的最小服务组件） 为了避免服务之间的耦合，必须了解导致紧密耦合的原因。紧密耦合的一个原因是通过 API 公开服务的内部实现。这种公开方式将服务的使用者与它的内部实现绑定在一起，从而导致更高的耦合度。在这种情况下，如果更改微服务的内部架构，可能还需要更改服务的使用者，否则就会破坏使用者。这可能会增加更改的成本，给实现更改带来潜在隐患，进而增加服务中的技术债务。必须避免任何导致公开服务的内部实现的方法，以确保微服务之间松散耦合。 另一个错误是让服务的 API 太过精细。如果 API 太过精细，服务之间的调用可能变得太过频繁，也就是说，会通过网络执行更多的调用。除了前缀的性能问题，过度频繁的通信还可能造成紧密耦合。因此，设计服务接口的方式必须能够最大限度地减少网络中执行的来回调用。 必须避免一个服务内的实现过于分散，方法是将表示业务实体的相关属性、行为放在尽可能相近的地方。将相关属性放在一个微服务中；如果更改某个属性或行为，可以在一个位置更改它并快速部署该更改。否则，必须在不同部分中执行更改，然后同时一起部署这些散乱的部分；这会导致这些部门之间紧密耦合。 每个微服务必须有自己的源代码管理存储，以及用于构建和部署的交付管道。这样即可在必要时部署每个服务，而不需要与其他服务的所有者进行协调。如果您有两个服务，而且始终在一次部署中一起发布这两个服务，这可能表明两个服务最好合并为一个服务，而且必须对当前服务执行更多分解工作。松散耦合还支持更频繁、更快速的部署，最终提高应用程序对其用户需求的响应能力。 实现容易观察微服务架构要求您能够可视化系统中所有服务的健康状态，以及它们之间的连接。这使您能快速找到并响应可能发生的问题。实现可视化的工具包含一种全面的日志机制，能够记录日志，存储日志，并使日志容易搜索，以便执行更有效的分析。 向系统中配置和添加的新服务越多，让这些服务变得可观察就会越难。因为在添加更多动态部分时，微服务架构会增加复杂性，所以观察设计必须明确，使可视化的日志和监视数据能为分析提供有帮助的信息。 自动化自动化是有效设计和实现软件应用程序的一个重要要求。对于微服务，自动化是一个至关重要但又充满挑战的任务。除了需要在生产中运行系统之外，微服务架构还向系统的实现引入了更多复杂性。在处理的机器和组件数量较少时，可能可以接受手动配置机器，安装中间件，部署组件，或者手动登录到服务器并收集日志，以及执行其他手动任务。但是，当组件数量增加时，在某个时刻后，您可能无法使用手动方法。 自动化可帮助组建一个服务器并安装必要的运行时环境。然后，只需使用几行代码，就能快速将微服务放在这些运行时环境上。这种自动化使您能编写微结构代码，访问用于部署生产服务的准确的工具链，从而及早发现问题。自动化是连续集成和连续交付方法的核心推动力量。如果您想将微服务架构的复杂性保持在控制范围内，推崇自动化文化是关键。为此，您需要一种综合的、端到端的方法，以便在整个软件开发生命周期中推广自动化。这个生命周期涉及通过一些操作执行测试驱动开发，比如 IBM Bluemix® Garage Method。有关更多信息，请访问网站。https://www.ibm.com/cloud/garage/ 有界上下文开发模型时，请记住识别它的有界上下文，即模型的有效范围。有界上下文是具有明确边界的模型，模型在该边界内是没有歧义的。如果您不在模型周围设置一条边界，最终使用的上下文可能不在您的应用程序内。适合应用程序的某个部分的上下文不得适合另一个部分，即使它们具有相同的名称，而且指向相同的实体。例如，如果您构建一个预约系统，则必须知道客户的基本信息。但是，如果您在账单上下文中有一个账单系统，您可能希望在其中包含客户的联系信息和支付信息，而在预约系统上下文中，不需要该信息。如果您尝试在多个地方重用完全相同的客户模型，可能会在系统中导致不一致的行为。这是一个放入预约系统的上下文中的简单模型，包含一些除客户名称外的行为。 例如，您可能决定在客户模型上包含某种形式的验证，以确保拥有足够的信息来向他们收账。如果您不够小心，验证可能意外地阻止您使用客户模型安排预约；这不是那您想要的行为。账单系统可能要求客户拥有有效的信用卡，然后才能保存更改。但是，如果缺少信用卡信息，则会阻止您将客户预约信息保存到预约系统中，这是不合理的。 在这个示例中，您有两个上下文，但它们之间的边界是模糊和重叠的。Eric Evans 在他撰写的图书《领域驱动设计》中说道”模型仅在特定的上下文内有效。因此，最好显式定义应用该模型的上下文。您可以避免损坏该上下文内的模型，将它严格保持在这些边界内，并避免被外部问题分心或混淆。” 当显示定义了有界上下文后，通常能看到您是否拥有一个尝试扩展到多个上下文中的模型元素。在这个示例中，您希望在预约系统中保持简单的客户视图，而在账单上下文中提供包含联系信息和账单信息的更完整的客户视图版本。在两个不同的类中定义客户的这两个视图，然后将它们放在不同的应用程序中。Eric Evans 建议，通过为每个上下文提供它们自己的团队、代码库、数据库模式和交付管道，让有界上下文保持分离。 有界上下文的原则在微服务架构中至关重要。可使用这些原则作为指导，正确地确定系统并将其分解为微服务。明确定义有界上下文（意味着业务领域是通过显式边界分离的），有助于推断系统中最终包含的微服务。拥有有界上下文，还有助于正式化不同服务之间的交互，有效且高效地构建它们之间的接口。 1.通过服务实现组件化传统实现组件的方式是通过库（library），传统组件是和应用一起运行在进程中，组件的局部变化意味着整个应用的重新部署。 通过服务来实现组件，意味着将应用拆散为一系列的服务运行在不同的进程中，那么单一服务的局部变化只需重新部署对应的服务进程。 另外将服务作为组件可以更明确的定义出组件的边界，因为服务之间的调用是跨进程的，清晰的边界和职责定义是设计时必须考虑的。 即单一服务可以作为一个单独进程存在。 2.按业务能力来划分服务与组织团队康威定律（Conway’s law）指出： organizations which design systems … are constrained to produce designs which are copies of the communication structures of these organizations.任何设计系统的组织，最终产生的设计等同于组织之内、之间的沟通结构。 传统开发方式中，我们将工程师按技能专长分层为前端层、中间层、数据层，前端对应的角色为UI、页面构建师等，中间层对应的角色为服务端业务开发工程师，数据层对应着DBA等角色。事实上传统应用设计架构的分层结构正反应了不同角色的沟通结构。 而微服务架构的开发模式不同于传统方式，它将应用按业务能力来划分为不同的服务，每个服务都要求在对应业务领域的全栈（从前端到后端）软件实现，从界面到数据存储到外部沟通协作等等。因此团队的组织是跨功能的，包含实现业务所需的全面的技能。 近年兴起的全栈工程师正是因为架构和开发模式的转变而出现，当然具备全栈的工程师其实很少，但将不同领域的工程师组织为一个全栈的团队就容易的多。 3.服务即产品传统的应用开发都是基于项目模式的，开发团队根据一堆功能列表开发出一个软件应用并交付给客户后，该软件应用就进入维护模式，由另一个维护团队负责，开发团队的职责结束。 而微服务架构的倡导者提议避免采用这种项目模式，更倾向于让开发团队负责整个产品的全部生命周期。Amazon 对此提出了一个观点： You buidl it, you run it.开发团队对软件在生产环境的运行负全部责任，让服务的开发者与服务的使用者（客户）形成每天的交流反馈，来自直接客户端的反馈有助于开发者提升服务的质量。 4.智能终端与哑管道微服务架构抛弃了 ESB 过度复杂的业务规则编排、消息路由等（即去ESB化）。 服务作为智能终端，所有的业务智能逻辑在服务内部处理，而服务间的通信尽可能的轻量化，不添加任何额外的业务规则。 5.去中心统一化传统应用中倾向采用统一的技术平台或产品来解决所有问题。 不是每个问题都是钉子，也不是每个解决方案都是一个锤子。 问题有其具体性，解决方案也应有其针对性。 用最适合的技术方案去解决具体的问题，在大一统的传统应用中其实很难做到，而微服务的架构意味着，你可以针对不同的业务服务特征选择不同的技术平台或产品，有针对性的解决具体的业务问题。 6.基础设施自动化单一进程的传统应用被拆分为一系列的多进程服务后，意味着开发、调试、测试、集成、监控和发布的复杂度都会相应增大。 必须要有合适的自动化基础设施来支持微服务架构模式，否则开发、运维成本将大大增加（硬件水平跟上软件速度）。 7.Design for failure正因为将服务独立在不同的进程中后，引入了额外的失败因素。 任何时刻对服务的调用都可能因为服务方不可用导致失败，这就要求服务的消费方需要优雅的处理此类错误。 这其实是相对传统应用开发方式的一个缺点，不过随着一些开源服务化框架的出现，对业务开发人员而言适当的屏蔽了类似的错误处理，不过开发人员依然需要知道对服务的调用是完全不同于进程内的方法或函数调用的。 8.进化设计一旦采用了微服务架构模式，那么在服务需要变更时我们要特别小心，服务提供者的变更可能引发服务消费者的兼容性破坏，时刻谨记保持服务契约（接口）的兼容性。 对于解耦服务消费方和服务提供方，伯斯塔尔法则（Postel’s law）特别适用： Be conservative in what you send, be liberal in what you accept.发送时要保守，接收时要开放。按照伯斯塔尔法则的思想来设计实现服务调用时，发送的数据要更保守，意味着最小化的传送必要的信息，接收时更开放意味着要最大限度的容忍信息的兼容性。 多余的信息不认识可以忽略，而不应该拒绝或抛出错误。 微服务架构应用采用微服务架构面临的第一个问题就是如何将一个单一应用拆分为多个服务。 有一个一般的原则是，单一服务提供的功能是可以独立被替换和升级的。 也就是说如果有 A 和 B 两个功能，如果 A 功能发生变化时同时 B 功能也需要变化，那么 A 和 B 这两个功能应该被划在一个服务中。 微服务架构应用的成功经验近年已越来越多，例如国外的 Amazon，Netflix，国内如阿里都采用微服务架构取得了很多正面的成功案例。 但通过上文所述微服务架构特征看出，其实微服务架构模式有利有弊，需要根据实际的业务、团队、环境进行仔细权衡利弊。 其中的服务拆分带来的额外开发、测试、运维、监控的复杂度，在现有的环境、团队下是否能够很好的支持。 另外，有人可能会说，我一开始不采用微服务架构方式，而是在单一进程内基于清晰定义的模块化方式，模块之间通过接口调用，到了适当阶段，必要的时候再将模块拆分为服务。 其实这个想法显得过于理想，因为进程内良好定义的接口通常不是很好的服务化接口。 一开始没有考虑服务化的设计方法，那么后期拆分时依然是一个痛苦的过程（改整体架构对一个已成型的产品来说比开发一个新产品还要吃力）。 难在架构，也赢在架构！ 总结正如，Martin Fowler 在其文中所说，微服务架构是否就是企业应用开发的未来，还有待时间的检验。 就目前的情况看，对此我们可以保持谨慎的乐观，这条路依然值得去探索。 实际任何的架构决策都是基于我们不完美的现状做出的，这正是架构取舍的微妙之处，超越任何的方法论。 在整体式应用程序中，大部分逻辑都部署在一个集中化、单一的运行时环境或服务器中，并在其中运行。整体式应用程序通常很大，由一个大型团队或多个团队构建。采用此方法，各个团队需要花更多精力和统筹安排才能执行更改或部署。 随着时间的推移，整体式模型中已引入了更好的架构模式，有助于显著提高架构的灵活性。例如，一种著名的模式是模型-视图-控制器 (MVC)，它将应用程序分解为层和合理的组件。这些模式有多种优点，比如更快的初始开发、更简单的应用程序治理，等等。但是，整体式模型也有缺点，尤其是在当今环境中的技术瞬息万变的背景下。（MVC【模型-视图-控制结构的整体式模型】已经不满足当前网络开发的情况了） 整体式方法可能带来许多挑战，有以下四点： 庞大的应用程序代码库 庞大的代码库可能给希望熟悉代码的开发人员带来困扰，尤其是团队的新成员。庞大的应用程序代码库可能还会让应用程序开发过程中使用的开发环境工具和运行时容器不堪重负。最终，这会导致开发人员效率降低，可能会阻止对执行更改的尝试。开发人员在进入这个项目业务时，因为是整体式的架构项目，开发人员可能负责的是整个控制层，业务层或者持久层的逻辑，虽然各层的处理逻辑和分类管理做的好的项目容易看得懂，但是各层的代码库随着产品的深入会加上各种功能，导致代码库庞大且凌乱，这就是导致了开发人员的工作重点不是在有效的开发上而是在找代码甚至是理清他们的关系上，开发效率低下也导致无用功过多，也会使开发人员的情绪受到影响。 不频繁的更新 在典型的整体式应用程序中，大部分（几乎是全部）逻辑组件都部署在单一运行时容器中，并在其中运行。这意味着要更新对某个组件的一处细微更改，必须重新部署整个应用程序。另外，如果需要推广细微但关键的应用程序更改，则需要投入大量精力来对未更改的部分运行回归测试。这些挑战意味着整体式应用程序很难连续交付，这导致部署次数减少，对需要的更改的响应变慢。利用微服务架构后若出现问题则可以单个组件更新部署，不需要整个项目进行维护升级。 依赖单一类型的技术 对于整体式模型，由于应用更改方面的挑战，以增量方式采用新技术或技术栈开发框架的新版本会变得很困难。最终，整体式架构应用程序通常必须一直使用这一种技术，这最终会阻碍应用程序跟上新的发展趋势。微服务架构利用各项技术优势互补，哪个效率高用哪个。新技术增加的功能可以直接部署在系统上，各组件的技术也可以用不同的语言来实现。甚至是为整个项目整体更换新框架时也会变得容易。 可扩展性 可扩展性是整体式架构面临的最大挑战之一。Martin Abbot 和 Michael Fisher 在他们合著图书《可扩展的艺术》中介绍了一种查看系统的可扩展性的有用方式；他们使用了一种三维可扩展性模型或扩展立方体。在此模型中，通过在负载平衡器后运行克隆版本来扩展应用程序称为 X 轴扩展或水平复制。另外两种扩展是 Y 轴扩展（或功能分解）和 Z 轴扩展（或数据分割），Y 轴扩展通过拆分不同实体来实现扩展，Z 轴扩展通过拆分类似实体来实现扩展。由于整体上的凝聚性，典型的整体式应用程序通常只能在扩展立方体的一个维度上扩展。随着生产环境收到更多请求，该应用程序通常采用的垂直扩展方式是添加更多资源供其使用，或者克隆到多个副本来进行响应。这种扩展方式低效且很耗资源。当应用程序达到一定规模时，开发团队必须拆分为更小的团队，每个小团队专注于一个特定的功能区域，各团队彼此独立工作，而且通常位于不同地理位置。但是，由于应用程序的各部分间的自然凝聚性，需要各个团队协力执行更改和重新部署。 使用微服务架构的最重要目的是，解决整体式模型面临的难题。将从应用程序的不同涉众角度，介绍微服务方法如何帮助解决整体式系统的问题。 对于业务所有者 作为业务所有者，您希望您的服务适用于新客户和业务需求。但是，在整体式模型中，由于庞大的代码库，为满足业务需求而执行并推广更改的过程会很缓慢。这个过程缓慢的另一个原因是，各个组件和层之间有严格的内部限制和依赖关系。 依靠传统框架来将各个层和组件整合起来，这样的话各层的关系不仅会受到整体框架的限制，还会让开发周期变得很长。 微服务架构原则是围绕高灵活性和恢复能力而建立的。这两个特征有助于快速推广更改。这有助于业务所有者更快地收到反馈，调整业务和投资战略，从而让客户满意和提高市场竞争力。 从资源分配的角度讲，由于团队更小且更专注，所以可以更轻松地测量和可视化效率。然后，业务所有者可以更轻松地制定投资决策，可将资源从低业务影响区域转移到高业务影响区域。 对于服务管理人员 作为服务管理团队成员，您希望协调各个团队的管理操作负担更少，以便您可以提高服务的生产力。整体式模型需要做大量的工作。活动之间需要的协调更多，因为整体式应用程序通常拥有庞大的业务范围，以及许多基础架构和操作接触点。因此，对应用程序的每次更改都可能需要不同涉众多次评审和批准。微服务架构推崇利用自助服务，在服务交付管道的每个阶段利用自动化。 这有助于减少服务管理团队的日常管理协调。 微服务架构中的一个重要原则是高可观察性。高可观察性功能为服务管理团队提供了必要的工具，以便更好地监督系统中或产品中的每个微服务的状态。这有助于提高服务管理效率。 对于开发人员 作为加入团队的新开发人员，您希望快速熟悉源代码，以便快速上手并带来成果。典型整体式应用程序中的代码库很大，可能阻碍您并潜在地延长学习曲线。对于所有开发人员，庞大的代码库会增加载入开发环境中并运行的负担，从而导致生产力降低。 庞大的代码库可能让代码评审和其他合作开发活动面临更大压力。此外，在处理更改时，破坏其他功能的风险可能导致开发人员对创新和增强应用程序犹豫不决。然而，微服务更小且更轻量，这可以缩短新开发人员的学习曲线。微服务还可以帮助消除加载和运行的繁重负担，鼓励引入突破性的更改，从而帮助提高生产力和创新水平。 ———————————————更新线————————————————— 云时代为什么现在是采用微服务架构的好时机。 云环境和产品的激增 微服务架构体现了使用连续集成和连续部署的许多优势。该架构也引入了新的复杂性，需要一种在构建应用程序的每个步骤中实施自动化的现代方法。例如，从基础架构和治理的角度讲，首先需要一个能动态地快速为服务组建运行时环境的业务连续性基础架构。该环境可能是一个虚拟机、容器等。另外，还需要一种统筹安排和监视服务的高效方式。当今环境中的云平台（比如 IBM Bluemix）可通过其自然的动态性和恢复能力满足此需求。借助可用于各种服务模型的不同云产品，无论是基础架构即服务 (IaaS) 还是平台即服务 (PaaS)，开发人员都可以通过更多选择转变为微服务战略。借助 IaaS 选项，您可以在几分钟内快速组建一台机器，而且可以将基础架构配置打包到一组脚本中，以便根据需要自动化该流程。如果您不想接触基础架构级别的复杂性，也可采用平台级选项，采用不同的语言和服务的形式来快速打包，然后根据意愿包含和启动微服务。IBM Bluemix 是这些平台的一个示例。IBM Bluemix 有许多适合使用云技术构建微服务的服务，比如 IBM 容器、消息中心、日志记录、监视和其他技术。Bluemix 使开发人员能够快速创建、部署和管理他们的云应用程序，为简化操作、确定安全策略和管理系统中微服务的健康提供关键基础。 工具的可用性和成熟性 除了云基础架构可为微服务战略的采用所提供的动态性和恢复能力，拥有全面的工具也是采用微服务战略的关键需求。微服务工具在不断演变和进步。在当今环境中，开发人员有许多选择，他们可以使用一组合适的工具来实施其微服务战略，比如日志工具组合、监视工具组合、服务注册表或容器化技术。这些先进工具可帮助解决微服务架构所引入的挑战，以便更有效地交付、管理和统筹安排服务。图展示了基于微服务架构而构建的 IBM Watson™ 云服务的完整组合示例。这种革命性架构有云技术、一组全面的工具及敏捷流程提供支持。 该架构包含多个主要的技术组合：DevOps每个 Watson 云服务在开发后，都会在一个不可变的虚拟机中容器化，然后通过明显的 DevOps 流程自动部署到 IBM SoftLayer 云基础架构上。微服务架构的典型模式（比如服务发现、API 网关等）是通过 IBM 独有的和开源的技术来使用的。然后，可以在 IBM Bluemix 平台上公开这些服务。Elasticsearch、Logstash、Kibana (ELK) 组合或 Elastic 组合Elk 组合是该系统的日志工具组合，包含一组工具来捕获日志，并将其存储在一个强大的、集中化的、可搜索的日志数据库中。有关更多信息，请查阅 elastic 网站。监视工具组合展示了一组工具，它们可从一个中央仪表板监视整个系统，包含一种通知机制，以便基于特定事件来发送提醒。 从整体式应用程序向微服务的转变 虚构公司 A 的业务问题 虚构公司 A 是一家电子商务公司，它使用了一个名为 Customer Order Service 的基于 Java EE 的传统 Web 应用程序来提供在线购买服务和运营业务。尽管该应用程序能很好地处理业务，但公司 A 已开始努力响应新的业务需求。这些需求包括： 接触使用移动设备的客户基于对客户在互联网上的个人行为的洞察，改善客户体验扩展基础架构，以便处理来自新客户和现有客户的更多请求保持较低的 IT 成本以及其他需求目前的客户订购服务应用程序的设计不支持在业务领域中执行更改，而且无法应用新技术来加速创新。图中介绍了当前的整体式应用程序的逻辑架构概述。 公司 A 希望改变客户订单服务应用程序，以便从业务和技术角度促进和更好地处理更改，它拥有一些主要的业务需求： 新系统必须是经过进化的，意味着它必须能灵活地处理更改。在将流量从当前系统转移到新构建的系统的过程中，不允许宕机。新应用程序必须能基于发送给系统的有效负载来按需或自动扩展，以便应对动态的购物行为模式。新系统必须支持利用新兴技术来促进创新。 采用微服务来实现一种革命性架构： 采用微服务架构的主要动机是，解决很难更改的传统整体式架构的问题。微服务方法支持对架构的每个组成部分执行更改。对于业务需求，公司 A 非常适合在构建新系统时采用微服务架构。 公司 A 应用最佳实践和模式将现有的整体式应用程序转变为更加革命性的架构，以期最终将应用程序迁移到微服务架构上。执行以下主要步骤和活动： 演化战略要拥有一种转型案例分析中的整体式应用程序的合适战略，必须发现和考虑不同的模式和建议实践。 识别要转变为微服务的候选功能在这一步中，选择应用程序的相对较小的组件或功能片段。从这些功能片段，可配置新微服务来让这些片段更有利于经常或渐进式的更改。 数据访问模式因为数据是 IT 系统中最重要的资产，所以一个关键步骤是在转变为微服务架构的过程中采取正确的数据访问方法。 安全和治理“安全和治理”将介绍如何在更加分布式的新模型中管理应用程序的组件，以及如何处理安全挑战。 性能和可扩展性解决整体式应用程序的可扩展性问题时，微服务架构（拥有更多分布式特征）带来了性能挑战。 DevOps 和自动化自动化是让微服务方法成为可能的推动因素。 重点介绍了微服务的重要概念、特征以及它为何对现代软件应用程序的开发如此有吸引力的原因。最后，通过一个示例简单的描述了从整体应用程序向微服务的转变。目前为止，相信您已经对微服务有一个初步的了解。下一部分将更深入的介绍如何在 Java 中创建微服务。 内容转自 Evolve the Monolith to Microservices with Java and Node （IBM产品devoloper开发红皮书）]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>架构</tag>
        <tag>Dubbo</tag>
        <tag>SpringCloud</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创业之路--关于风险投资]]></title>
    <url>%2F2018%2F03%2F19%2Ffengtou%2F</url>
    <content type="text"><![CDATA[关于风投泄密及其保密协议保密协议就属于防君子不防小人,特别是在商业模式、概念以及技术在早期还很不具象的时候(这恰恰都是初创公司所面临的情况),保密协议起到的作用就更小,因为连保护的标的都不清楚,总不能说保护一个头脑中的idea吧?即使起到作用,通过法律手段花上无数的时间和精力去索赔并成功,损失的时间和精力放在自己的梦想和创业上,是不是会更加划算?天使投资人最大的价值就是,给钱不捣乱,同时需要的时候提供不太计较的支持. 所以题主关心的方向不对,应该更关注如何找到靠谱的天使投资人,而不是在这种ROI极低的地方选择如何去保护自己的利益。如同找老公过日子,越是天使投资越容易走得远成为长期的婚姻关系,关注点更多在于如何选对人,而不是如何利用拉链守住门户. 而且还有一个基本原则,如果在天使阶段,一个潜在投资人对于你的技术绌节和实现方式如此关注,而对于创始人的人品以及公司的远景却不那么在乎。是不是有点熟悉的感觉,上来就问你的银行存折、小区档次等等? 保密协议的注意事项1、保密信息的范围从披露方来看，只要是自己提供给对方的非公知信息，都可以纳入保密信息的范围，包括关于自身的保密信息，也包括自己获准披露的第三方保密信息。另一方面，也不能范围过宽延伸到接受信息方已有或者自行开发的信息以及无关第三方披露等原因造成公开的信息，尤其是保密期限比较长的时候，接受信息方也可能因为各种现实原因无法承受过重的保密限制。 2、商业秘密上文提过，并非所有的保密信息都能构成商业秘密，商业秘密的保护须权利人采取保密措施，例如：技术措施以及要求接触到商业秘密的员工签署保密协议的法律措施等。商业秘密和普通的保密信息需要区分开来。针对保密信息双方可以商定一个有限的持续时间（很多信息因为技术发展几年后就根本没有保密的意义了），期间届满后则不再负有保密义务；但商业秘密则不能轻易地披露给对方，披露给对方也要注意保密措施，并且不受保密期限的限制。 3、知识产权专利等知识产权与商业秘密的保护方式各有利弊，此处不再展开。有些创意、方法和流程可能无法获得知识产权的保护，所以只能作为商业秘密并通过保密协议等方式防止保密信息进入公知领域。即便是能够申请专利的技术或方案，也需要在申请专利前注意保密，并与接触者签署保密协议，防止被公众所知破坏其新颖性，导致无法申请专利。 4、保密信息的使用限制保密协议不仅仅限制信息接受方不得对外披露保密信息，而且需要限制信息接受方对保密信息的使用范围和目的。一方面，信息接受方应当仅限于向项目合作过程中确需了解保密信息的人群披露，还要限制信息接受方使用于讨论、评估或依据各方协议约定实施项目合作之外的目的，例如：不得因项目之外的目的擅自使用保密信息。此外，如果披露具有知识产权的内容则需要明确，依据保密协议进行的披露与知识产权的许可使用不同，为签约各方项目合作之目的披露给对方，并不意味着给予了对方知识产权的使用许可。结语保密协议仅仅是保护公司保密信息的一种常见手段，创业公司还需要提高保密意识，并结合技术手段以及知识产权等各种方式，更全面地保护自己来之不易的创造性成果。如果运用得当，即便是创业公司也能有效对抗巨头“友商”窃取自己的劳动果实。 在《要求投资人签署保密协议 能否保护你的商业秘密》这篇文章中，我曾经表示，通过要求投资人签署保密协议的方式，不能从根本上保护自己的商业秘密。那创业者在融资时，还需要与投资人签署保密协议吗？答案是肯定的。也就是说，保密协议还是要签的。既然它不能从根本上解决问题，那在这里为什么还要建议创业者与投资人签署保密协议呢？因为尽管它不能从根本上解决问题，但毕竟能起一定的作用。而对有些投资人，这点作用就已经足够。不同的投资人，愿意承担的违约风险不同。如果对方可以承受的违约风险幅度很大，那单靠一纸保密协议，不能引起他们足够重视，自然也约束不住他们。这时候，就要附加其他手段。但如果对方可以承受的违约风险幅度很小，那这一纸保密协议就能很好的约束对方。同一份协议，在不同的投资人眼里分量不同，自然对其影响和制约能力也不同。在《周永信说融资泄密17：Term Sheet前投资人不愿签署保密协议的原因及例外》这篇文章中，我也曾提到，投资人是不愿意签署保密协议的，但那指的是在Term Sheet之前。真正到了谈判Term Sheet阶段，甚至尽职调查阶段，投资人是不会拒绝签署保密协议的。因为这是一种合理且常规的要求。到了这一阶段，如果创业者还不要求签署保密协议，投资人甚至会认为创业者不够成熟。因为保密意识是企业成功的必要要素之一，没有投资人愿意投资一个保密意识淡薄的创业者。所以不管是为了保护自己，还是为了取信于投资人，创业者都要要求投资人签署保密协议。只是，要掌握好时间点。 那么，投资人看重的是什么？第一，商业案思路清晰，市场回报设计方案路径清楚，言之成理，第二，最最重要的：执行团队。换句话说，绝大多数投资人看重的，不是案子，而是能把案子做成买卖的创业团队！所以，千万别想着拿几张天马行空的A4纸去打动别人把几千万给你，他们看都不会看，即便里面写了九阴真经。但如果这纸上的方案，你已经建立起了自己稳定的核心团队，而且靠这个团队已经在小规模范围内取得了一定的商业成功，现在是需要资金扩大规模，那么，加上你的好点子，投资人对你是有兴趣的。 是否需要签订保密协议,主要还是要具体评估下题主展示或向对方提供的内容是什么。如果是机变的数据资料或者一些数据模型等,付出大量的人力、物力的智力成果,可以达到商业秘密了众所知悉、能芾来经济利益,具有实用性并采取保密措施的技术信息和经营信息)还是非常有必要签署保密协议的。虽然有人说保密协议是防君子不防小人,但有保密协议在,毕竟会对一部分投资人产生威慑,而且后续如果有纠纷,诉诸司法,保密协议毕竟要求对方承担的一个法律文件和基础如果只是一些创意或电子,毕竟法律不保护创意,签与不签,并无实质性区别。 家装风投经纬中国，合力投资，红杉资本，IDG资本，沸点资本，创湃资本，稳国基金，创吧投资，58集团，梧桐树资本，红星美凯龙，北辰星，天使湾，印尼力保集团，华耀资本，氪空间，中骏基金等等 基本涵盖种子轮，天使轮，Pre-A轮，A轮，B轮，C轮，C+轮以后，。。。，F轮，上市前。 天使投资人需要做进一步的网络爬虫 不局限于拿国内资本投资，可考虑境外投资或者联合投资形式 A+轮后可考虑资本市场，多家融资 VC风险投资(venture capital) 轮次融资同时也是解读资本与企业发展之间关系的学问。 种子轮种子投资极具风险，大多数种子投资者明白70%的初创企业投资最终都会失败。但是，他们也明白，只需一个成功的初创企业，便可带来大量的财富。因此，种子投资者明白，在寻找成功企业的过程中，他们需要与企业“风险共担”，更重要的是，要避免“FOMO”的陷阱，即“错过”下一个取得巨大成功的初创企业[2] 。由于更多的种子投资者承担更多的风险，因此早期的种子融资变得更加普遍、丰富，也更容易获得。尤其在许多初创企业明白快速进入市场的重要性之后，种子融资的易得性和不断增长的资金池对初创企业来说无疑是好消息。具有讽刺意味的是，这种发展也给初创企业带来了严重的问题。种子融资后，会对初创企业创始人进行早期的估值、收入和客户标准检查，并规定达成业绩的严格时限。如果初创企业过早进入种子融资，在接下来的轮次融资阶段，如A轮融资时，则会遇到严重的挑战。轮次融资通常更加重要和复杂，因此如不能满足标准，会给接下来的投资人带来强烈的负面信息。 在融资中，这一般被称之为“种子轮”投资，它是指公司发展的一个阶段。在这个阶段，公司只有创意却没有具体的产品或服务，创业者只拥有一项技术上的新发明、新设想以及对未来企业的一个蓝图，缺乏初始资金投入。创业者在这一阶段寻找投资的时候，最需要的讲清楚的是“我要做什么”，重要的是要给投资人画好你想象中的“大饼”。一般来讲，因为前期资金需求相对较小，也或许是创业者大都抱有“先做出一点成绩或者先做出个样子”的想法，种子期的所需要的资金都是创业者自筹或者通过亲朋好友来筹集。当然，也有种子期投资人和投资机构。注意，种子期的投资资金一般在10万-100万RMB左右。当然，也有破例。比如，最近VR比较火，森声科技在3月就宣布获得了数百万元的天使投资。森声科技想做的是，基于双耳录音(Binaural recording)技术，为广大 VR 影视团队提供一套简便的全景声录制和播放解决方案。 种子期的项目，往往只有一个idea和初始团队（有些只有一两个创始人），idea能不能转换为一个make sense的business，具有高度的不确定性，需要通过一段时间的尝试，对idea背后的各种假设进行验证，从而探索到真正可行的方向。在此过程中项目的方向和内容随时有可能面临调整，而且项目一没有历史，二缺乏连续性，唯一稳定的、可供投资者参考的因素就是团队（而且主要是创始人），因此种子期的投资主要看人。人是极其复杂的，要想对一个人作出判断就必须深入了解他、和他打交道。由于这个过程依赖大量的经验和直觉，很难进行理性的分析，因此履行这个任务、做这个决策的，一般都是个人投资者。这也是“天使”这个称号的来历。此外，由于尝试和探索所需的资金量一般不是太多，个人投资者出得起，且项目越早期风险越大，所以天使投资的金额一般也较小，一般都在500万（人民币）以下。 —————————–破土期——————————————————— 天使轮天使轮是指公司有了产品初步的模样，商业模式也已初步形成，同时积累了一部分的核心用户。投资来源一般是天使投资人、天使投资机构。相比之后的融资，这个时期想要拿到融资还是相对容易的，投资人主要看的是创业团队和创业方向。对于成熟理性的天使投资机构来说，种子期和天使期项目在阶段上的差别不大，区别最大的就是在融资金额上。因此，也有人会把这种初创公司的融资阶段称为“种子天使”。 天使轮，即天使投资（Angel Investment），是指个人出资协助具有专门技术或独特概念而缺少自有资金的创业家进行创业，并承担创业中的高风险和享受创业成功后的高收益，或者说是自由投资者或非正式风险投资机构对原创项目构思或小型初创企业进行的一次性的前期投资。天使投资是风险投资的一种特殊形式。 天使轮的投资资金额度一般在100万RMB到1000万RMB左右。上限为2000万左右。2015年12月，刚刚成立半年多的美国P2P保险公司Lemonade获得红杉资本1300万美元的种子轮投资。Quartz称，这是红杉历史上最大的种子轮投资之一。 种子、天使轮，顾名思义就是很早期的公司，可能只是一个idea、没有实际的走出去，这个时候VC机构，一般会看创业者的背景、愿景，但最重要的还是个人背景，如果你是一个已经很牛的人，那你的天使轮估值肯定会很高，比如瓜子二手车天使轮就能拿到6000万美元的融资，就是因为杨浩涌以前有过成功创业的经验，即使他做出渣来也会有人投资的。 天使投资也属于风险投资的范围，只是更为早期，风险更大，同时回报也可能更高。天使投资的来源更广泛，早年最常见的三类天使投资人，3F，family，friend，fool。 随着越来越多的互联网公司上市，大量掌握大笔资金，熟悉上市流程，和投资机构关系密切，了解行业动向，具有大量人脉的前互联网公司高管们开始成为专业的天使投资人，他们是最合适的天使投资人选，也会有相当多的好项目被他们挖掘获得成长。 同时，天使机构化的趋势也开始明显，一些新的天使投资开始就以正规军的方式作战，一些以往A轮起投的风险投资机构也开始向天使扩展，所以天使投资和风险投资的界限也开始模糊，天使和VC主要用在界定投资金额的多少，比如通常天使是100-200万人民币，A轮是100-200万美金，B轮是500-1000万美金，等等，但是也并不确定，有人天使就上千万美金了。 除了资金多少的区别，我通常会按照项目的状态来界定项目的阶段以及投资款的用途。天使阶段，商业模式还没有建立，钱是不够烧的，天使投资的钱更主要用来搭建团队，大量试错，为A轮较大资金进入，具有可复制的商业模式的基础做准备，要能证明后续资金可以高效的使用才值得引进新的投资，也才容易拿到后续的风险投资。（当然，这个市场上充满了反例和个案） 成长期则似乎没有公认的定义，我个人的理解是当一个项目经历过种子期的摸索，探索到一条有较大可行性的道路时，便进入到成长期。可以说种子期是纸上谈兵，成长期则经过了实践，从市场的反应中看到了希望。企业进入成长期以后，战略基本成型，准备着手投入资源（其中资金是关键资源）去实现这个战略。这个时候投进去的，就可以算作是VC了。所以VC是企业战略初步成型以后用以支撑企业去实施战略的投资。此时企业刚刚在市场上取得一些成绩，或者看到了一些成功的苗头，但企业自身的资源不足以支撑它，需要引进外部的资源。对投资者而言，企业战略所隐含的关键性的假设通过市场已经有所验证，此时可以对项目进行理性的分析，并能够对面临的风险进行相对准确的评估。这就有了机构化投资的基础，也即实际的出资人可以委托专业的投资人士进行操作并对投资人士实施监督，从而在投资领域产生了委托－代理关系；另一方面，这个阶段企业需要的资金量相对比较大，如果由个人投资者投资将很难分散风险，因此投资的机构化也成为必然。因此，VC一般都是以基金的方式实行机构化运作的，投资额一般在千万量级。 天使的话，我喜欢你，看得起你， 相信你，就可以了。看中的可能只是被投资人的某个闪光点而已。 天使投资无疑是“单个项目的回报”最高的。早期项目估值低，一旦项目成为了独角兽，百倍千倍的回报完全可以实现。一版情况下项目如果没有5倍、10倍的回报，都不好意思拿出来和同行说。但是对应风险也是非常高的。比如，天使投资人一年投10个项目，其中有9个血本无归，只靠成功的那个项目赚了百倍收益来弥补9个的亏损，这种情况也是时有发生。VC和PE投资随着公司估值不断上升，单个项目的回报倍数越低，相对投资成功的概率会比天使投资高不少。 所以天使轮也被称为独角兽投资。 Pre-A轮按照正常的融资进程，下一轮的融资应该是是A轮融资。但是，有的公司会增加一轮融资：Pre-A轮。 Pre的意思是前期，Pre-A就是A轮之前的融资。增加这轮融资的原因是多方面的，创业公司在估值不理想;资金困难，但又没有达到A轮阶段的规模，以上这些情况都有可能。Pre-A可以看作一个缓冲阶段，可以让创业者资金压力缓解，也可以让新的投资人进来。 ———————————天堑期—————————————————————– A轮A轮，其实A轮也是初创公司，很多有资历、有人脉的，通过自己或创始团队搞定了天使轮，真正向VC拿钱的时候是A轮，A轮公司特征是已经有了产品原型，可以拿到市场上面对用户了，但基本还没有收入或者收入很小，类似的公司有小米、平安好医生等，都是有产品了拿到了A轮，但公司还是不盈利；这个时候投资人还是更看重创业者资历背景，但这时候也会看些市场前景，以及公司的愿景了。 在A轮融资阶段，创业公司的产品已经基本成熟，产品上线或者服务已经正常运作一段时间，并有完整详细的商业及盈利模式。另外，创业公司在行业内拥有一定地位和口碑，但是还处于亏损的状态，有诱人的前景。A轮的投资资金额度一般在1000万RMB到1亿RMB左右。 2015年12月，58到家宣布与阿里巴巴，平安，KKR完成了A轮融资协议的签署，58到家将以超过10亿美金的估值融资3亿美金。据说，这是互联网史上最巨额的A轮融资。当然，从天使到A轮是很多创业公司很难踏出的一步。在这个时期，是考验创业公司的产品、商业模式、盈利模式和创业团队等各个方面的阶段。如果一个环节出现问题就可能导致一个项目的失败或者说创业失败。 B轮创业公司经过一轮烧钱后，获得了较大发展。甚至一些公司开始盈利，盈利模式趋于完善，可能需要推出新业务、拓展新领域。资金来源一般是大多是上一轮的风险投资机构跟投、新的风投机构加入、私募股权投资机构加入。投资资金额度一般在2亿RMB以上。 ———————————换血期———————————————————————— C轮到达C轮融资的时候，创业公司又会遇到一个瓶颈。这时公司已经非常成熟了，离上市不远了。这轮除了拓展新业务，讲讲“生态”，就要开始准备上市了。资金来源主要是私募股权投资，有些之前的VC也会选择跟投，投资资金一般在10亿RMB左右。大部分公司一般C轮后就会上市，但也有公司选择融D轮，甚至更多轮的融资。这些需要更多轮融资的公司大部分是其本身的业务所决定的，有的项目需要大量的烧钱，过早上市就不符合这类公司的发展战略，最为形象的例子就是滴滴了，滴滴由于烧钱补贴，现在已经G轮了，还在融资。 C+轮–至死亡轮简单来说就是无数的资本融入。 IPO期完成以上这些融资阶段后，公司就可以上市了。上市之前第一件事就是IPO。IPO(Initial Public Offerings)即为首次公开募股，是指一家企业或公司 (股份有限公司)第一次将它的股份向公众出售(首次公开发行，指股份公司首次向社会公众公开招股的发行方式)。只有首次公开上市完成后，公司才可以申请到证券交易所或报价系统挂牌交易。IPO需要经过多个环节的严格审核。以上，就是一家公司从创立初到上市的全过程。 天使投资、VC 以及 PE 的区别是什么？VC即风险投资，PE即私募股权 回答这个问题,需要看一下私募股权基金的发展历史,以及在PE这个大概念里产生的V和 Angel 私募股权基金,是伴随着上世纪七八十年代的并购潮起来的概念,可以看做一个替代资本市场,为需要资本或者想岀手资产的企业提供类似股票市场和银行信贷功能的一种金融产品 私募股权基金,最早从事的就是兼并收购,随看市场的变化,老牌的PE基金,比如黑石,KKR在随时调整并改变其投资策略,但是,这些老的PE基金在折腾纳贝斯克,折腾UCAR,折腾轮轴制造公司的时候,他们亻脑子里想的是现金流状况,是持续盈利能力和稳定的偿债能力 于是,这些老PE和他们所实际控制的传统企业的光芒,被来自硅谷的人和他们背后的资本力量所遮挡了,而VC和天使们的故事,要从1995年4月 Netscape的IPO开始谈起。 对于黑石和KKR来说,像网景这样 Pre ipo收入只有1600万美元,净亏损430万美元的公司能上市简直就是荒唐的事情,但来自硅谷的另类PE——VC们告诉黑石,新世纪,投资者会对那些用新技术实现高成长的公司充满信心。于是,传统金融领域闻所未闻的IPO大批出现,包括:雅虎Amazon甚至包括 Google。 买股票,不用看盈利水平,不用看财务报表,来自西海岸的投资者们一次又一次抽打华尔街那些穿看笔挺西装的人们的脸。一个又一个天价的IPO,让华尔街的PE们开始反思传统并购和私募股权融资创造的资本价值是否还有没有意义。 时,美国的PE团伙集中在曼哈顿,而∨C和天使们则聚集在苹果的老家帕拉奥图。PE们每天打看爱马仕领带,而VC们则穿看和乔布斯一样的衣服到处瞎比溜达,找他们心中无需看财务报表,只看技术和团队的下一个网景或者亚马逊。 如果说PE们考的是财务模型和数学分析,那V们靠的就是对技术领先性和市场占有率的赌局。事实证明,西海岸的∨C们比曼哈顿的那帮老头更讨人喜欢。黑石,KKR们动辄几百亿的并购,也敌不过西海岸一个又一个高科技上市公司的造富神话。 那么,说完了历史问题,针对题主的可题,讲点更直接的东西 1.PE更关注的是财务数据,现金流状况,以及各种极限环境下的损失测算,因此PE失败的概率较小,但暴富的概率也不大2.VC需要关注的则是产品本身,市场潜力和社会价值,要准确的洞察历史的演进和应用的趋势,并成为下一个风囗里的猪3.天使更需要准确的洞察趋势,以及赌对一个产品或团队上世纪末,因为互联网和高科技行业的譟动,伴随看大批高科技企业的IPO,一大批VC和天使基金就此崛起。 但,VC和天使们所面对的行业趋势不可预知性和极大地投资风险,是他们相对PE最大的区别所在要了解vc和天使的区别,就必须要了解他们的起源和PE的发展历史,以及几个重要的历史变革vc和天使的投资理念,孬资风格都是从我讲的这些历史里演化而来的。 VC和PE的发展时间较长，资金来源比较丰富，高净值个人、专业风险基金、杠杆并购基金、战略投资者、养老基金、保险公司。]]></content>
      <categories>
        <category>创业之路</category>
      </categories>
      <tags>
        <tag>风险投资</tag>
        <tag>VC</tag>
        <tag>融资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[google-Google Protocol Buffer]]></title>
    <url>%2F2018%2F03%2F15%2Fgoogle-tech00%2F</url>
    <content type="text"><![CDATA[Google Protocol Buffer什么是 Google Protocol Buffer？ 假如您在网上搜索，应该会得到类似这样的文字介绍：Google Protocol Buffer( 简称 Protobuf) 是 Google 公司内部的混合语言数据标准，目前已经正在使用的有超过 48,162 种报文格式定义和超过 12,183 个 .proto 文件。他们用于 RPC 系统和持续数据存储系统。Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python 三种语言的 API。或许您和我一样，在第一次看完这些介绍后还是不明白 Protobuf 究竟是什么，那么我想一个简单的例子应该比较有助于理解它。 移动应用客户端与服务器之间的通信协议,目前比较主流的有Facebook的Thrift,腾讯的JCE,以及Google的ProtocolBuffer(以下简称protobuf),本文主要介绍protobuf基本概念,协议解析,以及在Android中的应用实践。 定义一种 结构化数据 的数据存储格式（类似于 XML、Json ）,用于网络世界的数据交换 Google 出品 （开源）Protocol Buffer 目前有两个版本：proto2 和 proto3因为proto3 还是beta 版，所以本次讲解是 proto2 Protobuf是一种灵活高效的,用于序列化结构化数据的机制,类似于XML,但比XML更小,更快,更简单。Protobuf序列化为二进制数据,不依赖于平台和语言,同时具备很好的兼容性。 通过将 结构化的数据 进行 串行化（序列化），从而实现 数据存储 / RPC(Remote Procedure Call Protocol远程过程调用协议) 数据交换的功能： 序列化： 将 数据结构或对象 转换成 二进制串 的过程反序列化：将在序列化过程中所生成的二进制串 转换成 数据结构或者对象 的过程 应用场景传输数据量大 &amp; 网络环境不稳定 的数据存储、RPC 数据交换 的需求场景 如 即时IM （QQ、微信）的需求场景 在 传输数据量较大的需求场景下，Protocol Buffer比XML、Json 更小、更快、使用 &amp; 维护更简单！ 序列化与反序列化网络通信协议OSI/RM协议模型说到GPB（Google protocol Buffer）通过序列化与反序列化来进行数据传输，那么我们来聊聊序列化的问题。 序列化 &amp; 反序列化 属于通讯协议的一部分通讯协议采用分层模型：TCP/IP模型（四层） &amp; OSI 模型 （七层） ISO/OSI的参考模型共有7层，由低层至高层分别为：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 各层功能分别为： （1）物理层 提供建立、维护和拆除物理链路所需的机械、电气、功能和规程的特性；提供有关在传输介质上传输非结构的位流 及物理链路故障检测指示。在这一层，数据还没有被组织，仅作为原始的位流或电气电压处理，单位是比特。 （2）数据链路层 负责在两个相邻结点间的线路上，无差错地传送以帧为单位的数据，并进行流量控制。每一帧包括一定数量的数据 和一些必要的控制信息。与物理层相似，数据链路层要负责建立、维持和释放数据链路的连接。在传送数据时，如 果接收点检测到所传数据中有差错，就要通知发方重发这一帧。 （3）网络层 为传输层实体提供端到端的交换网络数据传送功能，使得传输层摆脱路由选择、交换方式、拥挤控制等网络传输 细节；可以为传输层实体建立、维持和拆除一条或多条通信路径；对网络传输中发生的不可恢复的差错予以报告。 网络层将数据链路层提供的帧组成数据包，包中封装有网络层包头，其中含有逻辑地址信息——源站点和目的站点 地址的网络地址。 （4）传输层 为会话层实体提供透明、可靠的数据传输服务，保证端到端的数据完整性；选择网络层的最适宜的服务；提供建 立、维护和拆除传输连接功能。传输层根据通信子网的特性，最佳的利用网络资源，为两个端系统的会话层之间提 供建立、维护和取消传输连接的功能，并以可靠和经济的方式传输数据。在这一层，信息的传送单位是报文。 （5）会话层 为彼此合作的表示层实体提供建立、维护和结束会话连接的功能；完成通信进程的逻辑名字与物理名字间的对应； 提供会话管理服务。 （6）表示层 为应用层进程提供能解释所交换信息含义的一组服务，即将欲交换的数据从适合于某一用户的抽象语法，转换为 适合于OSI系统内部使用的传送语法，提供格式化的表示和转换数据服务。数据的压缩，解压缩，加密和解密等 工作都由表示层负责。 （7）应用层 提供OSI用户服务，即确定进程之间通信的性质，以满足用户需要以及提供网络与用户应用软件之间的接口服务。 简言之，各层的作用： 物理层：在物理媒体上传输原始的数据比特流。 数据链路层：将数据分成一个个数据帧，以数据帧为单位传输。有应有答，遇错重发。 网络层：将数据分成一定长度的分组，将分组穿过通信子网，从信源选择路径后传到信宿。 传输层：提供不具体网络的高效、经济、透明的端到端数据传输服务。 会话层：进程间的对话也称为会话，会话层管理不同主机上各进程间的对话。 表示层： 为应用层进程提供格式化的表示和转换数据服务。 应用层：提供应用程序访问OSI环境的手段。 OSI网络体系结构各层协议： （1）应用层：TELNET、FTP、TFTP、SMTP、SNMP、HTTP、BOOTP、DHCP、DNS（2）表示层： 文本：ASCII，EBCDIC 图形：TIFF，JPEG，GIF，PICT 声音：MIDI，MPEG，QUICKTIME（3）会话层：NFS、SQL、RPC 、X-WINDOWS、ASP（APPTALK会话协议）、SCP（4）传输层：TCP、UDP、SPX（5）网络层：IP、IPX、ICMP、RIP、OSPF(Open Shortest Path First开放式最短路径优先)（6）数据链路层：SDLC、HDLC、PPP、STP（Spanning Tree Protocol）、帧中继（7）物理层：EIA/TIA RS-232、EIA/TIA RS-449、V.35、RJ-45 常用端口（每个协议都有一个缺省端口） TCP/IP协议模型TCP/IP概念层次中与OSI模型相互对应 两者关系OSI引入了服务、接口、协议、分层的概念，TCP/IP借鉴了OSI的这些概念建立TCP/IP模型。 OSI先有模型，后有协议，先有标准，后进行实践；而TCP/IP则相反，先有协议和应用再提出了模型，且是参照的OSI模型。 OSI是一种理论下的模型，而TCP/IP已被广泛使用，成为网络互联事实上的标准。 TCP：transmission control protocol 传输控制协议 UDP：user data protocol 用户数据报协议 OSI七层协议模型算是一个理想的规范模型而不应用在实践中，而TCP/IP模型则是现在国际所通用的网络传输协议模型 序列化 / 反序列化 属于 TCP/IP模型 应用层 和 OSI`模型 展示层的主要功能： （序列化）把 应用层的对象 转换成 二进制串 （反序列化）把 二进制串 转换成 应用层的对象所以， Protocol Buffer属于 TCP/IP模型的应用层 &amp; OSI模型的展示层 对于数据结构和对象数据结构、对象与二进制串不同的计算机语言中，数据结构，对象以及二进制串的表示方式并不相同。 对于面向对象的语言（如Java）：对象 = Object = 类的实例化；在Java中最接近数据结构 即 POJO（Plain Old Java Object），或Javabean（只有 setter/getter 方法的类） 对于半面向对象的语言（如C++），对象 = class，数据结构 = struct 对于C++，因为具有内存操作符，所以 二进制串 容易理解：C++的字符串可以直接被传输层使用，因为其本质上就是以 ‘\0’ 结尾的存储在内存中的二进制串 对于 Java，二进制串 = 字节数组 =byte[]byte 属于 Java 的八种基本数据类型二进制串 容易和 String混淆：String 一种特殊对象（Object）。对于跨语言间的通讯，序列化后的数据当然不能是某种语言的特殊数据类型。 T - L - V 的数据存储方式 定义 即 Tag - Length - Value，标识 - 长度 - 字段值 存储方式 作用以 标识 - 长度 - 字段值 表示单个数据，最终将所有数据拼接成一个 字节流，从而 实现 数据存储 的功能 其中 Length可选存储，如 储存Varint编码数据就不需要存储Length 示意图]]></content>
      <categories>
        <category>Google</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>java类库</tag>
        <tag>google</tag>
        <tag>protocol Buffer</tag>
        <tag>数据交换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--JDBC故障诊断]]></title>
    <url>%2F2018%2F03%2F15%2Fjava-api13%2F</url>
    <content type="text"><![CDATA[有不错的JDBC扩展库的存在使得调试变得很容易，例如P6spy，这是一个针对数据库访问操作的动态监测框架，它使得数据库数据可无缝截取和操纵，而不必对现有应用程序的代码作任何修改。P6Spy 分发包包括P6Log，它是一个可记录任何 Java 应用程序的所有JDBC事务的应用程序。其配置完成使用时，可以进行数据访问性能的监测。 在我们 Java 开发应用程序的过程中，难免会碰到系统的性能问题，特别在企业应用的开发过程中，都会与数据库进行打交道。当我们碰到数据库性能时，最有效的就是直接跟踪每一个 SQL 语句的执行情况，SQL 语句的优化、索引的优化往往也是最容易取得最直接的效果的。 在应用程序开发过程中，为了方便调试，通常都需要知道在DAO层程序执行的SQL是什么，而P6spy这个组件正是提供了该功能。 已在Github上开源：https://github.com/p6spy/p6spy 依赖添加 &lt;dependency&gt; &lt;groupId&gt;p6spy&lt;/groupId&gt; &lt;artifactId&gt;p6spy&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt; &lt;/dependency&gt; 然后将p6spy的配置文件spy.properties放置项目的src/main/resources目录下,该文件只需要修改logfile，logMessageFormat，dateformat属性即可，如下图所示: logfile=mybatis-practices-core.log //具体文件位置 logMessageFormat= net.ittimeline.mybatis.practices.core.p6spy.CustomizeLineFormat //具体文件信息格式 dateformat=yyyy-MM-dd HH:mm:ss //数据格式 databaseDialectDateFormat=yyyy-MM-dd HH:mm:ss //数据库方言数据格式 配置参数文档说明在 https://github.com/p6spy/p6spy/blob/master/docs/configandusage.md 中写的很清楚 然后实现自定义的SQL输出格式 为了输出的内容足够的简洁，这里只保留了当前时间，执行SQL的耗时以及执行的SQL语句,具体实现如下所示这里用了alibaba.druid的数据库连接池技术，具体大同小异。 package net.ittimeline.mybatis.practices.core.p6spy; import com.alibaba.druid.sql.SQLUtils; import com.p6spy.engine.spy.appender.MessageFormattingStrategy; /** * @author tony ittimeline@163.com * @date 2018-01-30-下午10:45 * @website wwww.ittimeline.net * @see * @since JDK8u162 */ public class CustomizeLineFormat implements MessageFormattingStrategy { public String buildMessage(String now, long elapsed, String sql) { StringBuffer content = new StringBuffer(); if (org.apache.commons.lang3.StringUtils.isNotEmpty(now) &amp;&amp; org.apache.commons.lang3.StringUtils.isNotEmpty(Long.valueOf(elapsed).toString()) &amp;&amp; org.apache.commons.lang3.StringUtils.isNotEmpty(sql)) { content.append(&quot;当前时间:&quot; + now); content.append(&quot; SQL执行耗时(毫秒)为&quot; + elapsed); content.append(&quot; SQL执行的语句是\n&quot; + SQLUtils.formatMySql(sql)+&quot;\n\n&quot;); } return content.toString(); } @Override public String formatMessage(int connectionId, String now, long elapsed, String category, String prepared, String sql) { return buildMessage(now, elapsed, sql); } } 然后增加一个database.properties文件，配置内容如下 和传统的jdbc配置相比，不同之处在于驱动类和连接地址的配置。 jdbc.driver=com.p6spy.engine.spy.P6SpyDriver jdbc.url=jdbc:p6spy:mysql://127.0.0.1:3306/mybatis?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false jdbc.username=koronto jdbc.password=xxxxxxxx 而mybatis-config.xml文件中只需要增加一行配置，然后就可以采用${属性名}的方式获取数据库配置了，配置如下所示 &lt;properties resource=&quot;database.properties&quot;/&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;&gt;&lt;/transactionManager&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;${jdbc.driver}&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;${jdbc.url}&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;${jdbc.username}&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 然后运行CountryMapperTest.selectAll()方法，会发现在mybatis-practices-core模块的根路径下存在一个mybatis-practices-core.log的文件，内容如下 当前时间:2018-01-30 23:03:07 SQL执行耗时(毫秒)为0 SQL执行的语句是 SELECT country_id AS countryId, country_name AS countryName, country_code AS countryCode FROM t_country 把具体的SQL语句信息给打印出来了。 在 P6Spy 发布包中，它包含 P6Log 和 P6Outage 两个模块： P6LogP6Log 是用来拦截和记录任务应用程序的 JDBC 语句的。这个功能对于开发者监控 EJB 服务器上的 SQL 语句执行情况尤其有用，可以让开发者完成尽可能高效的代码。同时 P6Spy 的部署是极其简单的，而且根本不需要更改任何一行代码，即对现有的应用是无侵入性的。 P6OutageP6Outage 专门用来检测和记录执行时间比较长的 SQL 语句，P6Outage 只记录超过配置条件里时间的那些信息，并对可能影响到数据库的运行效率减小到最低。 架构原理简单地讲，我们可以认为 P6Spy 就是一个代理（Proxy），它只做了一层对 JDBC 驱动的拦截，然后转发出去，这样的设计与实际的应用程序没有任何的耦合性，除了在配置中将驱动程序改成 P6Spy 的拦截驱动外，程序其他地方并不需要做任何的改变。这层拦截器除了可能会给系统带来略微的性能下降外，对程序其他方面没有任何的影响。而相对于这一点点的性能下降，在开发环境中对于开发人员来说是无法感觉到，相比它所带来的好处，完全可以忽略不计。 P6Spy 对数据库进行拦截监控的处理过程如下： 问题与解决如果在你的应用程序启动后，却在 spy.log 文件中发现了如下的提示信息，那就是驱动程序加载先后的问题了。 &lt;你的程序的数据库驱动名称&gt; is a real driver in spy.properties, but it has been loaded before p6spy. p6spy will not wrap these connections. Either prevent the driver from loading, or try setting&apos;deregisterdrivers&apos; to true in spy.properties 解决办法： 请把 spy.properties 配置文件里的 deregisterdrivers=false 改为 deregisterdrivers=true，重新运行即可。这是因为有些应用系统中会先于 P6Spy 加载了真正的数据库的驱动程序，导致 P6Spy 无法监控到，设置 deregisterdrivers 为 true，是显式地把真正的数据库的驱动程序进行反注册掉，而采用 P6Spy 的驱动程序。 配置参数及相关意义下表列出了 spy.properties 配置文件中的各配置项的名称、默认值及其意义和相关注意事项： 配置项名称 默认值 module.log com.p6spy.engine.logging. P6LogFactory module.outage com.p6spy.engine.outage. P6OutageFactory realdriver realdriver2 realdriver3 deregisterdrivers false executionthreshold outagedetection false outagedetectioninterval filter false include exclude sqlexpression autoflush true dateformat includecategories excludecategories stringmatcher stacktrace false stacktraceclass reloadproperties false reloadpropertiesinterval 60 useprefix false appender com.p6spy.engine.logging. appender.FileLogger logfile spy.log append true log4j.appender.STDOUT org.apache.log4j.ConsoleAppender log4j.appender.STDOUT.layout org.apache.log4j.PatternLayout log4j.appender.STDOUT. layout.ConversionPattern p6spy - %m%n log4j.logger.p6spy INFO,STDOUT realdatasource realdatasourceclass realdatasourceproperties jndicontextfactory jndicontextproviderurl jndicontextcustom SQL ProfilerSQL Profiler 是一个由 Jahia.org 提供的基于 P6Spy 引擎的快速剖析工具，用来统计 SQL 查询语句以便了解哪里是性能瓶颈，在哪里创建索引或者采取相应的办法才能提高效率，并且能根据 SQL 查询语句的情况帮你生成合适的索引脚本。 这个小工具可以实时地显示数据库查询的情况，通过集成的 SQL 解析器，在访问大多数表与列上面建立统计分析，并生成索引脚本。当然，其它的信息也会进行收集和显示，比如：单个数据库请求的时间、一类请求的时间以及所有请求的时间。因此，可以有效地通过视图的排序来检测数据的性能问题所在。这个工具对于大量的需要进行分析的请求是非常有用的，而不是人工一个个地去做分析。当你需要知道比如对相同的表和列进行访问但是采用不同的查询值时，这种分组的查询可以用建立在 ANTLR 上的 SQL 解析器进行分析。 使用步骤首先，你的应用系统同样也应当是基于数据库的，然后你需要去获取 SQL Profiler 相关的文件（在 参考资源 中可以找到下载链接，您可以直接下载软件包）。下面介绍 SQL Profiler 的安装与使用的详细操作过程：下载 SQL Profiler 的文件包进行安装；把 p6spy.jar 及 sqlprofiler.jar 放到 CLASSPATH 中，如果是 Web 应用程序则放在 YourWebApp/WEB-INF/lib/ 目录下；把 spy.properties 放到 CLASSPATH 目录下，如果是 Web 应用程序就放在 YourWebApp/WEB-INF/classess/ 目录下，注意不是 lib/ 目录；修改你应用系统中的数据库驱动名称为 P6Spy 的驱动程序名称 com.p6spy.engine.spy.P6SpyDriver 其它的全部使用默认值，暂时不用修改；打开 spy.properties 文件，把 realdriver 的值改为你的程序的数据库驱动名称；注意要先运行 java -jar sqlprofiler.jar 来启动 SQL Profiler，并成功看到启动界面；然后再启动你的应用程序或服务器，并开始进行正常的系统请求处理操作；这样就可以在 SQL Profiler 图形化的界面上看到结果并进行分析了。 参考资料：http://blog.csdn.net/heyeqingquan/article/details/71743814 分析结果经过一段时间的系统运行后，点击 Pause 按钮停止拦截，可以得到分析结果如下图： SQL Profiler 的分析结果 Profiler 视图 接着，可以切换到 Loggers 视图，这是 Lgger 视图的信息： SQL Profiler 的分析结果 Logger 视图 当然，也可以切换到 Analysis 视图，这是 Analysis 视图的分析结果信息： SQL Profiler 的分析结果 Analysis 视图 在经过分析后，我们可以直接通过 SQLProfiler 提交的保存按钮，直接导出应当进行数据库优化的建议的索引脚本，通过查看索引脚本，我们可以看到创建索引的详细 SQL 脚本，这样，我们就可以非常方便地进行数据库调优了。 问题与解决最后一个需要注意的问题就是需要先启动 SQLProfiler，然后再启动应用程序或者 Tomcat 等应用服务器。这是因为 SQLProfiler 默认使用的是 Log4j 的 SocketAppender，所以要先启动。否则，会因你的应用程序或应用服务器中的 Web 应用之类的因连接不到 Socket 的服务器（SQLProfiler 相当于 Socket 的服务器）而发生错误，可以通过 SQL Profiler 控制界面最下面的连接状态就可以知道是否有程序连接上来。 IronTrack SQLIronEye，一个专注于 JDBC 性能的监控和测试的开源项目，它包含有三个工具：IronEye SQL，IronEye Cache，IronTrack SQL。其中，IronEye SQL 用于监测 Java 应用和数据库服务器之间查询开销的时间，诊断在性能方面是否存在着相关问题，让开发人员在测试之前就能发现问题。IronEye 于 2003 年 10 月 1 日开始基于 Apache Software License 发布。IronEye SQL 这个轻量级的 Java 工具提供所有流动在数据库与应用程序之间的 SQL 统计信息并用多张图表展现，可以快速优化程序的性能。IronGrid 相对于 Continuous Integration 提出了 Continuous Performance 的概念，即在项目开发过程中随时关注性能问题，而不是传统的出了问题再解决的方案。IronGrid 在应用程序对数据库的操作上的 Continuous Performance 是通过 IronTrack SQL 进行体现的。IronTrack SQL 能通过对 JDBC 的包装来拦截应用程序对数据库的请求，完成性能监控。IronTrack SQL 的好处在于不需要修改任何代码或者在数据库端安装任何程序，只需要在测试时把依赖的 JDBC 替换就可以了。 使用步骤首先，你的应用系统同样也应当是基于数据库的，然后你需要去获取 IronTrack SQL 相关的文件（在 参考资源 中可以找到下载链接，您可以直接下载软件包）。下面介绍 IronTrack SQL 的安装与使用的详细操作过程：下载 IronTrack SQL 的文件包进行安装；把 irontracksql.jar, p6spy.jar 和 log4j-1.2.8.jar 放到 CLASSPATH 中，如果是 Web 应用程序则放在 YourWebApp/WEB-INF/lib/ 目录下；把 spy.properties 放到 CLASSPATH 目录下，如果是 Web 应用程序就放在 YourWebApp/WEB-INF/classess/ 目录下，注意不是 lib/ 目录；修改你程序的数据库驱动名称为 P6Spy 的驱动程序名称 com.p6spy.engine.spy.P6SpyDriver 其它的都不用更改；打开配置文件 spy.properties 文件，找到 realdriver，把它的值改为你的应用系统的真正的数据库驱动名称；设置监听端口号 monitorport=2000；先运行 java -jar irontracksql.jar 来启动 IronTrack SQL；再启动你的应用程序或服务器；可以在 IronTrack SQL 图形化的界面上看到结果并进行分析了。 连接设置点击“Config”按钮就可以设置主机名、端口与刷新的时间（毫秒为单位）。根据你的服务器与端口的不同而进行相应地改变，下面以本地和 2000 端口，刷新时间为 500 毫秒为示例。设置完成后，确定，点击“Connect”就可以连接应用系统并进行监测与分析了，当要停止分析时，只要点击“Disconnect”按扭即可立刻停止分析了。在分析的过程中，我们可以根据需要点击“Purge”按钮，它可以清除目前所监测到的内容，然后重新进行记录监测信息，很方便地进行重新开始。 IronTrack SQL 连接示例 分析结果经过一段时间的系统运行后，我们可以直接得到分析的结果与相应的图形分析示例。相关的信息显示如下： IronTrack SQL 分析结果 Count 列显示 SQL 语句的调用次数；Avg Time 列显示 SQL 语句的执行平均时间；Max Time 列显示 SQL 语句花费的最高时间；SQL 列显示真正执行的 SQL 语句内容。同时也可以通过设置过滤条件来显示指定条件的结果，比如：只关注平均调用次数大于 100 次 的结果。点击“Filtering”左边的小三角图标，可以显示如下的过滤条件设置栏目： IronTrack SQL 设置相关的过滤条件 设置完成后，点击“Apply Filter”按钮即可以获取所需要的相关结果了。这样可以更加方便地集中精力进行所需要的内容分析，可以更加方便快速地定位到问题的所在之处，然后进行解决。 总结通过使用 P6Spy、SQL Profiler、IronTrack SQL 工具，我们可以无侵入已有的应用系统而有效地进行数据库操作的监控与剖析，为发现系统的性能瓶颈，寻找系统的性能调优提供了相当便利的方法。 p6spy专区：http://www.p6spy.com/sqlprofiler专区：https://sourceforge.net/projects/sqlprofiler/https://www.ibm.com/developerworks/cn/java/]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--嵌入式SQL]]></title>
    <url>%2F2018%2F03%2F15%2Fjava-api12%2F</url>
    <content type="text"><![CDATA[嵌入式SQL嵌入式SQL(英文 Embedded SQL)是一种将SQL语句直接写入C语言，COBOL，FORTRAN, Ada等编程语言的源代码中的方法。借此方法，可使得应用程序拥有了访问数据以及处理数据的能力。在这一方法中，将SQL文嵌入的目标源码的语言称为宿主语言。 在SQL标准的SQL86(1986年发布)中定义了对于COBOL, FORTRAN（福传语言，即数学公式语言）, PI/L等语言的嵌入式SQL的规范。在SQL89(1989年发布)规范中，定义了对于C语言的嵌入式SQL的规范。一些大型的数据库厂商发布的数据库产品中，都提供了对于嵌入式SQL的支持。比如Oracle, DB2等。 工作原理提供对于嵌入式SQL的支持，需要数据库厂商除了提供DBMS之外，还必须提供一些工具。为了实现对于嵌入式SQL的支持，技术上必须解决以下4个问题: 1.宿主语言的编译器不可能识别和接受SQL文，需要解决如何将SQL的宿主语言源代码编译成可执行码;2.宿主语言的应用程序如何与DBMS之间传递数据和消息;3.如何把对数据的查询结果逐次赋值给宿主语言程序中的变量以供其处理;4.数据库的数据类型与宿主语言的数据类型有时不完全对应或等价，如何解决必要的数据类型转换问题。 嵌入式SQL源码的处理流程 为了解决上述这些问题，数据库厂商需要提供一个嵌入式SQL的预编译器，把包含有嵌入式SQL文的宿主语言源码转换成纯宿主语言的代码。这样一来，源码即可使用宿主语言对应的编译器进行编译。通常情况下，经过嵌入式SQL的预编译之后，原有的嵌入式SQL会被转换成一系列函数调用。因此，数据库厂商还需要提供一系列函数库，以确保链接器能够把代码中的函数调用与对应的实现链接起来。 扩展语法嵌入式SQL中除了可以执行标准SQL文之外，为了对应嵌入的需要，还增加了一些额外的语法成分。主要包含以下内容： 宿主变量使用声明的语法 数据库访问的语法 事务控制的语法 游标操作的语法 嵌入形式对宿主型数据库语言SQL，DBMS可以采用两种方法处理，一种是预编译，另一种是修改和扩充主语言使之能处理SQL语句。目前采用较多的是预编译的方法。即有DBMS的预处理程序对源程序进行扫描，识别出SQL语句，把它们转换成主语言调用语句，以使主语言编译程序能识别它，最后由主语言的编译程序将整个源程序编译成目标码。在嵌入式SQL中，为了能够区分SQL语句与主语言语句，所以SQL语句都必须加前缀EXEC SQL。SQL语句的结束标准则随主语言的不同而不同。例如：在PL/1和C中以分号（;）结束：EXEC SQL;在COBOL中以END-EXEC结束：EXEC SQL&lt;SQL语句&gt; END-EXEC例如一条交互形式的SQL语句：DROP TABLE Student;嵌入到C程序中应写作：EXEC SQL DROP TABLE Student; 可参考文章 http://blog.csdn.net/shaoshuo/article/details/2463632]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--HTML解析库]]></title>
    <url>%2F2018%2F03%2F15%2Fjava-api11%2F</url>
    <content type="text"><![CDATA[dom解析是常用dom4j。android中我们常用的是sax、pull。因为它们更省内存。而html解析,则也有很多框架 java的HTML解析通常用在利用java做网页的垂直爬虫 HTML是WEB的核心，互联网中你看到的所有页面都是HTML，不管它们是由JavaScript，JSP，PHP,ASP或者是别的什么WEB技术动态生成的。你的浏览器会去解析HTML并替你去渲染它们。不过如果你需要自己在Java程序中解析HTML文档并查找某些元素，标签，属性或者检查某个特定的元素是否存在的话，那又该如何呢？如果你已经使用Java编程多年了，我相信你肯定试过去解析XML，也使用过类似DOM或者SAX这样的解析器，不过很有可能你从未进行过任何的HTML解析的工作。更讽刺的是，在Java应用中，很少会有需要你去解析HTML文档的时候，这里并不包括Servlet或者其它的Java WEB技术。更糟糕的是，JDK核心里也没有包括HTTP或者HTML的库，至少我并不知道有这个。这就是为什么一碰上解析HTML文件时，许多Java程序员就得先Google一下 ，看看如何在Java中取出一个HTML的标签。当我有这个需要的时候，我相信肯定会有一些开源库能实现这个，不过我没有想到竟然有JSoup这么酷的并且功能齐全的库。它不仅能支持读取并解析HTML文档，而且还能让你从HTML文件抽取出任何的元素，以及它们的属性，它们的CSS属性，你还能进它们进行修改。有了JSoup你简直可以对HTML文档做任何事情。我们将会看到如何在Java中从google主页或者任何URL中下载并解析HTML文件的示例。 一般来说，一个爬虫包括几个部分： 页面下载 页面下载是一个爬虫的基础。下载页面之后才能进行其他后续操作。 链接提取 一般爬虫都会有一些初始的种子URL，但是这些URL对于爬虫是远远不够的。爬虫在爬页面的时候，需要不断发现新的链接。 URL管理 最基础的URL管理，就是对已经爬过的URL和没有爬的URL做区分，防止重复爬取。 内容分析和持久化 一般来说，我们最终需要的都不是原始的HTML页面。我们需要对爬到的页面进行分析，转化成结构化的数据，并存储下来。 不同的爬虫，对这几部分的要求是不一样的。 HTML分析是一个比较复杂的工作，Java世界主要有几款比较方便的分析工具： Jsoup Jsoup是一个集强大和便利于一体的HTML解析工具。它方便的地方是，可以用于支持用jquery中css selector的方式选取元素，这对于熟悉js的开发者来说基本没有学习成本。 Jsoup是一个开源的Java库，它可以用于处理实际应用中的HTML。它提供了非常便利的API来进行数据的提取及修改，充分利用了DOM，CSS以及jquery风格方法的长处。Jsoup实现了WAHTWG HTML5的规范，它从HTML解析出来的DOM和Chrome以及Firefox这样的现代浏览器解析出来的完全一致。下面是Jsoup库的一些有用的特性： 1.Jsoup可以从URL，文件，或者字符串中获取并解析HTML。2.Jsoup可以查找并提取数据，可以使用DOM遍历或者CSS选择器。3.你可以使用Jsoup来修改HTML元素，属性以及文本。4.Jsoup通过一个安全的白名单确保了用户提交的内容是干净的，以防止XSS攻击。5.Jsoup还能输出整洁的HTML。 在Java中使用Jsoup进行HTML解析 在这篇Java HTML解析的教程中，我们会看到在Java中使用Jsoup解析及遍历HTML的三个不同的示例。第一个例子中，我们会解析一个HTML字符串，它的内容就是Java中的字符串字面量组成的标签。第二个例子中，我们会从WEB中下载HTML文档，而第三个例子中，我们会加载一个HTML示例文件login.html来进行解析。这个文件是一个HTML文档的示例，它包含title标签,body里面有一个div标签，里面包含一个表单。它拥有input标签来用于获取用户名及密码，同时还有提交及重置的按钮用来进行下一步操作。它是一个正确有效的HTML，也就是说，所有的标签和属性都是正确地闭合的。下面是我们这个HTML的示例文件： 复制代码代码如下: &lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot; &quot;http://www.w3.org/TR/html4/loose.dtd&quot;&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=ISO-8859-1&quot;&gt; &lt;title&gt;Login Page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div id=&quot;login&quot; class=&quot;simple&quot; &gt; &lt;form action=&quot;login.do&quot;&gt; Username : &lt;input id=&quot;username&quot; type=&quot;text&quot; /&gt;&lt;br&gt; Password : &lt;input id=&quot;password&quot; type=&quot;password&quot; /&gt;&lt;br&gt; &lt;input id=&quot;submit&quot; type=&quot;submit&quot; /&gt; &lt;input id=&quot;reset&quot; type=&quot;reset&quot; /&gt; &lt;/form&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 使用Jsoup来解析HTML非常简单，你只需调用它的静态方法Jsoup.parse()并传入你的HTML字符串给它就可以了。Jsoup提供了多个重载的parse()方法，它可以从字符串，文件，URI，URL，甚至InputStream中读取HTML文件。如果不是UTF-8编码的话，你还可以指定字符编码，这样可以正确地读取HTML文件。下面是Jsoup库中HTML解析方法的一个完整的列表。parse(String html)方法将输入的HTML解析成一个新的Document。在Jsoup里，Document继承了Element，而它又继承自Node。同样的TextNode也继承自Node。只要你传入的是一个不为null的字符串，你就肯定能获取到一个成功的有意义的解析，得到一个包含head和body元素的Document。一旦你拿到这个Document，你就可以调用Document以及它的父类Element和Node上面的适当的方法来获取到你想要的数据了。 解析HTML文档的Java程序 下面是一个解析HTML字符串，网络上下载的HTML文件，以及本地文件系统中的HTML文件的完整的Java程序。你可以使用Eclipse IDE或者别的IDE甚至命令来运行这个程序。在Eclipse里面则很简单，拷贝这份代码，新建一个Java工程，在src包上右键并粘贴进去就可以了。Eclipse会去创建正确的包及同名的Java源文件的，因此工作量最小。如果你已经有一个Java示例工程了，那么仅需一步就可以了。下面的这个Java程序展示了解析及遍历HTML文件的三个不同例子。第一个例子中，我们直接解析了一个内容为HTML的字符串，第二个例子中我们解析了一个从URL中下载的HTML文件，第三个中我们从本地文件系统中加载了一个HTML文档并进行解析。第一和第三个例子中都用到了parse方法来获取一个Document对象，你可以查询它来提取出任何的标签值或者属性值。第二个例子中，我们用到了Jsoup.connect方法，它会去创建URL的连接，下载HTML并进行解析。这个方法也会返回Document，它可以用于后续的查询及获取标签或者属性的值。 复制代码代码如下: import java.io.IOException; import org.jsoup.Jsoup; import org.jsoup.nodes.Document; import org.jsoup.nodes.Element; /** [*] Java Program to parse/read HTML documents from File using Jsoup library. [*] Jsoup is an open source library which allows Java developer to parse HTML [*] files and extract elements, manipulate data, change style using DOM, CSS and [*] JQuery like method. [*] [*] @author Javin Paul [*]/ public class HTMLParser{ public static void main(String args[]) { // Parse HTML String using JSoup library String HTMLSTring = &quot;&lt;!DOCTYPE html&gt;&quot; + &quot;&lt;html&gt;&quot; + &quot;&lt;head&gt;&quot; + &quot;&lt;title&gt;JSoup Example&lt;/title&gt;&quot; + &quot;&lt;/head&gt;&quot; + &quot;&lt;body&gt;&quot; + &quot;|[b]HelloWorld[/b]&quot; + &quot;&quot; + &quot;&lt;/body&gt;&quot; + &quot;&lt;/html&gt;&quot;; Document html = Jsoup.parse(HTMLSTring); String title = html.title(); String h1 = html.body().getElementsByTag(&quot;h1&quot;).text(); System.out.println(&quot;Input HTML String to JSoup :&quot; + HTMLSTring); System.out.println(&quot;After parsing, Title : &quot; + title); System.out.println(&quot;Afte parsing, Heading : &quot; + h1); // JSoup Example 2 - Reading HTML page from URL Document doc; try { doc = Jsoup.connect(&quot;http://google.com/&quot;).get(); title = doc.title(); } catch (IOException e) { e.printStackTrace(); } System.out.println(&quot;Jsoup Can read HTML page from URL, title : &quot; + title); // JSoup Example 3 - Parsing an HTML file in Java //Document htmlFile = Jsoup.parse(&quot;login.html&quot;, &quot;ISO-8859-1&quot;); // wrong Document htmlFile = null; try { htmlFile = Jsoup.parse(new File(&quot;login.html&quot;), &quot;ISO-8859-1&quot;); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } // right title = htmlFile.title(); Element div = htmlFile.getElementById(&quot;login&quot;); String cssClass = div.className(); // getting class form HTML element System.out.println(&quot;Jsoup can also parse HTML file directly&quot;); System.out.println(&quot;title : &quot; + title); System.out.println(&quot;class of div tag : &quot; + cssClass); } } 输出： 复制代码代码如下: Input HTML String to JSoup : &lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;JSoup Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;h1&gt;HelloWorld&lt;/h1&gt;&lt;/tr&gt; &lt;/table&gt; &lt;/body&gt; &lt;/html&gt; After parsing, Title : JSoup Example Afte parsing, Heading : HelloWorld Jsoup Can read HTML page from URL, title : Google Jsoup can also parse HTML file directly title : Login Page class of div tag : simple Jsoup的好处就是它的健壮性很强。Jsoup HTML解析器会对你提供的HTML进行尽量干净的解析，而不去考虑这个HTML是否是格式良好的。它可以处理如下这些错误：未闭合的标签（比如，Java Scala to JavaScala)，隐式标签（比如，一个裸的|Java is Great被封装到了|里面），它总能创建出一个文档结构（包含head及body的HTML，并且head里只会包含正确的元素）。这就是在Java中如何进行HTML的解析。Jsoup是一个优秀的健壮的开源库，它使得读取HTML文档，body片段，HTML字符串，以及直接从WEB中解析HTML内容都变得相当简单。在这篇文章中，我们学习了如何在Java中获取一个特定的HTML标签，正如第一个例子中我们将title及H1标签的值提取成了文本，而第三个例子中我们学习到了如何通过提取CSS属性来从HTML标签中获取属性值。除了强大的jQuery风格的html.body().getElementsByTag(“h1”).text()方法，你还可以提取任意的HTML标签，它还提供了像Document.title()和Element.className()这样便捷的方法，你可以快速获取到标题及CSS类。希望JSoup能让你玩得愉快，很快我们将会看到关于这个API的更多的一些例子。 Jsoup的设计初衷是用于处理现实生活中出现的各种不同的HTML，包括正确有效的HTML以及不完整的无效的标签集合。Jsoup的一个核心竞争力就是它的健壮性。 转载自 http://www.jb51.net/article/55620.html 在线文档:http://www.osctools.net/apidocs/apidoc?api=jsoup-1.6.3;http://jsoup.org/ &lt;!-- lang: java --&gt; String content = &quot;blabla&quot;; Document doc = JSoup.parse(content); Elements links = doc.select(&quot;a[href]&quot;); Jsoup还支持白名单过滤机制，对于网站防止XSS攻击也是很好的。 HtmlParser HtmlParser的功能比较完备，也挺灵活，但谈不上方便。这个项目很久没有维护了，最新版本是2.1。HtmlParser的核心元素是Node，对应一个HTML标签，支持getChildren()等树状遍历方式。HtmlParser另外一个核心元素是NodeFilter，通过实现NodeFilter接口，可以对页面元素进行筛选。 htmlparser是一个纯的java写的html解析的库,它不依赖于其它的java库文件,主要用于改造或提取html。它能超高速解析html,而且不会出错。现在htmlparser最新版本为2.0。 据说htmlparser就是目前最好的html解析和分析的工具。 无论你是想抓取网页数据还是改造html的内容,用了htmlparser绝对会忍不住称赞。 在线文档:http://www.osctools.net/apidocs/apidoc?api=HTMLParser[/url];http://htmlparser.sourceforge.net/project-info.html 示例代码: Parser parser = new Parser (“http://www.dangdang.com&quot;); NodeList list = parser.parse (null); Node node = list.elementAt (0); NodeList sublist = node.getChildren (); System.out.println (sublist.size ()); Apache tika tika是专为抽取而生的工具，还支持PDF、Zip甚至是Java Class。使用tika分析HTML，需要自己定义一个抽取内容的Handler并继承org.xml.sax.helpers.DefaultHandler，解析方式就是xml标准的方式。crawler4j中就使用了tika作为解析工具。SAX这种流式的解析方式对于分析大文件很有用，我个人倒是认为对于解析html意义不是很大。 InputStream inputStream = null; HtmlParser htmlParser = new HtmlParser(); htmlParser.parse(new ByteArrayInputStream(page.getContentData()), contentHandler, metadata, new ParseContext()); HtmlCleaner与XPath HtmlCleaner最大的优点是：支持XPath的方式选取元素。XPath是一门在XML中查找信息的语言，也可以用于抽取HTML元素。XPath与CSS Selector大部分功能都是重合的，但是CSS Selector专门针对HTML，写法更简洁，而XPath则是通用的标准，可以精确到属性值。XPath有一定的学习成本，但是对经常需要编写爬虫的人来说，这点投入绝对是值得的。 学习XPath可以参考w3school的XPath 教程。http://www.w3school.com.cn/xpath/下面是使用HtmlCleaner和xpath进行抽取的一段代码： &lt;!-- lang: java --&gt; HtmlCleaner htmlCleaner = new HtmlCleaner(); TagNode tagNode = htmlCleaner.clean(text); Object[] objects = tagNode.evaluateXPath(&quot;xpathStr&quot;); 几个工具的对比 在这里评价这些工具的主要标准是“方便”。就拿抽取页面所有链接这一基本任务来说，几种代码分别如下： XPath: &lt;!-- lang: java --&gt; tagNode.evaluateXPath(&quot;//a/@href&quot;) CSS Selector: &lt;!-- lang: java --&gt; //使用类似js的实现 $(&quot;a[href]&quot;).attr(&quot;href&quot;) HtmlParser： &lt;!-- lang: java --&gt; Parser p = new Parser(value); NodeFilter aFilter = new TagNameFilter(&quot;a&quot;); NodeList nodes = p.extractAllNodesThatMatch(aFilter); for (int i = 0; i &lt; nodes.size(); i++) { Node eachNode = nodes.elementAt(i); if (eachNode instanceof LinkTag) { LinkTag linkTag = (LinkTag) eachNode; System.out.println(linkTag.extractLink()); } } XPath是最简单的，可以精确选取到href属性值；而CSS Selector则次之，可以选取到HTML标签，属性值需要调用函数去获取；而HtmlParser和SAX则需要手动写程序去处理标签了，比较麻烦。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--邮件]]></title>
    <url>%2F2018%2F03%2F15%2Fjava-api10%2F</url>
    <content type="text"><![CDATA[所想要看的java邮件相关知识在这里都有，因为作者的权限问题，不好转载。 http://blog.csdn.net/acmman/article/category/6843840]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--HTTP库]]></title>
    <url>%2F2018%2F03%2F15%2Fjava-api09%2F</url>
    <content type="text"><![CDATA[我不是很喜欢JDK的一个重要原因就包括他们缺乏对HTTP的支持。虽然可以使用java.net包类，但是这和直接使用像 Apache HttpClient 和 HttpCore 等开源类库比起来麻烦太多了。 尽管JDK 9将开始HTTP 2.0，也对HTTP的支持做了优化，但是我还是强烈建议所有的Java开发人员熟悉流行的HTTP处理类库，例如HttpClient和HttpCore HTTP等库。 传统的java.net包类InetAddress类:表示互联网协议 (IP) 地址 URL类:代表一个统一资源定位符，它是指向互联网“资源”的指针。 资源可以是简单的文件或目录，也可以是对更为复杂的对象的引用，例如对数据库或搜索引擎的查询。 URLConnection类:用于读取和写入此 URL 引用的资源。 使用步骤:通过在 URL 上调用 openConnection 方法创建连接对象。处理设置参数和一般请求属性。使用 connect 方法建立到远程对象的实际连接。远程对象变为可用。远程对象的头字段和内容变为可访问。 DatagramPacket类:此类表示数据报包。 数据报包用来实现无连接包投递服务。每条报文仅根据该包中包含的信息从一台机器路由到另一台机器。 DatagramSocket类:此类表示用来发送和接收数据报包的套接字。 数据报套接字是包投递服务的发送或接收点。每个在数据报套接字上发送或接收的包都是单独编址和路由的。从一台机器发送到另一台机器的多个包可能选择不同的路由，也可能按不同的顺序到达。 ServerSocket类:此类实现服务器套接字。 ServerSocket类主要用在服务器端程序的开发上，用于接收客户端的连接请求 服务器套接字等待请求通过网络传入。它基于该请求执行某些操作，然后可能向请求者返回结果。 Socket类:此类实现客户端套接字（也可以就叫“套接字”）。 则是在客户端上运行的，在服务器每次运行时都要使用accept()方法来获取客户端连接，此方法执行之后服务器端将进入阻塞状态，直到客户端连接之后才可以继续向下执行，此方法的返回值类型是Socket，每一个Socket都表示一个客户端对象 本包中的主线应该是Socket和ServalSocket两个类。将围绕socket和serverSocket的通信而展开。 首先，如何建立一个客户端和服务器端的通信？ 一、客户端 1.首先new一个socket（有连接的socket和未连接的socket两种：指定服务器端的ip地址（或主机名）与port号的为连接的，没指定或使用socket（Proxy proxy）的为未连接的）。 2.设置socket的各项参数：时间？（连接之后还能设置么？) 3.通过getInputputStream()和getOutputStream()方法得到InputStream和OutputStream流。 4.操作InputStream和OutputStream流读取或写入数据。 5.close 二、服务器端 1.首先new一个serverSocket（有绑定的serverSocket和未绑定的serverSocket两种：指定服务器端的Port号为绑定的，未指定的为未绑定的）。 2.设置serverSocket的各项参数：buffersize？ 3.调用accept()方法获得一个和客户端连接的Socket。 4.通过getInputputStream()和getOutputStream()方法得到InputStream和OutputStream流。 5.close 通过以上步骤我们基本上就建立起了客户端和服务器端的通信链路。然后我们讲讲关于Socket和ServerSocket参数设置的问题。 本文的第二个主线是DatagramSocket和DatagramPacket两个类。 首先，如何通过UDP协议建立一个客户端和服务器端的通信？ 一、客户端 1.首先new 一个DatagramSocket（有绑定的DatagramSocket和未绑定的DatagramSocket两种：需指定本地的ip地址和port号，DatagramSocket(SocketAddress localAddr)如果localAddr未null，则为未绑定）。 2,new一个DatagramPacket实例（需指定主机地址和端口号），填充数据，将其传递给DatagramSocket的send方法。 3.接收数据时将DatagramPacket实例传递给DatagramSocket的receive方法。 3,close 二、服务器端 其实本质和客户端一样的。 示例： import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.io.PrintWriter; import java.net.URL; import java.net.URLConnection; import java.util.List; import java.util.Map; /** * @Title SendRequestUtils.java * @Package com.pro.huanbao.common.utils * @author wanpu_ly * @dade 2017年10月13日 上午8:43:42 * @version V1.0 * 类说明: */ public class SendRequestUtils { /** * 向指定URL发送GET方法的请求 * * @param url * 发送请求的URL * @param param * 请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。 * @return URL 所代表远程资源的响应结果 */ public static String sendGet(String url, String param) { String result = &quot;&quot;; BufferedReader in = null; try { String urlNameString = url + &quot;?&quot; + param; if (param == null) { urlNameString = url; } URL realUrl = new URL(urlNameString); // 打开和URL之间的连接 URLConnection connection = realUrl.openConnection(); // 设置通用的请求属性 connection.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;); connection.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;); connection.setRequestProperty(&quot;user-agent&quot;, &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;); // 建立实际的连接 connection.connect(); // 获取所有响应头字段 Map&lt;String, List&lt;String&gt;&gt; map = connection.getHeaderFields(); // 遍历所有的响应头字段 for (String key : map.keySet()) { System.out.println(key + &quot;---&gt;&quot; + map.get(key)); } // 定义 BufferedReader输入流来读取URL的响应 in = new BufferedReader(new InputStreamReader( connection.getInputStream())); String line; while ((line = in.readLine()) != null) { result += line; } } catch (Exception e) { System.out.println(&quot;发送GET请求出现异常！&quot; + e); e.printStackTrace(); } // 使用finally块来关闭输入流 finally { try { if (in != null) { in.close(); } } catch (Exception e2) { e2.printStackTrace(); } } return result; } /** * 向指定 URL 发送POST方法的请求 * * @param url * 发送请求的 URL * @param param * 请求参数，请求参数应该是 name1=value1&amp;name2=value2 的形式。 * @return 所代表远程资源的响应结果 */ public static String sendPost(String url, String param) { PrintWriter out = null; BufferedReader in = null; String result = &quot;&quot;; try { URL realUrl = new URL(url); // 打开和URL之间的连接 URLConnection conn = realUrl.openConnection(); // 设置通用的请求属性 conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;); conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;); conn.setRequestProperty(&quot;user-agent&quot;, &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;); // 发送POST请求必须设置如下两行 conn.setDoOutput(true); conn.setDoInput(true); // 获取URLConnection对象对应的输出流 out = new PrintWriter(conn.getOutputStream()); // 发送请求参数 out.print(param); // flush输出流的缓冲 out.flush(); // 定义BufferedReader输入流来读取URL的响应 in = new BufferedReader( new InputStreamReader(conn.getInputStream())); String line; while ((line = in.readLine()) != null) { result += line; } } catch (Exception e) { System.out.println(&quot;发送 POST 请求出现异常！&quot;+e); e.printStackTrace(); } //使用finally块来关闭输出流、输入流 finally{ try{ if(out!=null){ out.close(); } if(in!=null){ in.close(); } } catch(IOException ex){ ex.printStackTrace(); } } return result; } } 不过由于java.net的局限性和性能上的不足，基本上开发互联网产品的时候不会用到 接下来介绍一些第三方流行的Http库 apache的httpclient工具类示例： package org.egg.utils; import org.apache.http.client.config.RequestConfig; import org.apache.http.client.methods.CloseableHttpResponse; import org.apache.http.client.methods.HttpGet; import org.apache.http.client.methods.HttpPost; import org.apache.http.entity.StringEntity; import org.apache.http.impl.client.CloseableHttpClient; import org.apache.http.impl.client.HttpClients; import org.apache.http.impl.conn.PoolingHttpClientConnectionManager; import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.nio.charset.Charset; /** * @author dataochen * @Description * @date: 2017/11/7 17:49 */ public class HttpRequestUtil { private static CloseableHttpClient httpClient; static { PoolingHttpClientConnectionManager cm = new PoolingHttpClientConnectionManager(); cm.setMaxTotal(100); cm.setDefaultMaxPerRoute(20); cm.setDefaultMaxPerRoute(50); httpClient = HttpClients.custom().setConnectionManager(cm).build(); } public static String get(String url) { CloseableHttpResponse response = null; BufferedReader in = null; String result = &quot;&quot;; try { HttpGet httpGet = new HttpGet(url); RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(30000).setConnectionRequestTimeout(30000).setSocketTimeout(30000).build(); httpGet.setConfig(requestConfig); httpGet.setConfig(requestConfig); httpGet.addHeader(&quot;Content-type&quot;, &quot;application/json; charset=utf-8&quot;); httpGet.setHeader(&quot;Accept&quot;, &quot;application/json&quot;); response = httpClient.execute(httpGet); in = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); StringBuffer sb = new StringBuffer(&quot;&quot;); String line = &quot;&quot;; String NL = System.getProperty(&quot;line.separator&quot;); while ((line = in.readLine()) != null) { sb.append(line + NL); } in.close(); result = sb.toString(); } catch (IOException e) { e.printStackTrace(); } finally { try { if (null != response) { response.close(); } } catch (IOException e) { e.printStackTrace(); } } return result; } public static String post(String url, String jsonString) { CloseableHttpResponse response = null; BufferedReader in = null; String result = &quot;&quot;; try { HttpPost httpPost = new HttpPost(url); RequestConfig requestConfig = RequestConfig.custom().setConnectTimeout(30000).setConnectionRequestTimeout(30000).setSocketTimeout(30000).build(); httpPost.setConfig(requestConfig); httpPost.setConfig(requestConfig); httpPost.addHeader(&quot;Content-type&quot;, &quot;application/json; charset=utf-8&quot;); httpPost.setHeader(&quot;Accept&quot;, &quot;application/json&quot;); httpPost.setEntity(new StringEntity(jsonString, Charset.forName(&quot;UTF-8&quot;))); response = httpClient.execute(httpPost); in = new BufferedReader(new InputStreamReader(response.getEntity().getContent())); StringBuffer sb = new StringBuffer(&quot;&quot;); String line = &quot;&quot;; String NL = System.getProperty(&quot;line.separator&quot;); while ((line = in.readLine()) != null) { sb.append(line + NL); } in.close(); result = sb.toString(); } catch (IOException e) { e.printStackTrace(); } finally { try { if (null != response) { response.close(); } } catch (IOException e) { e.printStackTrace(); } } return result; } } 代码所用jar包maven坐标： org.apache.httpcomponents httpclient 4.5.3 org.apache.httpcomponents httpcore 4.4.8 Http协议的重要性相信不用我多说了，HttpClient相比传统JDK自带的URLConnection，增加了易用性和灵活性（具体区别，日后我们再讨论），它不仅是客户端发送Http请求变得容易，而且也方便了开发人员测试接口（基于Http协议的），即提高了开发的效率，也方便提高代码的健壮性。因此熟练掌握HttpClient是很重要的必修内容，掌握HttpClient后，相信对于Http协议的了解会更加深入。 一、简介HttpClient是Apache Jakarta Common下的子项目，用来提供高效的、最新的、功能丰富的支持HTTP协议的客户端编程工具包，并且它支持HTTP协议最新的版本和建议。HttpClient已经应用在很多的项目中，比如Apache Jakarta上很著名的另外两个开源项目Cactus和HTMLUnit都使用了HttpClient。 二、特性 基于标准、纯净的java语言。实现了Http1.0和Http1.1 以可扩展的面向对象的结构实现了Http全部的方法（GET, POST, PUT, DELETE, HEAD, OPTIONS, and TRACE）。 支持HTTPS协议。 通过Http代理建立透明的连接。 利用CONNECT方法通过Http代理建立隧道的https连接。 Basic, Digest, NTLMv1, NTLMv2, NTLM2 Session, SNPNEGO/Kerberos认证方案。 插件式的自定义认证方案。 便携可靠的套接字工厂使它更容易的使用第三方解决方案。 连接管理器支持多线程应用。支持设置最大连接数，同时支持设置每个主机的最大连接数，发现并关闭过期的连接。 自动处理Set-Cookie中的Cookie。 插件式的自定义Cookie策略。 Request的输出流可以避免流中内容直接缓冲到socket服务器。 Response的输入流可以有效的从socket服务器直接读取相应内容。 在http1.0和http1.1中利用KeepAlive保持持久连接。 直接获取服务器发送的response code和 headers。 设置连接超时的能力。 实验性的支持http1.1 response caching。 源代码基于Apache License 可免费获取 三、使用方法 Mavn坐标Java代码 收藏代码 org.apache.httpcomponents httpclient 4.3.4 使用HttpClient发送请求、接收响应很简单，一般需要如下几步即可。 创建HttpClient对象。 创建请求方法的实例，并指定请求URL。如果需要发送GET请求，创建HttpGet对象；如果需要发送POST请求，创建HttpPost对象。 如果需要发送请求参数，可调用HttpGet、HttpPost共同的setParams(HetpParams params)方法来添加请求参数；对于HttpPost对象而言，也可调用setEntity(HttpEntity entity)方法来设置请求参数。 调用HttpClient对象的execute(HttpUriRequest request)发送请求，该方法返回一个HttpResponse。 调用HttpResponse的getAllHeaders()、getHeaders(String name)等方法可获取服务器的响应头；调用HttpResponse的getEntity()方法可获取HttpEntity对象，该对象包装了服务器的响应内容。程序可通过该对象获取服务器的响应内容。 释放连接。无论执行方法是否成功，都必须释放连接 四.post跟get请求示例Java代码 收藏代码 package com.ickes; import java.util.ArrayList; import java.util.List; import org.apache.http.HttpEntity; import org.apache.http.NameValuePair; import org.apache.http.client.entity.UrlEncodedFormEntity; import org.apache.http.client.methods.CloseableHttpResponse; import org.apache.http.client.methods.HttpGet; import org.apache.http.client.methods.HttpPost; import org.apache.http.entity.StringEntity; import org.apache.http.impl.client.CloseableHttpClient; import org.apache.http.impl.client.HttpClients; import org.apache.http.message.BasicNameValuePair; import org.apache.http.protocol.HTTP; import org.apache.http.util.EntityUtils; public class HttpClientDemo { public static void main(String[] args) throws Exception { get(); } /** * post方式提交json代码 * @throws Exception */ public static void postJson() throws Exception{ //创建默认的httpClient实例. CloseableHttpClient httpclient = null; //接收响应结果 CloseableHttpResponse response = null; try { //创建httppost httpclient = HttpClients.createDefault(); String url =&quot;http://192.168.16.36:8081/goSearch/gosuncn/deleteDocs.htm&quot;; HttpPost httpPost = new HttpPost(url); httpPost.addHeader(HTTP.CONTENT_TYPE,&quot;application/x-www-form-urlencoded&quot;); //参数 String json =&quot;{&apos;ids&apos;:[&apos;html1&apos;,&apos;html2&apos;]}&quot;; StringEntity se = new StringEntity(json); se.setContentEncoding(&quot;UTF-8&quot;); se.setContentType(&quot;application/json&quot;);//发送json需要设置contentType httpPost.setEntity(se); response = httpclient.execute(httpPost); //解析返结果 HttpEntity entity = response.getEntity(); if(entity != null){ String resStr = EntityUtils.toString(entity, &quot;UTF-8&quot;); System.out.println(resStr); } } catch (Exception e) { throw e; }finally{ httpclient.close(); response.close(); } } /** * post方式提交表单（模拟用户登录请求） * @throws Exception */ public static void postForm() throws Exception { // 创建默认的httpClient实例. CloseableHttpClient httpclient = null; //发送请求 CloseableHttpResponse response = null; try { httpclient = HttpClients.createDefault(); // 创建httppost String url= &quot;http://localhost:8080/search/ajx/user.htm&quot;; HttpPost httppost = new HttpPost(url); // 创建参数队列 List&lt;NameValuePair&gt; formparams = new ArrayList&lt;NameValuePair&gt;(); formparams.add(new BasicNameValuePair(&quot;username&quot;, &quot;admin&quot;)); formparams.add(new BasicNameValuePair(&quot;password&quot;, &quot;123456&quot;)); //参数转码 UrlEncodedFormEntity uefEntity = new UrlEncodedFormEntity(formparams, &quot;UTF-8&quot;); httppost.setEntity(uefEntity); response = httpclient.execute(httppost); HttpEntity entity = response.getEntity(); if (entity != null) { System.out.println(EntityUtils.toString(entity, &quot;UTF-8&quot;)); } //释放连接 } catch (Exception e) { throw e; }finally{ httpclient.close(); response.close(); } } /** * 发送 get请求 * @throws Exception */ public static void get() throws Exception { CloseableHttpClient httpclient = null; CloseableHttpResponse response = null; try { httpclient = HttpClients.createDefault(); // 创建httpget. HttpGet httpget = new HttpGet(&quot;http://www.baidu.com/&quot;); // 执行get请求. response = httpclient.execute(httpget); // 获取响应实体 HttpEntity entity = response.getEntity(); // 打印响应状态 System.out.println(response.getStatusLine().getStatusCode()); if (entity != null) { // 打印响应内容 System.out.println(&quot;Response content: &quot; + EntityUtils.toString(entity)); } } catch (Exception e) { throw e; }finally{ httpclient.close(); response.close(); } } } 五、SSL跟上传文件实例 Java代码 收藏代码 package com.test; import java.io.File; import java.io.FileInputStream; import java.io.IOException; import java.io.UnsupportedEncodingException; import java.security.KeyManagementException; import java.security.KeyStore; import java.security.KeyStoreException; import java.security.NoSuchAlgorithmException; import java.security.cert.CertificateException; import java.util.ArrayList; import java.util.List; import javax.net.ssl.SSLContext; import org.apache.http.HttpEntity; import org.apache.http.NameValuePair; import org.apache.http.ParseException; import org.apache.http.client.ClientProtocolException; import org.apache.http.client.entity.UrlEncodedFormEntity; import org.apache.http.client.methods.CloseableHttpResponse; import org.apache.http.client.methods.HttpGet; import org.apache.http.client.methods.HttpPost; import org.apache.http.conn.ssl.SSLConnectionSocketFactory; import org.apache.http.conn.ssl.SSLContexts; import org.apache.http.conn.ssl.TrustSelfSignedStrategy; import org.apache.http.entity.ContentType; import org.apache.http.entity.mime.MultipartEntityBuilder; import org.apache.http.entity.mime.content.FileBody; import org.apache.http.entity.mime.content.StringBody; import org.apache.http.impl.client.CloseableHttpClient; import org.apache.http.impl.client.HttpClients; import org.apache.http.message.BasicNameValuePair; import org.apache.http.util.EntityUtils; import org.junit.Test; public class HttpClientTest { @Test public void jUnitTest() { ssl(); } /** * HttpClient连接SSL */ public void ssl() { CloseableHttpClient httpclient = null; try { KeyStore trustStore = KeyStore.getInstance(KeyStore.getDefaultType()); FileInputStream instream = new FileInputStream(new File(&quot;d:\\tomcat.keystore&quot;)); try { // 加载keyStore d:\\tomcat.keystore trustStore.load(instream, &quot;123456&quot;.toCharArray()); } catch (CertificateException e) { e.printStackTrace(); } finally { try { instream.close(); } catch (Exception ignore) { } } // 相信自己的CA和所有自签名的证书 SSLContext sslcontext = SSLContexts.custom().loadTrustMaterial(trustStore, new TrustSelfSignedStrategy()).build(); // 只允许使用TLSv1协议 SSLConnectionSocketFactory sslsf = new SSLConnectionSocketFactory(sslcontext, new String[] { &quot;TLSv1&quot; }, null, SSLConnectionSocketFactory.BROWSER_COMPATIBLE_HOSTNAME_VERIFIER); httpclient = HttpClients.custom().setSSLSocketFactory(sslsf).build(); // 创建http请求(get方式) HttpGet httpget = new HttpGet(&quot;https://localhost:8443/myDemo/Ajax/serivceJ.action&quot;); System.out.println(&quot;executing request&quot; + httpget.getRequestLine()); CloseableHttpResponse response = httpclient.execute(httpget); try { HttpEntity entity = response.getEntity(); System.out.println(&quot;----------------------------------------&quot;); System.out.println(response.getStatusLine()); if (entity != null) { System.out.println(&quot;Response content length: &quot; + entity.getContentLength()); System.out.println(EntityUtils.toString(entity)); EntityUtils.consume(entity); } } finally { response.close(); } } catch (ParseException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } catch (KeyManagementException e) { e.printStackTrace(); } catch (NoSuchAlgorithmException e) { e.printStackTrace(); } catch (KeyStoreException e) { e.printStackTrace(); } finally { if (httpclient != null) { try { httpclient.close(); } catch (IOException e) { e.printStackTrace(); } } } } /** * 上传文件 */ public void upload() { CloseableHttpClient httpclient = HttpClients.createDefault(); try { HttpPost httppost = new HttpPost(&quot;http://localhost:8080/myDemo/Ajax/serivceFile.action&quot;); FileBody bin = new FileBody(new File(&quot;F:\\image\\sendpix0.jpg&quot;)); StringBody comment = new StringBody(&quot;A binary file of some kind&quot;, ContentType.TEXT_PLAIN); HttpEntity reqEntity = MultipartEntityBuilder.create().addPart(&quot;bin&quot;, bin).addPart(&quot;comment&quot;, comment).build(); httppost.setEntity(reqEntity); System.out.println(&quot;executing request &quot; + httppost.getRequestLine()); CloseableHttpResponse response = httpclient.execute(httppost); try { System.out.println(&quot;----------------------------------------&quot;); System.out.println(response.getStatusLine()); HttpEntity resEntity = response.getEntity(); if (resEntity != null) { System.out.println(&quot;Response content length: &quot; + resEntity.getContentLength()); } EntityUtils.consume(resEntity); } finally { response.close(); } } catch (ClientProtocolException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } finally { try { httpclient.close(); } catch (IOException e) { e.printStackTrace(); } } } } 本实例是采用HttpClient4.3最新版本。该版本与之前的代码写法风格相差较大，大家多留意下。 参考：http://blog.csdn.net/wangpeng047/article/details/19624529]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--消息传递库]]></title>
    <url>%2F2018%2F03%2F14%2Fjava-api08%2F</url>
    <content type="text"><![CDATA[像日志和数据库连接池一样，消息传递也是很多实际的Java项目中必备的。Java提供了JMS Java消息服务，但这不是JDK的一部分,你需要单独的引入jms.jar。类似地，如果您准备使用第三方消息传递协议， Tibco RV 是个不错的选择。 1、问： 什么是 Java 消息服务？答: Java 消息服务(Java Message Service，JMS) API 是一个用于访问企业消息传递系统的 API。是 Java 2 Platform, Enterprise（J2EE）的一部分。 2、目前流行的消息传送产品有哪些？答：目前流行的有ActiveMQ、IBM WebSphere MQ、SonicMQ等 3、什么时候可以用到java消息机制？答 （1）异构系统集成，整合现有资源，提高资源的利用率 （2）异步请求处理，减轻或消除系统瓶颈，提高用户生产率和系统的整体可伸缩性 （3）组件解偶，增加系统的灵活性 4、消息传送的两种模型发布/订阅模型客户端发送消息到一个名为主题（topic）的虚拟通道中，每个订阅该主题的消费者都会接收到每条消息的一个副本。点对点模型客户端通过队列（queue）这个虚拟通道来同步和异步发送、接收消息，发送到队列的消息只能被一个接收者所接收，即使有多个消费者时也只能有一个消费者处理消息 5、JMS包含哪些接口？JMS API可以分为3个主要部分：公共API、点对点API和发布/订阅API。在JMS1.1中，公共API可被用于向一个队列或一个主题发送消息，或从其中接收消息。点对点API专门用于使用队列的消息传送，而发布/订阅API则专门用于使用主题的消息传送。在JMS公共API内部，和发送和接收JMS消息有关的JMS API接口主要有7个： ConnectionFactory Destination Connection Session Message MessageProducer MessageConsumer 在这些公共接口中，ConnectionFactory和Destination必须使用JNDI（遵照JMS规范要求）从提供者处获得。其他接口则可以通过工厂方法在不同的API接口中创建。举例来说，一旦有了一个ConnectionFactory，就可以创建一个 Connection。一旦有了一个Connection，就可以创建一个Session。而一旦有了一个Session，就可以创建一个 Message、MessageProducer和MessageConsumer。这7个主要的JMS公共API接口之间的关系如图1-5所示。在JMS中，是Session对象保存着用于消息传送的事务性工作单元（transactional unit），而不是Connection对象。这和JDBC不同，JDBC中是Connection对象保存事务性工作单元。这就意味着在使用JMS时，一个应用程序通常只会有一个Connection对象，但是它可以有一个Session对象池。另外，还有和异常处理、消息优先级及消息持久性有关的其他接口 6、java消息分为哪些部分？消息头、消息属性、消息自身 7、消息过滤消息订阅者需要对消息进行过滤，否则订阅者就会接受到主题或队列的每一条消息，浪费了不必要的资源（CPU、内存等），而使用消息过滤技术，能让订阅者只接受它需要的消息。（消息过滤对于队列消费尤其重要，因为一个队列消费者消费消息后其他消费者就不再可用，此时如果不对消息进行过滤处理，这条消息就很可能被浪费掉）。消息选择器使用消息属性和消息头作为条件表达式的传送载体（消息体不能作为消息选择器的参考对象）。消息选择器由标识符、常量和比较运算符组成：例： 创建一个消息如下：TextMessage textMessage = Session.createTextMessage(); textMessage.setText(“mytestMsg”); textMessage.setStringProperty(“city”,”hangzhou”); textMessage.setStringProperty(“company”,”mycompany”); 这条消息中设置的消息属性名“city”和“company”代表消息选择器的标识符，”hangzhou”和”mycompany”代表常量 在消费端创建一个选择器： String selector = “city = ‘hangzhou’ AND company=’mycompany’”; QueueReceiver qReceiver = qSession.createReceiver(testQ,selector); 其中“=”和“AND”为比较运算符，其他常用比较运算符还有： 算数比较运算符（=、&gt;、&lt;、&lt;=、&lt;&gt;等） like运算符、BETWEEN运算符、IN运算符、IS NULL运算符等 8 、消息过滤 消息订阅者需要对消息进行过滤，否则订阅者就会接受到主题或队列的每一条消息，浪费了不必要的资源（ CPU、内存等），而使用消息过滤技术，能让订阅者只接受它需要的消息。（消息过滤对于队列消费尤其重要，因为一个队列消费者消费消息后其他消费者就不再可用，此时如果不对消息进行过滤处理，这条消息就很可能被浪费掉）。 消息选择器使用消息属性和消息头作为条件表达式的传送载体（消息体不能作为消息选择器的参考对象）。 消息选择器由标识符、常量和比较运算符组成： 例： 创建一个消息如下： TextMessage textMessage = Session.createTextMessage(); textMessage.setText(“mytestMsg”); textMessage.setStringProperty(“city”,”hangzhou”); textMessage.setStringProperty(“company”,”alibaba”); 这条消息中设置的消息属性名 “city” 和 “company” 代表消息选择器的标识符， ”hangzhou” 和 ”alibaba”代表常量 在消费端创建一个选择器： String selector = “city = ‘hangzhou’ AND company=’alibaba’ ”; QueueReceiver qReceiver = qSession.createReceiver(testQ,selector); 其中 “=” 和 “AND” 为比较运算符，其他常用比较运算符还有： 算数比较运算符（ = 、 &gt; 、 &lt; 、 &lt;= 、 &lt;&gt; 等） like 运算符、 BETWEEN 运算符、 IN 运算符、 IS NULL 运算符等 9 、消息传送的可靠性 在消息的传送过程中由于网络、软硬件故障等都会导致消息的发送失败， jms 为保证消息的传送定义了 3 条法则： （ 1 ）消息自主性，消息是自包含的自主性实体，当发送端发出这条消息后这条消息就不再受发送端的限制，它可以在多个进程间被多次发送。 （ 2 ）存储转发，当消息被标记位持久性消息时，就由 jms 提供者利用 “ 保存并转发 ” 机制，将消息保存在可信的介质上，防止发生故障时仍然可以正常恢复 （ 3 ）消息确认机制，服务器确认已经从发送端收到了消息，消费者则从确认从服务器接收了消息，对消息传送过程的监控，保证了消息的可靠传送。 消息确认的 3 种模式： 1 、 AUTO_ACKNOWLEDGE从消息生产者角度：发送消息后就开始阻塞，直到从消息服务器收到回复，期间如发生异常则认为消息未被传送 从消息服务器角度：非持久消息在接受到消息后通知生产者，并将消息存入内存，持久性消息在接受道消息后先存入磁盘，然后通知生产者 从消费者角度：接受到消息后就向服务器发送确认信息，如果服务器没有收到确认，会重新发送 2 、 CLIENT_ACKNOWLEDGE消费者可在处理完业务逻辑后在代码重显示调用 message.acknowledge() 通知 jms 提供者已成功接收道消息3 、 DUPS_ACKNOWLEDGE可将一条消息向同一目的地发送两次以上 这里介绍两个概念： （ 1 ）持久化消息：消息持久化就是在发送者将消息发送出去后，消息中心首先将消息存储到本地数据文件、内存数据库或者远程数据库等，然后试图将消息发送给接收 者，发送成功则将消息从存储中删除，失败则继续尝试。消息中心启动以后首先要检查制定的存储位置，如果有未发送成功的消息，则需要把消息发送出去。通过在消息头设置来实现：如 MessageProducer producer = session.createProducer(destination); producer.setDeliveryMode(DeliveryMode.PERSISTENT); (2) 持久订阅者和非持久订阅者：非持久订阅者是指某个订阅者由于某种原因停止运行，那么在停止运行期间发布到该订阅者所订阅主题的消息就会无法获得，持久订阅者则刚好相反，持久订阅者会接收发送到订阅主题的所有消息，无论订阅者是否正常运行，电子邮件就是类似的一个例子。 持久化消息和持久订阅者在服务器和消费者端之间的消息传送保证机制比较类似，但在有一种情况下他们还是存在区别的，对持久订阅者来说，当消息服务器向发送者发送确认消息之后，并为当前未运行的持久订阅者将消息保存到介质之间如果发送故障，该消息就会丢失，而持久化消息则是先保存消息道介质，然后才向发送者发送确认消息，所以不存在这个问题，因此严格来说持久化消息的可靠性会更高。 10 、事务性消息 jms 事务性保证了一组发送的消息或接收的消息要么全部成功要么全部失败，概念上和我们在java 中使用的jta 相似，但jms 事务是由jms 提供者来管理的，而不是jta 。 使用方法如下： // 此处用true ，表示使用事务性消息 Session session =connection.createSession(true, Session.AUTO_ACKNOWLEDGE); // 用来发送的3 个消息 MessageProducer sender = session.createProducer(“queue/testQueue”); try{ TextMessage message1 = session.createTextMessage(“ 要发送的消息1”); sender.send(message); TextMessage message2 = session.createTextMessage(“ 要发送的消息2”); sender.send(message); TextMessage message3 = session.createTextMessage(“ 要发送的消息3”); sender.send(message); session.commit(); }catch(Exception e){ try{ session.rollback(); }catch(JMSException e){ } }]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--字节码库]]></title>
    <url>%2F2018%2F03%2F14%2Fjava-api07%2F</url>
    <content type="text"><![CDATA[也许你写了无数行的代码，也许你能非常溜的使用高级语言，但是你未必了解那些高级语言的执行过程。例如大行其道的Java。 Java号称是一门“一次编译到处运行”的语言，但是我们对这句话的理解深度又有多少呢？从我们写的java文件到通过编译器编译成java字节码文件（也就是.class文件），这个过程是java编译过程；而我们的java虚拟机执行的就是字节码文件。不论该字节码文件来自何方，由哪种编译器编译，甚至是手写字节码文件，只要符合java虚拟机的规范，那么它就能够执行该字节码文件。那么本文主要讲讲java字节码文件相关知识。接下来我们通过具体的Demo来深入理解： 首先我们来写一个java源文件 上面是我们写的一个java程序，很简单，只有一个成员变量a以及一个方法testMethod() 。 接下来我们用javac命令或者ide工具将该java源文件编译成java字节码文件。 上图是编译好的字节码文件，我们可以看到一堆16进制的字节。如果你使用IDE去打开，也许看到的是已经被反编译的我们所熟悉的java代码，而这才是纯正的字节码，这也是我们今天需要讲的内容重点。 也许你会对这样一堆字节码感到头疼，不过没关系，我们慢慢试着你看懂它，或许有不一样的收获。在开始之前我们先来看一张图 这张图是一张java字节码的总览图，我们也就是按照上面的顺序来对字节码进行解读的。一共含有10部分，包含魔数，版本号，常量池等等，接下来我们按照顺序一步一步解读。 魔数从上面的总览图中我们知道前4个字节表示的是魔数，对应我们Demo的是 0XCAFE BABE。什么是魔数？魔数是用来区分文件类型的一种标志，一般都是用文件的前几个字节来表示。比如0XCAFE BABE表示的是class文件，那么有人会问，文件类型可以通过文件名后缀来判断啊？是的，但是文件名是可以修改的（包括后缀），那么为了保证文件的安全性，讲文件类型写在文件内部来保证不被篡改。从java的字节码文件类型我们看到，CAFE BABE翻译过来是咖啡宝贝之意，然后再看看java图标。 CAFE BABE = 咖啡。 版本号我们识别了文件类型之后，接下来要知道版本号。版本号含主版本号和次版本号，都是各占2个字节。在此Demo种为0X0000 0033。其中前面的0000是次版本号，后面的0033是主版本号。通过进制转换得到的是次版本号为0，主版本号为51。从oracle官方网站我们能够知道，51对应的正式jdk1.7，而其次版本为0，所以该文件的版本为1.7.0。如果需要验证，可以在用java –version命令输出版本号，或者修改编译目标版本–target重新编译，查看编译后的字节码文件版本号是否做了相应的修改。 至此，我们共了解了前8字节的含义，下面讲讲常量池相关内容。 常量池紧接着主版本号之后的就是常量池入口。常量池是Class文件中的资源仓库，在接下来的内容中我们会发现很多地方会涉及，如Class Name，Interfaces等。常量池中主要存储2大类常量：字面量和符号引用。字面量如文本字符串，java中声明为final的常量值等等，而符号引用如类和接口的全局限定名，字段的名称和描述符，方法的名称和描述符。 为什么需要类和接口的全局限定名呢？系统引用类或者接口的时候不是通过内存地址进行操作吗？这里大家仔细想想，java虚拟机在没有将类加载到内存的时候根本都没有分配内存地址，也就不存在对内存的操作，所以java虚拟机首先需要将类加载到虚拟机中，那么这个过程设计对类的定位（需要加载A包下的B类，不能加载到别的包下面的别的类中），所以需要通过全局限定名来判别唯一性。这就是为什么叫做全局，限定的意思，也就是唯一性。 在进行具体常量池分析之前，我们先来了解一下常量池的项目类型表： 上面的表中描述了11中数据类型的结构，其实在jdk1.7之后又增加了3种（CONSTANT_MethodHandle_info,CONSTANT_MethodType_info以及CONSTANT_InvokeDynamic_info)。这样算起来一共是14种。接下来我们按照Demo的字节码进行逐一翻译。 0x0015：由于常量池的数量不固定（n+2），所以需要在常量池的入口处放置一项u2类型的数据代表常量池数量。因此该16进制是21，表示有20项常量，索引范围为1~20。明明是21，为何是20呢？因为Class文件格式规定，设计者就讲第0项保留出来了，以备后患。从这里我们知道接下来我们需要翻译出20项常量。Constant #1 （一共有20个常量，这是第一个，以此类推…）0x0a-：从常量类型表中我们发现，第一个数据均是u1类型的tag，16进制的0a是十进制的10，对应表中的MethodRef_info。0x-00 04-：Class_info索引项#40x-00 11-：NameAndType索引项#17Constant #20x-09: FieldRef_info0x0003 :Class_info索引项#30x0012：NameAndType索引项#18Constant #30x07-: Class_info0x-00 13-: 全局限定名常量索引为#19Constant #40x-07 :Class_info0x0014:全局限定名常量索引为#20Constant #50x01:Utf-8_info0x-00 01-:字符串长度为1（选择接下来的一个字节长度转义）0x-61:”a”(十六进制转ASCII字符)Constant #60x01:Utf-8_info0x-00 01：字符串长度为10x-49:”I”Constant #70x01:Utf-8_info0x-00 06:字符串长度为60x-3c 696e 6974 3e-:”“Constant #80x01 :UTF-8_info0x0003:字符串长度为30x2829 56:”()V”Constant #90x-01:Utf-8_info0x0004：字符串长度为40x436f 6465:”Code”Constant #100x01:Utf-8_info0x00 0f:字符串长度为150x4c 696e 654e 756d 6265 7254 6162 6c65:”LineNumberTable”Constant #11ox01: Utf-8_info0x00 12字符串长度为180x-4c 6f63 616c 5661 7269 6162 6c65 5461 626c 65:”LocalVariableTable”Constant #120x01:Utf-8_info0x0004 字符串长度为40x7468 6973 :”this”Constant #130x01:Utf-8_info0x0f:字符串长度为150x4c 636f 6d2f 6465 6d6f 2f44 656d 6f3b:”Lcom/demo/Demo;”Constant #140x01:Utf-8_info0x00 0a:字符串长度为10ox74 6573 744d 6574 686f 64:”testMethod”Constant #150x01:Utf-8_info0x000a:字符串长度为100x536f 7572 6365 4669 6c65 :”SourceFile”Constant #160x01:Utf-8_info0x0009:字符串长度为90x-44 656d 6f2e 6a61 7661 :”Demo.java”Constant #170x0c :NameAndType_info0x0007:字段或者名字名称常量项索引#70x0008:字段或者方法描述符常量索引#8Constant #180x0c:NameAndType_info0x0005:字段或者名字名称常量项索引#50x0006:字段或者方法描述符常量索引#6Constant #190x01:Utf-8_info0x00 0d:字符串长度为130x63 6f6d 2f64 656d 6f2f 4465 6d6f:”com/demo/Demo”Constant #200x01:Utf-8_info0x00 10 :字符串长度为160x6a 6176 612f 6c61 6e67 2f4f 626a 6563 74 :”java/lang/Object”到这里为止我们解析了所有的常量。接下来是解析访问标志位。 Access_Flag 访问标志访问标志信息包括该Class文件是类还是接口，是否被定义成public，是否是abstract，如果是类，是否被声明成final。通过上面的源代码，我们知道该文件是类并且是public。 0x 00 21：是0x0020和0x0001的并集。其中0x0020这个标志值涉及到了字节码指令，后期会有专题对字节码指令进行讲解。 类索引 类索引用于确定类的全限定名0x00 03 表示引用第3个常量，同时第3个常量引用第19个常量，查找得”com/demo/Demo”。#3.#19 父类索引0x00 04 同理：#4.#20(java/lang/Object) 接口索引通过java_byte.jpeg图我们知道，这个接口有2+n个字节，前两个字节表示的是接口数量，后面跟着就是接口的表。我们这个类没有任何接口，所以应该是0000。果不其然，查找字节码文件得到的就是0000。 字段表集合字段表用于描述类和接口中声明的变量。这里的字段包含了类级别变量以及实例变量，但是不包括方法内部声明的局部变量。同样，接下来就是2+n个字段属性。我们只有一个属性a，按道理应该是0001。查找文件果不其然是0001。那么接下来我们要针对这样的字段进行解析。附上字段表结构图 0x00 02 ：访问标志为private（自行搜索字段访问标志）0x00 05 : 字段名称索引为#5，对应的是”a”0x 00 06 :描述符索引为#6，对应的是”I”0x 00 00 :属性表数量为0，因此没有属性表。tips:一些不太重要的表（字段，方法访问标志表）可以自行搜索，这里就不贴出来了，防止篇幅过大。 方法我们只有一个方法testMethod，按照道理应该前2个字节是0001。通过查找发现是0x00 02。这是什么原因，这代表着有2个方法呢？且继续看…… 上图是一张方法表结构图，按照这个图我们分析下面的字节码： 第1个方法： 0x00 01：访问标志 ACC_PUBLIC，表明该方法是public。（可自行搜索方法访问标志表）0x00 07：方法名索引为#7，对应的是”“0x00 08：方法描述符索引为#8，对应的是”()V”0x00 01：属性表数量为1(一个属性表)那么这里涉及到了属性表。什么是属性表呢？可以这么理解，它是为了描述一些专有信息的，上面的方法带有一张属性表。所有属性表的结构如下图：一个u2的属性名称索引，一个u2的属性长度加上属性长度的info。虚拟机规范预定义的属性有很多，比如Code，LineNumberTable，LocalVariableTable，SourceFile等等，这个网上可以搜索到。 按照上面的表结构解析得到下面信息：0x0009:名称索引为#9(“Code”)。0x000 00038：属性长度为56字节。那么接下来解析一个Code属性表，按照下图解析 前面6个字节（名称索引2字节+属性长度4字节）已经解析过了，所以接下来就是解析剩下的56-6=50字节即可。0x00 02 ：max_stack=20x00 01 : max_locals=10x00 0000 0a : code_length=100x2a b700 012a 04b5 0002 b1 : 这是code代码，可以通过虚拟机字节码指令进行查找。2a=aload_0(将第一个引用变量推送到栈顶)b7=invokespecial(调用父类构造方法)00=什么都不做01 =将null推送到栈顶2a=同上04=iconst_1 将int型1推送到栈顶b5=putfield 为指定的类的实例变量赋值00= 同上02=iconst_m1 将int型-1推送栈顶b1=return 从当前方法返回void整理，去除无动作指令得到下面0 : aload_01 : invokespecial4 : aload_05 : iconst_16 : putfield9 : return关于虚拟机字节码指令这块内容，后期会继续深入下去…… 目前只需要了解即可。接下来顺着Code属性表继续解析下去:0x00 00 : exception_table_length=00x00 02 : attributes_count=2(Code属性表内部还含有2个属性表)0x00 0a: 第一个属性表是”LineNumberTable” 0x00 0000 0a : “属性长度为10”0x00 02 ：line_number_table_length=2line_number_table是一个数量为line_number_table_length，类型为line_number_info的集合，line_number_info表包括了start_pc和line_number两个u2类型的数据项，前者是字节码行号，后者是Java源码行号0x00 00 : start_pc =00x00 03 : end_pc =30x00 04 : start_pc=40x00 04 : end_pc=4 0x00 0b 第二个属性表是：”LocalVariableTable” local_variable_table.pnglocal_variable_info.png0x00 0000 0c：属性长度为120x00 01 : local_variable_table_length=1然后按照local_variable_info表结构进行解析：0x00 00 : start_pc=00x00 0a：length=100x000c : name_index=”this”0x000d : descriptor_index #13 (“Lcom/demo/Demo”)0000 index=0//——-到这里第一个方法就解析完成了——-//Method（)–1个属性Code表-2个属性表（LineNumberTable ，LocalVariableTable）接下来解析第二个方法 第2个方法： 0x00 04：”protected”0x00 0e: #14（”testMethod”）0x00 08 : “()V”0x0001 ： 属性数量=10x0009 ：”Code”0x0000 002b 属性长度为43解析一个Code表0000 :max_stack =00001 : max_local =10000 0001 : code_length =10xb1 : return(该方法返回void)0x0000 异常表长度=00x0002 属性表长度为2//第一个属性表0x000a : #10，LineNumberTable0x0000 0006 : 属性长度为60x0001 : line_number_length = 10x0000 : start_pc =00x0008 : end_pc =8//第二个属性表0x000b : #11 ，LocalVariableTable0x0000 000c : 属性长度为120x0001 : local_variable_table_length =10x0000 :start_pc = 00x0001: length = 10x000c : name_index =#12 “this”0x000d : 描述索引#13 “Lcom/demo/Demo;”0000 index=0 //到这里为止，方法解析都完成了，回过头看看顶部解析顺序图，我们接下来就要解析Attributes了。 Attribute0x0001 ：同样的，表示有1个Attributes了。0x000f : #15(“SourceFile”)0x0000 0002 attribute_length=20x0010 : sourcefile_index = #16(“Demo.java”)SourceFile属性用来记录生成该Class文件的源码文件名称。 另话其实，我们写了这么多确实很麻烦，不过这种过程自己体验一遍的所获所得还是不同的。现在，使用java自带的反编译器来解析字节码文件。javap -verbose Demo //不用带后缀.class 总结到此为止，讲解完成了class文件的解析，这样以后我们也能看懂字节码文件了。了解class文件的结构对后面进一步了解虚拟机执行引擎非常重要，所以这是基础并重要的一步。 作者：小腊月链接：https://www.jianshu.com/p/252f381a6bc4來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 以上是java字节码的介绍属于JVM底层的知识 接下来介绍的是java操作字节库的类库有哪些及相关操作 字节码操作1.Java动态性的两种常见实现方式： 字节码操作反射 2.运行时操作字节码可以实现如下功能： 动态生成新的类动态改变某个类的结构(添加/删除/修改 新的属性/方法) 3.优势： 比反射开销小，性能高Javaasist性能高于反射，低于ASM 常见的字节码操作类库1.BCEL Byte Code Engineering Library(BCEL)，这是Apache Software Foundation的Jakarta项目的一部分。BCEL是Java classworking 广泛使用的一种框架，它可以让您深入jvm汇编语言进行类库操作的细节。BCEL与javassist有不同的处理字节码方法，BCEL在实际的jvm指令层次上进行操作(BCEL拥有丰富的jvm指令集支持) 而javassist所强调的是源代码级别的工作。 2.ASM 是一个轻量级Java字节码操作框架，直接涉及到JVM底层的操作和指令高性能，高质量 3.CGLB(code generation library) 生成类库，基于ASM实现 4.javassist 是一个开源的分析，编辑和创建Java字节码的类库。性能较ASM差，跟cglib差不多，但是使用简单。很多开源框架都在使用它。主页： http://www.csg.ci.i.u-tokyo.ac.jp/~chiba/javassist/ javassist库的API详解javassist的最外层的API和java的反射包中的API及其类似 它主要有CtClass, CtMethod,CtField几个类组成，用于执行和JDK反射API中java.lang.Class, java.lang,reflect.Method,java.lang.reflect.Method.Field相同的操作 创建一个类 public class JavassistTest { public static void main(String[] args) throws Exception { ClassPool pool = ClassPool.getDefault(); CtClass cc = pool.makeClass(&quot;bean.User&quot;); //创建属性 CtField field01 = CtField.make(&quot;private int id;&quot;,cc); CtField field02 = CtField.make(&quot;private String name;&quot;, cc); cc.addField(field01); cc.addField(field02); //创建方法 CtMethod method01 = CtMethod.make(&quot;public String getName(){return name;}&quot;, cc); CtMethod method02 = CtMethod.make(&quot;public void setName(String name){this.name = name;}&quot;, cc); cc.addMethod(method01); cc.addMethod(method02); //添加有参构造器 CtConstructor constructor = new CtConstructor(new CtClass[]{CtClass.intType,pool.get(&quot;java.lang.String&quot;)},cc); constructor.setBody(&quot;{this.id=id;this.name=name;}&quot;); cc.addConstructor(constructor); //无参构造器 CtConstructor cons = new CtConstructor(null,cc); cons.setBody(&quot;{}&quot;); cc.addConstructor(cons); cc.writeFile(&quot;E:/workspace/TestCompiler/src&quot;); } } 方法操作 修改已有的方法体（插入代码到已有方法体）新增方法删除方法 $0 $1 $2 $0代表是this, $1代表方法参数的第一个参数，$2代表方法参数的第二个参数，以此类推，$N代表方法参数的第N个$args The type of $args is OBject[]. $args(0)对应的是$1，不是$0$$ 一个方法调用的深度$r 方法返回值的类型$_ 方法返回值。(修改方法体时不支持)addCatch() 方法中加入try catch块 $e代表 异常对象$class this的类型(Class)。也就是$0的类型$sig 方法参数的类型(Class)数组，数组的顺序。 构造方法操作 getConstructors() 注解操作 public @interface Author{ String name(); int year(); } @Author(name=&quot;over&quot;,year=2012) public class Point{int x,int y;} CtClass cc=ClassPool.getDefault().get(&quot;Point&quot;); Object[] all = cc.getAnnotations(); Author a =(Author)all[0]; String name = a.name(); int year = a.year(); System.out.println(name+&quot;:&quot;+year); 局限性JDK5.0新语法不支持(包括泛型，枚举)，不支持注解修改，但可以通过底层的javassist类来解决，具体参考：javassist,bytecode.annotation不支持数组的初始化，如String[]{“a”,”b”},除非只有数组的容量为1不支持内部类和匿名类不支持continue和break 表达式对于继承关系，有些不支持 。例如 class A{} class B extends A{} Class C extends B{} 查资料： javassist与反射的性能比较 public class Demo01 { //获取类的简单信息 public static void test01() throws Exception{ ClassPool pool = ClassPool.getDefault(); CtClass cc = pool.get(&quot;bean.User&quot;); //得到字节码 byte[] bytes = cc.toBytecode(); System.out.println(Arrays.toString(bytes)); System.out.println(cc.getName());//获取类名 System.out.println(cc.getSimpleName());//获取简要类名 System.out.println(cc.getSuperclass());//获取父类 System.out.println(cc.getInterfaces());//获取接口 System.out.println(cc.getMethods());//获取 } //新生成一个方法 public static void test02() throws Exception{ ClassPool pool = ClassPool.getDefault(); CtClass cc = pool.get(&quot;bean.User&quot;); //第一种 //CtMethod cm = CtMethod.make(&quot;public String getName(){return name;}&quot;, cc); //第二种 //参数：返回值类型，方法名，参数，对象 CtMethod cm = new CtMethod(CtClass.intType,&quot;add&quot;,new CtClass[]{CtClass.intType,CtClass.intType},cc); cm.setModifiers(Modifier.PUBLIC);//访问范围 cm.setBody(&quot;{return $1+$2;}&quot;); //cc.removeMethod(m) 删除一个方法 cc.addMethod(cm); //通过反射调用方法 Class clazz = cc.toClass(); Object obj = clazz.newInstance();//通过调用无参构造器，生成新的对象 Method m = clazz.getDeclaredMethod(&quot;add&quot;, int.class,int.class); Object result = m.invoke(obj, 2,3); System.out.println(result); } //修改已有的方法 public static void test03() throws Exception{ ClassPool pool = ClassPool.getDefault(); CtClass cc = pool.get(&quot;bean.User&quot;); CtMethod cm = cc.getDeclaredMethod(&quot;hello&quot;,new CtClass[]{pool.get(&quot;java.lang.String&quot;)}); cm.insertBefore(&quot;System.out.println(\&quot;调用前\&quot;);&quot;);//调用前 cm.insertAt(29, &quot;System.out.println(\&quot;29\&quot;);&quot;);//行号 cm.insertAfter(&quot;System.out.println(\&quot;调用后\&quot;);&quot;);//调用后 //通过反射调用方法 Class clazz = cc.toClass(); Object obj = clazz.newInstance(); Method m = clazz.getDeclaredMethod(&quot;hello&quot;, String.class); Object result = m.invoke(obj, &quot;张三&quot;); System.out.println(result); } //修改已有属性 public static void test04() throws Exception{ ClassPool pool = ClassPool.getDefault(); CtClass cc = pool.get(&quot;bean.User&quot;); //属性 CtField cf = new CtField(CtClass.intType,&quot;age&quot;,cc); cf.setModifiers(Modifier.PRIVATE); cc.addField(cf); //增加响应的get set方法 cc.addMethod(CtNewMethod.getter(&quot;getAge&quot;,cf)); cc.addMethod(CtNewMethod.setter(&quot;setAge&quot;,cf)); //访问属性 Class clazz = cc.toClass(); Object obj = clazz.newInstance(); Field field = clazz.getDeclaredField(&quot;age&quot;); System.out.println(field); Method m = clazz.getDeclaredMethod(&quot;setAge&quot;, int.class); m.invoke(obj, 16); Method m2 = clazz.getDeclaredMethod(&quot;getAge&quot;, null); Object resutl = m2.invoke(obj,null); System.out.println(resutl); } //操作构造方法 public static void test05() throws Exception{ ClassPool pool = ClassPool.getDefault(); CtClass cc = pool.get(&quot;bean.User&quot;); CtConstructor[] cons = cc.getConstructors(); for(CtConstructor con:cons){ System.out.println(con); } } public static void main(String[] args) throws Exception { //test01(); //test02(); //test03(); //test04(); test05(); } }]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--常用类库]]></title>
    <url>%2F2018%2F03%2F14%2Fjava-api06%2F</url>
    <content type="text"><![CDATA[Java 类库概念：Java 的应用程序接口 (API) 以包的形式来组织，每个包提供了大量的相关类、接口和异常处理类，这些包的集合就是 Java 的类库 包名以 Java 开始的包是 Java 核心包 (Java Core Package) ； 包名以 Javax 开始的包是 Java 扩展包 (Java Extension Package) ，例如 javax.swing 包； 常用的 Java 核心包 (Java Core Package) java.lang Java 编程语言的基本类库 java.applet 创建 applet 需要的所有类 java.awt 创建用户界面以及绘制和管理图形、图像的类 java.io 通过数据流、对象序列以及文件系统实现的系统输入、输出 java.net 用于实现网络通讯应用的所有类 java.util 集合类、时间处理模式、日期时间工具等各类常用工具包 其它还有 java.sql 访问和处理来自于 Java 标准数据源数据的类 java.test 以一种独立于自然语言的方式处理文本、日期、数字和消息的类和接口 java.security 设计网络安全方案需要的一些类 java.beans 开发 Java Beans 需要的所有类 java.math 简明的整数算术以及十进制算术的基本函数 java.rmi 与远程方法调用相关的所有类，rmi即Remote Method Invoke远程方法调用 常用的 Java 扩展包 (Java Extension Package) javax.accessibility 定义了用户界面组件之间相互访问的一种机制 javax.naming.* 为命名服务提供了一系列类和接口 javax.swing.* 提供了一系列轻量级的用户界面组件，是目前 Java 用户界面常用的包 注 1 ：最重要且常用的是 1 和 6 ，已用黑体标出的为，需重点掌握 注 2 ：在使用 Java 时，除了 java.lang 外，其他的包都需要 import 语句引入之后才能使用。 重点讲解内容：java.lang和java.util。具体各方法在开发文档里写的很清楚 java.lang 包 这个包称为 java 语言包，是由编译器自动引入的。程序中不必用 import 语句就可以使用。它所包含的类和接口对所有实际的 Java 程序都是必要的。 object 类 数学类 (Math) 数据类型类 线程类 字符串类 (String 类和 StringBuffer 类 ) 系统及运行类 (System 类和 Runtime 类 ) 错误和异常处理类 (Throwable 、 Exception 、 Error) 过程类 (process) java.util 包 日期类、日历类（ Data 、 Calendar 、 GregorianCalendar ） 随机数类（ Random ） 位运算类（ BitSet ） 矢量类（ Vector ） 数据结构类（ Stack ） 散列表类（ Hashtable ） StringTokenizer类]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--日期和时间库]]></title>
    <url>%2F2018%2F03%2F13%2Fjava-api05%2F</url>
    <content type="text"><![CDATA[在Java之前，JDK的日期和时间库一直被人们所诟病，比如其非线程安全的、不可变的、容易出错等。很多开发人员会选择更好用的 JodaTime 类库。但是在Java8推出之后，我们就可以彻底放弃JodaTime了，因为Java 8提供了其所有功能。但是，如果你的代码运行在一个低版本的JDK中，那么JodaTime还是值得使用的。 Java 8 新特性： Java 类库的新特性之日期时间API (Date/Time API )文章来自：http://blog.csdn.net/sun_promise/article/details/51383618 Java8之前java.util.Date和Calendar类的弊端1）最开始的时候，Date既要承载日期信息，又要做日期之间的转换，还要做不同日期格式的显示，职责较繁杂（不遵守单一职责）。 后来从JDK 1.1 开始，这三项职责分开了： 使用Calendar类实现日期和时间字段之间转换；使用DateFormat类来格式化和分析日期字符串；Date只用来承载日期和时间信息。现在原有Date中的相应方法已废弃。无论是Date，还是Calendar，都使用着太不方便，这是API没有设计好的地方。 2）令人无语的year和month（month是从0开始的） eg： [java] view plain copy Date date = new Date(2016,1,1); System.out.println(date); 输出结果：Tue Feb 01 00:00:00 CST 3916这样得到的结果year为2012+1900，而month明明给定的参数是1，却输出的是二月。设置日期可以用java.util.Calendar [java] view plain copy Calendar calendar = Calendar.getInstance(); calendar.set(2016, 5, 2); 虽然Calendar年份的传值不需要减去1900，但Calendar的month也是从0开始的，表达5月份应该用4这个数字。3）java.util.Date与java.util.Calendar中的所有属性都是可变的，且线程不安全。 eg： [java] view plain copy public class Test { public static void main(String[] args) { Calendar birth = Calendar.getInstance(); birth.set(1975, Calendar.MAY, 26); Calendar now = Calendar.getInstance(); System.out.println(daysBetween(birth, now)); // 输出结果为14963，值不固定 System.out.println(daysBetween(birth, now)); // 输出结果 显示 0？ } } public static long daysBetween(Calendar begin, Calendar end) { long daysBetween = 0; while(begin.before(end)) { begin.add(Calendar.DAY_OF_MONTH, 1); daysBetween++; } return daysBetween; } } Note:daysBetween有点问题，如果连续计算两个Date实例的话，第二次会取得0，因为Calendar状态是可变的，考虑到重复计算的场合，最好复制一个新的Calendar。修改代码如下 [java] view plain copy public static long daysBetween(Calendar begin, Calendar end) { Calendar calendar = (Calendar) begin.clone(); // 复制 long daysBetween = 0; while(calendar.before(end)) { calendar.add(Calendar.DAY_OF_MONTH, 1); daysBetween++; } return daysBetween; } 新的日期时间APIJava 的日期与时间 API 问题由来已久，Java 8 之前的版本中关于时间、日期及其他时间日期格式化类由于线程安全、重量级、序列化成本高等问题而饱受批评。Java 8 吸收了 Joda-Time 的精华，以一个新的开始为 Java 创建优秀的 API。新的 java.time 中包含了所有关于时钟（Clock），本地日期（LocalDate）、本地时间（LocalTime）、本地日期时间（LocalDateTime）、时区（ZonedDateTime）和持续时间（Duration）的类。历史悠久的 Date 类新增了 toInstant() 方法，用于把 Date 转换成新的表示形式。这些新增的本地化时间日期 API 大大简化了了日期时间和本地化的管理。 目前Java8新增了java.time包定义的类表示日期-时间概念的规则，很方便使用；最重要的一点是值不可变，且线程安全。 下图是java.time包下的一些主要的类的日期时间 值的格式，方便理解使用： Note：不过尽管有了新的API，但仍有一个严重的问题——大量的旧代码和库仍然在使用老的API。现在，Java 8解决了这个问题，它给Date类增加了一个新的方法toInstant()，可以将Date转化成新的实例。这样就可以切换到新的API。 对于新API： 非常有用的值类型：Instant —– 与java.util.Date相似ZonedDateTime —– ZoneId -时区很重要的时候使用OffsetDateTime —– OffsetTime, ZoneOffset -对UTC的偏移处理Duration, Period —– 但如果你想找到两个日期之间的时间量，你可能会寻找ChronoUnit代替（详情见下文）其他有用的类型：DateTimeFormatter —– 将日期类型转换成字符串类型ChronoUnit —– 计算出两点之间的时间量，例如ChronoUnit.DAYS.between(t1, t2)TemporalAdjuster —– 例如date.with(TemporalAdjuster.firstDayOfMonth())Note：大多数情况下，新的值类型由JDBC提供支持。有一小部分异常，eg：ZonedDateTime在SQL中没有对应的（类型）。 Java 新旧日期API的区别 java.time包下的类Clock类 Clock类提供了访问当前日期和时间的方法。Clock使用时区来访问当前的instant, date和time。Clock类可以替换 System.currentTimeMillis() 和 TimeZone.getDefault()。 eg： [java] view plain copy //Clock 时钟 Clock clock1 = Clock.systemDefaultZone();//获取系统默认时区 (当前瞬时时间 ) System.out.println( &quot;系统时间日期：&quot;+clock1.instant() ); System.out.println( &quot;时间毫秒：&quot;+clock1.millis() ); final Clock clock = Clock.systemUTC();//获取系统时钟，并将其转换成使用UTC时区的日期和时间 System.out.println( &quot;时间日期：&quot;+clock.instant() ); System.out.println( &quot;时间毫秒值：&quot;+clock.millis() ); 输出结果：系统时间日期：2016-05-12T07:42:37.883Z时间毫秒：1463038957894时间日期：2016-05-12T07:42:37.894Z 时间毫秒值：1463038957894 某一个特定的时间点也可以使用Instant类来表示，Instant类也可以用来创建老的java.util.Date对象。 eg： [java] view plain copy Instant instant = clock1.instant(); Date javadate = Date.from(instant); System.out.println( &quot;date：&quot;+javadate); 输出结果： date：Thu May 12 15:47:00 CST 2016 ZoneId（时区） 在新API中时区使用ZoneId来表示。时区可以很方便的使用静态方法of()来获取到。时区定义了到UTS时间的时间差，在Instant时间点对象到本地日期对象之间转换的时候是极其重要的。 eg: [java] view plain copy // 输出所有可见的时区ID，eg：Asia/Aden, America/Cuiaba, Etc/GMT+9等 System.out.println(ZoneId.getAvailableZoneIds()); ZoneId zone1 = ZoneId.of(&quot;Europe/Berlin&quot;); ZoneId zone2 = ZoneId.of(&quot;Brazil/East&quot;); System.out.println(zone1.getRules()); System.out.println(zone2.getRules()); //输出结果： ZoneRules[currentStandardOffset=+01:00] //输出结果： ZoneRules[currentStandardOffset=-03:00] LocalTime（本地时间） LocalTime 定义了一个没有时区信息的时间。 eg： 1）获取现在的本地时间 [java] view plain copy // Get the local date and local time final LocalTime time = LocalTime.now(); final LocalTime timeFromClock = LocalTime.now( clock ); System.out.println( time ); System.out.println( timeFromClock ); 输出结果：16:03:23.21208:03:23.212 2）按时区显示时间 [java] view plain copy ZoneId zone1 = ZoneId.of(&quot;Europe/Berlin&quot;); ZoneId zone2 = ZoneId.of(&quot;Brazil/East&quot;); LocalTime now1 = LocalTime.now(zone1); LocalTime now2 = LocalTime.now(zone2); System.out.println(&quot;时区：Europe/Berlin---&quot;+now1); System.out.println(&quot;时区：Brazil/East---&quot;+now2); 输出结果： 时区：Europe/Berlin—10:03:23.217时区：Brazil/East—05:03:23.217 LocalTime 提供了多种工厂方法来简化对象的创建，包括解析时间字符串。 eg： [java] view plain copy LocalTime late = LocalTime.of(22, 12, 18);//时分秒 System.out.println(late); // 输出结果：22:12:18 DateTimeFormatter germanFormatter = DateTimeFormatter.ofLocalizedTime(FormatStyle.SHORT) .withLocale(Locale.GERMAN); LocalTime leetTime = LocalTime.parse(&quot;15:39&quot;, germanFormatter); System.out.println(leetTime); // 输出结果： 15:39 LocalDate(本地日期) LocalDate 表示了一个确切的日期（eg： 2014-03-11）。该对象值是不可变的，使用方式和LocalTime基本一致。 eg: [java] view plain copy Clock clock = Clock.systemDefaultZone();// 获取系统默认时区 (当前瞬时时间 ) // Get the local date and local time final LocalDate date = LocalDate.now(); final LocalDate dateFromClock = LocalDate.now(clock); System.out.println(date); System.out.println(dateFromClock); 输出结果:2016-05-122016-05-12 从字符串解析一个LocalDate类型和解析LocalTime一样简单. eg： [java] view plain copy DateTimeFormatter germanFormatter = DateTimeFormatter.ofLocalizedDate(FormatStyle.MEDIUM) .withLocale(Locale.GERMAN); LocalDate xmas = LocalDate.parse(&quot;25.10.2016&quot;, germanFormatter); System.out.println(xmas); 输出结果：2016-10-25 LocalDateTime（本地日期时间） 表示了具体时间和日期。LocalDateTime和LocalTime还有LocalDate一样，都是不可变的。LocalDateTime提供了一些能访问具体字段的方法。 eg： 1） [java] view plain copy Clock clock = Clock.systemDefaultZone();// 获取系统默认时区 (当前瞬时时间 ) // Get the local date/time final LocalDateTime datetime = LocalDateTime.now(); final LocalDateTime datetimeFromClock = LocalDateTime.now(clock); System.out.println(datetime); System.out.println(datetimeFromClock); 输出结果：2016-05-12T16:33:17.5462016-05-12T16:33:17.546 2） [java] view plain copy LocalDateTime sylvester = LocalDateTime.of(2016, Month.DECEMBER, 31, 23, 59, 59); DayOfWeek dayOfWeek = sylvester.getDayOfWeek(); System.out.println(dayOfWeek); Month month = sylvester.getMonth(); System.out.println(month); long minuteOfDay = sylvester.getLong(ChronoField.MINUTE_OF_DAY); System.out.println(minuteOfDay); 输出结果： SATURDAYDECEMBER1439 只要附加上时区信息，就可以将其转换为一个时间点Instant对象，Instant时间点对象可以很容易的转换为老式的java.util.Date。 eg： [java] view plain copy LocalDateTime sylvester = LocalDateTime.of(2016, Month.DECEMBER, 31, 23, 59, 59); Instant instant = sylvester .atZone(ZoneId.systemDefault()) .toInstant(); Date legacyDate = Date.from(instant); System.out.println(legacyDate); 输出结果： Sat Dec 31 23:59:59 CST 2016 格式化LocalDateTime和格式化时间和日期一样的，除了使用预定义好的格式外，我也可以自定义格式。 eg: [java] view plain copy DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;MM dd, yyyy - HH:mm&quot;); LocalDateTime parsed = LocalDateTime.parse(&quot;05 03, 2016 - 07:13&quot;, formatter); String string = formatter.format(parsed); System.out.println(string); 输出结果：05 03, 2016 - 07:13 Note:和java.text.NumberFormat不一样的是新版的DateTimeFormatter是不可变的，所以它是线程安全的。 ZonedDateTime（日期时间和时区信息） 使用ZonedDateTime，它保存有ISO-8601日期系统的日期和时间，而且有时区信息。 eg： [java] view plain copy Clock clock = Clock.systemDefaultZone();// 获取系统默认时区 (当前瞬时时间 ) // Get the zoned date/time final ZonedDateTime zonedDatetime = ZonedDateTime.now(); final ZonedDateTime zonedDatetimeFromClock = ZonedDateTime.now(clock); final ZonedDateTime zonedDatetimeFromZone = ZonedDateTime.now(ZoneId.of(&quot;America/Los_Angeles&quot;)); System.out.println(zonedDatetime); System.out.println(zonedDatetimeFromClock); System.out.println(zonedDatetimeFromZone); 输出结果： 2016-05-12T16:59:55.779+08:00[Asia/Shanghai]2016-05-12T16:59:55.779+08:00[Asia/Shanghai]2016-05-12T01:59:55.781-07:00[America/Los_Angeles] Duration类 Duration持有的时间精确到纳秒。很容易计算两个日期中间的差异。 eg：求时间差 [java] view plain copy // Get duration between two dates final LocalDateTime from = LocalDateTime.of(2014, Month.APRIL, 16, 0, 0, 0);//年月日时分秒 final LocalDateTime to = LocalDateTime.of(2015, Month.APRIL, 16, 23, 59, 59); final Duration duration = Duration.between(from, to); System.out.println(&quot;Duration in days: &quot; + duration.toDays()); System.out.println(&quot;Duration in hours: &quot; + duration.toHours()); 新日期时间API示例import java.time.Clock; import java.time.Duration; import java.time.Instant; import java.time.LocalDateTime; import java.time.ZoneId; import java.time.ZonedDateTime; import java.time.chrono.ChronoLocalDateTime; import java.time.chrono.Chronology; import java.time.chrono.HijrahChronology; import java.time.format.DateTimeFormatter; import java.time.temporal.IsoFields; import java.util.Date; public class TimeTest { public static void main(String[] args) throws InterruptedException { testClock(); // testInstant(); // testLocalDateTime(); // testZonedDateTime(); // testDuration(); // testChronology(); // testNewOldDateConversion(); } public static void testClock() throws InterruptedException { // 时钟提供给我们用于访问某个特定 时区的 瞬时时间、日期 和 时间的。 Clock c1 = Clock.systemUTC(); // 系统默认UTC时钟（当前瞬时时间 System.out.println(c1.millis()); // 每次调用将返回当前瞬时时间（UTC） //相当于System.currentTimeMillis()） Clock c2 = Clock.systemDefaultZone(); // 系统默认时区时钟（当前瞬时时间） Clock c31 = Clock.system(ZoneId.of(&quot;Europe/Paris&quot;)); // 巴黎时区 System.out.println(c31.instant()); // 每次调用将返回当前瞬时时间（UTC） Clock c32 = Clock.system(ZoneId.of(&quot;Asia/Shanghai&quot;));// 上海时区 System.out.println(c32.instant());// 每次调用将返回当前瞬时时间（UTC） Clock c4 = Clock.fixed(Instant.now(), ZoneId.of(&quot;Asia/Shanghai&quot;));// 固定上海时区时钟 System.out.println(c4.millis()); Thread.sleep(1000); System.out.println(c4.millis()); // 不变 即时钟时钟在那一个点不动 Clock c5 = Clock.offset(c1, Duration.ofSeconds(2)); // 相对于系统默认时钟两秒的时钟 System.out.println(c1.millis()); System.out.println(c5.millis()); } public static void testInstant() { // 瞬时时间 相当于以前的System.currentTimeMillis() Instant instant1 = Instant.now(); System.out.println(instant1.getEpochSecond());// 精确到秒 得到相对于1970-01-01 // 00:00:00 UTC的一个时间 System.out.println(instant1.toEpochMilli()); // 精确到毫秒 Clock clock1 = Clock.systemUTC(); // 获取系统UTC默认时钟 Instant instant2 = Instant.now(clock1);// 得到时钟的瞬时时间 System.out.println(instant2.toEpochMilli()); Clock clock2 = Clock.fixed(instant1, ZoneId.systemDefault()); // 固定瞬时时间时钟 Instant instant3 = Instant.now(clock2);// 得到时钟的瞬时时间 System.out.println(instant3.toEpochMilli());// equals instant1 } public static void testLocalDateTime() { // 使用默认时区时钟瞬时时间创建 Clock.systemDefaultZone() --&gt;即相对于 // ZoneId.systemDefault()默认时区 LocalDateTime now = LocalDateTime.now(); System.out.println(now); // 自定义时区 LocalDateTime now2 = LocalDateTime.now(ZoneId.of(&quot;Europe/Paris&quot;)); System.out.println(now2);// 会以相应的时区显示日期 // 自定义时钟 Clock clock = Clock.system(ZoneId.of(&quot;Asia/Dhaka&quot;)); LocalDateTime now3 = LocalDateTime.now(clock); System.out.println(now3);// 会以相应的时区显示日期 // 不需要写什么相对时间 如java.util.Date 年是相对于1900 月是从0开始 // 2013-12-31 23:59 LocalDateTime d1 = LocalDateTime.of(2013, 12, 31, 23, 59); // 年月日 时分秒 纳秒 LocalDateTime d2 = LocalDateTime.of(2013, 12, 31, 23, 59, 59, 11); // 使用瞬时时间 + 时区 Instant instant = Instant.now(); LocalDateTime d3 = LocalDateTime.ofInstant(Instant.now(), ZoneId.systemDefault()); System.out.println(d3); // 解析String---&gt;LocalDateTime LocalDateTime d4 = LocalDateTime.parse(&quot;2013-12-31T23:59&quot;); System.out.println(d4); LocalDateTime d5 = LocalDateTime.parse(&quot;2013-12-31T23:59:59.999&quot;);// 999毫秒 // 等价于999000000纳秒 System.out.println(d5); // 使用DateTimeFormatter API 解析 和 格式化 DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;yyyy/MM/dd HH:mm:ss&quot;); LocalDateTime d6 = LocalDateTime.parse(&quot;2013/12/31 23:59:59&quot;, formatter); System.out.println(formatter.format(d6)); // 时间获取 System.out.println(d6.getYear()); System.out.println(d6.getMonth()); System.out.println(d6.getDayOfYear()); System.out.println(d6.getDayOfMonth()); System.out.println(d6.getDayOfWeek()); System.out.println(d6.getHour()); System.out.println(d6.getMinute()); System.out.println(d6.getSecond()); System.out.println(d6.getNano()); // 时间增减 LocalDateTime d7 = d6.minusDays(1); LocalDateTime d8 = d7.plus(1, IsoFields.QUARTER_YEARS); // LocalDate 即年月日 无时分秒 // LocalTime即时分秒 无年月日 // API和LocalDateTime类似就不演示了 } public static void testZonedDateTime() { // 即带有时区的date-time 存储纳秒、时区和时差（避免与本地date-time歧义）。 // API和LocalDateTime类似，只是多了时差(如2013-12-20T10:35:50.711+08:00[Asia/Shanghai]) ZonedDateTime now = ZonedDateTime.now(); System.out.println(now); ZonedDateTime now2 = ZonedDateTime.now(ZoneId.of(&quot;Europe/Paris&quot;)); System.out.println(now2); // 其他的用法也是类似的 就不介绍了 ZonedDateTime z1 = ZonedDateTime.parse(&quot;2013-12-31T23:59:59Z[Europe/Paris]&quot;); System.out.println(z1); } public static void testDuration() { // 表示两个瞬时时间的时间段 Duration d1 = Duration.between(Instant.ofEpochMilli(System.currentTimeMillis() - 12323123), Instant.now()); // 得到相应的时差 System.out.println(d1.toDays()); System.out.println(d1.toHours()); System.out.println(d1.toMinutes()); System.out.println(d1.toMillis()); System.out.println(d1.toNanos()); // 1天时差 类似的还有如ofHours() Duration d2 = Duration.ofDays(1); System.out.println(d2.toDays()); } public static void testChronology() { // 提供对java.util.Calendar的替换，提供对年历系统的支持 Chronology c = HijrahChronology.INSTANCE; ChronoLocalDateTime d = c.localDateTime(LocalDateTime.now()); System.out.println(d); } /** * 新旧日期转换 */ public static void testNewOldDateConversion() { Instant instant = new Date().toInstant(); Date date = Date.from(instant); System.out.println(instant); System.out.println(date); } }]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--PDF处理库]]></title>
    <url>%2F2018%2F03%2F13%2Fjava-api04%2F</url>
    <content type="text"><![CDATA[iTextiText是一个能够快速产生PDF文件的java类库。iText的java类对于那些要产生包含文本，表格，图形的只读文档是很有用的。它的类库尤其与java Servlet有很好的给合。使用iText与PDF能够使你正确的控制Servlet的输出。该项目主页:http://www.lowagie.com/iText/ iText是著名的开放项目，是用于生成PDF文档的一个java类库。通过iText不仅可以生成PDF或rtf的文档，而且可以将XML、Html文件转化为PDF文件。 Harness the power of PDF with iText Easy PDF generation and manipulation for Java and .NET developers 生成一个PDF//Step 1—Create a Document. Document document = new Document(); //Step 2—Get a PdfWriter instance. PdfWriter.getInstance(document, new FileOutputStream(FILE_DIR + &quot;createSamplePDF.pdf&quot;)); //Step 3—Open the Document. document.open(); //Step 4—Add content. document.add(new Paragraph(&quot;Hello World&quot;)); //Step 5—Close the Document. document.close(); 页面大小,页面背景色,页边空白,Title,Author,Subject,Keywords//页面大小 Rectangle rect = new Rectangle(PageSize.B5.rotate()); //页面背景色 rect.setBackgroundColor(BaseColor.ORANGE); Document doc = new Document(rect); PdfWriter writer = PdfWriter.getInstance(doc, out); //PDF版本(默认1.4) writer.setPdfVersion(PdfWriter.PDF_VERSION_1_2); //文档属性 doc.addTitle(&quot;Title@sample&quot;); doc.addAuthor(&quot;Author@rensanning&quot;); doc.addSubject(&quot;Subject@iText sample&quot;); doc.addKeywords(&quot;Keywords@iText&quot;); doc.addCreator(&quot;Creator@iText&quot;); //页边空白 doc.setMargins(10, 20, 30, 40); doc.open(); doc.add(new Paragraph(&quot;Hello World&quot;)); 设置密码PdfWriter writer = PdfWriter.getInstance(doc, out); // 设置密码为：&quot;World&quot; writer.setEncryption(&quot;Hello&quot;.getBytes(), &quot;World&quot;.getBytes(), PdfWriter.ALLOW_SCREENREADERS, PdfWriter.STANDARD_ENCRYPTION_128); doc.open(); doc.add(new Paragraph(&quot;Hello World&quot;)); 添加Pagedocument.open(); document.add(new Paragraph(&quot;First page&quot;)); document.add(new Paragraph(Document.getVersion())); document.newPage(); writer.setPageEmpty(false); document.newPage(); document.add(new Paragraph(&quot;New page&quot;)); 添加水印(背景图)//图片水印 PdfReader reader = new PdfReader(FILE_DIR + &quot;setWatermark.pdf&quot;); PdfStamper stamp = new PdfStamper(reader, new FileOutputStream(FILE_DIR + &quot;setWatermark2.pdf&quot;)); Image img = Image.getInstance(&quot;resource/watermark.jpg&quot;); img.setAbsolutePosition(200, 400); PdfContentByte under = stamp.getUnderContent(1); under.addImage(img); //文字水印 PdfContentByte over = stamp.getOverContent(2); over.beginText(); BaseFont bf = BaseFont.createFont(BaseFont.HELVETICA, BaseFont.WINANSI, BaseFont.EMBEDDED); over.setFontAndSize(bf, 18); over.setTextMatrix(30, 30); over.showTextAligned(Element.ALIGN_LEFT, &quot;DUPLICATE&quot;, 230, 430, 45); over.endText(); //背景图 Image img2 = Image.getInstance(&quot;resource/test.jpg&quot;); img2.setAbsolutePosition(0, 0); PdfContentByte under2 = stamp.getUnderContent(3); under2.addImage(img2); stamp.close(); reader.close(); 插入Chunk, Phrase, Paragraph, List//Chunk对象: a String, a Font, and some attributes document.add(new Chunk(&quot;China&quot;)); document.add(new Chunk(&quot; &quot;)); Font font = new Font(Font.FontFamily.HELVETICA, 6, Font.BOLD, BaseColor.WHITE); Chunk id = new Chunk(&quot;chinese&quot;, font); id.setBackground(BaseColor.BLACK, 1f, 0.5f, 1f, 1.5f); id.setTextRise(6); document.add(id); document.add(Chunk.NEWLINE); document.add(new Chunk(&quot;Japan&quot;)); document.add(new Chunk(&quot; &quot;)); Font font2 = new Font(Font.FontFamily.HELVETICA, 6, Font.BOLD, BaseColor.WHITE); Chunk id2 = new Chunk(&quot;japanese&quot;, font2); id2.setBackground(BaseColor.BLACK, 1f, 0.5f, 1f, 1.5f); id2.setTextRise(6); id2.setUnderline(0.2f, -2f); document.add(id2); document.add(Chunk.NEWLINE); //Phrase对象: a List of Chunks with leading document.newPage(); document.add(new Phrase(&quot;Phrase page&quot;)); Phrase director = new Phrase(); Chunk name = new Chunk(&quot;China&quot;); name.setUnderline(0.2f, -2f); director.add(name); director.add(new Chunk(&quot;,&quot;)); director.add(new Chunk(&quot; &quot;)); director.add(new Chunk(&quot;chinese&quot;)); director.setLeading(24); document.add(director); Phrase director2 = new Phrase(); Chunk name2 = new Chunk(&quot;Japan&quot;); name2.setUnderline(0.2f, -2f); director2.add(name2); director2.add(new Chunk(&quot;,&quot;)); director2.add(new Chunk(&quot; &quot;)); director2.add(new Chunk(&quot;japanese&quot;)); director2.setLeading(24); document.add(director2); //Paragraph对象: a Phrase with extra properties and a newline document.newPage(); document.add(new Paragraph(&quot;Paragraph page&quot;)); Paragraph info = new Paragraph(); info.add(new Chunk(&quot;China &quot;)); info.add(new Chunk(&quot;chinese&quot;)); info.add(Chunk.NEWLINE); info.add(new Phrase(&quot;Japan &quot;)); info.add(new Phrase(&quot;japanese&quot;)); document.add(info); //List对象: a sequence of Paragraphs called ListItem document.newPage(); List list = new List(List.ORDERED); for (int i = 0; i &lt; 10; i++) { ListItem item = new ListItem(String.format(&quot;%s: %d movies&quot;, &quot;country&quot; + (i + 1), (i + 1) * 100), new Font( Font.FontFamily.HELVETICA, 6, Font.BOLD, BaseColor.WHITE)); List movielist = new List(List.ORDERED, List.ALPHABETICAL); movielist.setLowercase(List.LOWERCASE); for (int j = 0; j &lt; 5; j++) { ListItem movieitem = new ListItem(&quot;Title&quot; + (j + 1)); List directorlist = new List(List.UNORDERED); for (int k = 0; k &lt; 3; k++) { directorlist.add(String.format(&quot;%s, %s&quot;, &quot;Name1&quot; + (k + 1), &quot;Name2&quot; + (k + 1))); } movieitem.add(directorlist); movielist.add(movieitem); } item.add(movielist); list.add(item); } document.add(list); 插入Anchor, Image, Chapter, Section//Anchor对象: internal and external links Paragraph country = new Paragraph(); Anchor dest = new Anchor(&quot;china&quot;, new Font(Font.FontFamily.HELVETICA, 14, Font.BOLD, BaseColor.BLUE)); dest.setName(&quot;CN&quot;); dest.setReference(&quot;http://www.china.com&quot;);//external country.add(dest); country.add(String.format(&quot;: %d sites&quot;, 10000)); document.add(country); document.newPage(); Anchor toUS = new Anchor(&quot;Go to first page.&quot;, new Font(Font.FontFamily.HELVETICA, 14, Font.BOLD, BaseColor.BLUE)); toUS.setReference(&quot;#CN&quot;);//internal document.add(toUS); //Image对象 document.newPage(); Image img = Image.getInstance(&quot;resource/test.jpg&quot;); img.setAlignment(Image.LEFT | Image.TEXTWRAP); img.setBorder(Image.BOX); img.setBorderWidth(10); img.setBorderColor(BaseColor.WHITE); img.scaleToFit(1000, 72);//大小 img.setRotationDegrees(-30);//旋转 document.add(img); //Chapter, Section对象（目录） document.newPage(); Paragraph title = new Paragraph(&quot;Title&quot;); Chapter chapter = new Chapter(title, 1); title = new Paragraph(&quot;Section A&quot;); Section section = chapter.addSection(title); section.setBookmarkTitle(&quot;bmk&quot;); section.setIndentation(30); section.setBookmarkOpen(false); section.setNumberStyle( Section.NUMBERSTYLE_DOTTED_WITHOUT_FINAL_DOT); Section subsection = section.addSection(new Paragraph(&quot;Sub Section A&quot;)); subsection.setIndentationLeft(20); subsection.setNumberDepth(1); document.add(chapter); 画图//左右箭头 document.add(new VerticalPositionMark() { public void draw(PdfContentByte canvas, float llx, float lly, float urx, float ury, float y) { canvas.beginText(); BaseFont bf = null; try { bf = BaseFont.createFont(BaseFont.ZAPFDINGBATS, &quot;&quot;, BaseFont.EMBEDDED); } catch (Exception e) { e.printStackTrace(); } canvas.setFontAndSize(bf, 12); // LEFT canvas.showTextAligned(Element.ALIGN_CENTER, String.valueOf((char) 220), llx - 10, y, 0); // RIGHT canvas.showTextAligned(Element.ALIGN_CENTER, String.valueOf((char) 220), urx + 10, y + 8, 180); canvas.endText(); } }); //直线 Paragraph p1 = new Paragraph(&quot;LEFT&quot;); p1.add(new Chunk(new LineSeparator())); p1.add(&quot;R&quot;); document.add(p1); //点线 Paragraph p2 = new Paragraph(&quot;LEFT&quot;); p2.add(new Chunk(new DottedLineSeparator())); p2.add(&quot;R&quot;); document.add(p2); //下滑线 LineSeparator UNDERLINE = new LineSeparator(1, 100, null, Element.ALIGN_CENTER, -2); Paragraph p3 = new Paragraph(&quot;NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN&quot;); p3.add(UNDERLINE); document.add(p3); 设置段落Paragraph p = new Paragraph(&quot;In the previous example, you added a header and footer with the showTextAligned() method. This example demonstrates that it’s sometimes more interesting to use PdfPTable and writeSelectedRows(). You can define a bottom border for each cell so that the header is underlined. This is the most elegant way to add headers and footers, because the table mechanism allows you to position and align lines, images, and text.&quot;); //默认 p.setAlignment(Element.ALIGN_JUSTIFIED); document.add(p); document.newPage(); p.setAlignment(Element.ALIGN_JUSTIFIED); p.setIndentationLeft(1 * 15f); p.setIndentationRight((5 - 1) * 15f); document.add(p); //居右 document.newPage(); p.setAlignment(Element.ALIGN_RIGHT); p.setSpacingAfter(15f); document.add(p); //居左 document.newPage(); p.setAlignment(Element.ALIGN_LEFT); p.setSpacingBefore(15f); document.add(p); //居中 document.newPage(); p.setAlignment(Element.ALIGN_CENTER); p.setSpacingAfter(15f); p.setSpacingBefore(15f); document.add(p); 删除PageFileOutputStream out = new FileOutputStream(FILE_DIR + &quot;deletePage.pdf&quot;); Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, out); document.open(); document.add(new Paragraph(&quot;First page&quot;)); document.add(new Paragraph(Document.getVersion())); document.newPage(); writer.setPageEmpty(false); document.newPage(); document.add(new Paragraph(&quot;New page&quot;)); document.close(); PdfReader reader = new PdfReader(FILE_DIR + &quot;deletePage.pdf&quot;); reader.selectPages(&quot;1,3&quot;); PdfStamper stamp = new PdfStamper(reader, new FileOutputStream(FILE_DIR + &quot;deletePage2.pdf&quot;)); stamp.close(); reader.close(); 插入PageFileOutputStream out = new FileOutputStream(FILE_DIR + &quot;insertPage.pdf&quot;); Document document = new Document(); PdfWriter.getInstance(document, out); document.open(); document.add(new Paragraph(&quot;1 page&quot;)); document.newPage(); document.add(new Paragraph(&quot;2 page&quot;)); document.newPage(); document.add(new Paragraph(&quot;3 page&quot;)); document.close(); PdfReader reader = new PdfReader(FILE_DIR + &quot;insertPage.pdf&quot;); PdfStamper stamp = new PdfStamper(reader, new FileOutputStream(FILE_DIR + &quot;insertPage2.pdf&quot;)); stamp.insertPage(2, reader.getPageSize(1)); ColumnText ct = new ColumnText(null); ct.addElement(new Paragraph(24, new Chunk(&quot;INSERT PAGE&quot;))); ct.setCanvas(stamp.getOverContent(2)); ct.setSimpleColumn(36, 36, 559, 770); stamp.close(); reader.close(); 排序pagePdfWriter writer = PdfWriter.getInstance(doc, out); writer.setLinearPageMode(); doc.open(); doc.add(new Paragraph(&quot;1 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;2 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;3 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;4 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;5 page&quot;)); int[] order = {4,3,2,1}; writer.reorderPages(order); 目录// Code 1 document.add(new Chunk(&quot;Chapter 1&quot;).setLocalDestination(&quot;1&quot;)); document.newPage(); document.add(new Chunk(&quot;Chapter 2&quot;).setLocalDestination(&quot;2&quot;)); document.add(new Paragraph(new Chunk(&quot;Sub 2.1&quot;).setLocalDestination(&quot;2.1&quot;))); document.add(new Paragraph(new Chunk(&quot;Sub 2.2&quot;).setLocalDestination(&quot;2.2&quot;))); document.newPage(); document.add(new Chunk(&quot;Chapter 3&quot;).setLocalDestination(&quot;3&quot;)); // Code 2 PdfContentByte cb = writer.getDirectContent(); PdfOutline root = cb.getRootOutline(); // Code 3 @SuppressWarnings(&quot;unused&quot;) PdfOutline oline1 = new PdfOutline(root, PdfAction.gotoLocalPage(&quot;1&quot;, false), &quot;Chapter 1&quot;); PdfOutline oline2 = new PdfOutline(root, PdfAction.gotoLocalPage(&quot;2&quot;, false), &quot;Chapter 2&quot;); oline2.setOpen(false); @SuppressWarnings(&quot;unused&quot;) PdfOutline oline2_1 = new PdfOutline(oline2, PdfAction.gotoLocalPage(&quot;2.1&quot;, false), &quot;Sub 2.1&quot;); @SuppressWarnings(&quot;unused&quot;) PdfOutline oline2_2 = new PdfOutline(oline2, PdfAction.gotoLocalPage(&quot;2.2&quot;, false), &quot;Sub 2.2&quot;); @SuppressWarnings(&quot;unused&quot;) PdfOutline oline3 = new PdfOutline(root, PdfAction.gotoLocalPage(&quot;3&quot;, false), &quot;Chapter 3&quot;); Header, FooterPdfWriter writer = PdfWriter.getInstance(doc, new FileOutputStream(FILE_DIR + &quot;setHeaderFooter.pdf&quot;)); writer.setPageEvent(new PdfPageEventHelper() { public void onEndPage(PdfWriter writer, Document document) { PdfContentByte cb = writer.getDirectContent(); cb.saveState(); cb.beginText(); BaseFont bf = null; try { bf = BaseFont.createFont(BaseFont.HELVETICA, BaseFont.WINANSI, BaseFont.EMBEDDED); } catch (Exception e) { e.printStackTrace(); } cb.setFontAndSize(bf, 10); //Header float x = document.top(-20); //左 cb.showTextAligned(PdfContentByte.ALIGN_LEFT, &quot;H-Left&quot;, document.left(), x, 0); //中 cb.showTextAligned(PdfContentByte.ALIGN_CENTER, writer.getPageNumber()+ &quot; page&quot;, (document.right() + document.left())/2, x, 0); //右 cb.showTextAligned(PdfContentByte.ALIGN_RIGHT, &quot;H-Right&quot;, document.right(), x, 0); //Footer float y = document.bottom(-20); //左 cb.showTextAligned(PdfContentByte.ALIGN_LEFT, &quot;F-Left&quot;, document.left(), y, 0); //中 cb.showTextAligned(PdfContentByte.ALIGN_CENTER, writer.getPageNumber()+&quot; page&quot;, (document.right() + document.left())/2, y, 0); //右 cb.showTextAligned(PdfContentByte.ALIGN_RIGHT, &quot;F-Right&quot;, document.right(), y, 0); cb.endText(); cb.restoreState(); } }); doc.open(); doc.add(new Paragraph(&quot;1 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;2 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;3 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;4 page&quot;)); 左右文字PdfWriter writer = PdfWriter.getInstance(document, out); document.open(); PdfContentByte canvas = writer.getDirectContent(); Phrase phrase1 = new Phrase(&quot;This is a test!left&quot;); Phrase phrase2 = new Phrase(&quot;This is a test!right&quot;); Phrase phrase3 = new Phrase(&quot;This is a test!center&quot;); ColumnText.showTextAligned(canvas, Element.ALIGN_LEFT, phrase1, 10, 500, 0); ColumnText.showTextAligned(canvas, Element.ALIGN_RIGHT, phrase2, 10, 536, 0); ColumnText.showTextAligned(canvas, Element.ALIGN_CENTER, phrase3, 10, 572, 0); 幻灯片放映PdfWriter writer = PdfWriter.getInstance(doc, out); writer.setPdfVersion(PdfWriter.VERSION_1_5); writer.setViewerPreferences(PdfWriter.PageModeFullScreen);//全屏 writer.setPageEvent(new PdfPageEventHelper() { public void onStartPage(PdfWriter writer, Document document) { writer.setTransition(new PdfTransition(PdfTransition.DISSOLVE, 3)); writer.setDuration(5);//间隔时间 } }); doc.open(); doc.add(new Paragraph(&quot;1 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;2 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;3 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;4 page&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;5 page&quot;)); 压缩PDF到ZipZipOutputStream zip = new ZipOutputStream(new FileOutputStream(FILE_DIR + &quot;zipPDF.zip&quot;)); for (int i = 1; i &lt;= 3; i++) { ZipEntry entry = new ZipEntry(&quot;hello_&quot; + i + &quot;.pdf&quot;); zip.putNextEntry(entry); Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, zip); writer.setCloseStream(false); document.open(); document.add(new Paragraph(&quot;Hello &quot; + i)); document.close(); zip.closeEntry(); } zip.close(); 分割PDFFileOutputStream out = new FileOutputStream(FILE_DIR + &quot;splitPDF.pdf&quot;); Document document = new Document(); PdfWriter.getInstance(document, out); document.open(); document.add(new Paragraph(&quot;1 page&quot;)); document.newPage(); document.add(new Paragraph(&quot;2 page&quot;)); document.newPage(); document.add(new Paragraph(&quot;3 page&quot;)); document.newPage(); document.add(new Paragraph(&quot;4 page&quot;)); document.close(); PdfReader reader = new PdfReader(FILE_DIR + &quot;splitPDF.pdf&quot;); Document dd = new Document(); PdfWriter writer = PdfWriter.getInstance(dd, new FileOutputStream(FILE_DIR + &quot;splitPDF1.pdf&quot;)); dd.open(); PdfContentByte cb = writer.getDirectContent(); dd.newPage(); cb.addTemplate(writer.getImportedPage(reader, 1), 0, 0); dd.newPage(); cb.addTemplate(writer.getImportedPage(reader, 2), 0, 0); dd.close(); writer.close(); Document dd2 = new Document(); PdfWriter writer2 = PdfWriter.getInstance(dd2, new FileOutputStream(FILE_DIR + &quot;splitPDF2.pdf&quot;)); dd2.open(); PdfContentByte cb2 = writer2.getDirectContent(); dd2.newPage(); cb2.addTemplate(writer2.getImportedPage(reader, 3), 0, 0); dd2.newPage(); cb2.addTemplate(writer2.getImportedPage(reader, 4), 0, 0); dd2.close(); writer2.close(); 合并PDFPdfReader reader1 = new PdfReader(FILE_DIR + &quot;splitPDF1.pdf&quot;); PdfReader reader2 = new PdfReader(FILE_DIR + &quot;splitPDF2.pdf&quot;); FileOutputStream out = new FileOutputStream(FILE_DIR + &quot;mergePDF.pdf&quot;); Document document = new Document(); PdfWriter writer = PdfWriter.getInstance(document, out); document.open(); PdfContentByte cb = writer.getDirectContent(); int totalPages = 0; totalPages += reader1.getNumberOfPages(); totalPages += reader2.getNumberOfPages(); java.util.List&lt;PdfReader&gt; readers = new ArrayList&lt;PdfReader&gt;(); readers.add(reader1); readers.add(reader2); int pageOfCurrentReaderPDF = 0; Iterator&lt;PdfReader&gt; iteratorPDFReader = readers.iterator(); // Loop through the PDF files and add to the output. while (iteratorPDFReader.hasNext()) { PdfReader pdfReader = iteratorPDFReader.next(); // Create a new page in the target for each source page. while (pageOfCurrentReaderPDF &lt; pdfReader.getNumberOfPages()) { document.newPage(); pageOfCurrentReaderPDF++; PdfImportedPage page = writer.getImportedPage(pdfReader, pageOfCurrentReaderPDF); cb.addTemplate(page, 0, 0); } pageOfCurrentReaderPDF = 0; } out.flush(); document.close(); out.close(); AnnotationPdfWriter writer = PdfWriter.getInstance(doc, out); writer.setLinearPageMode(); doc.open(); doc.add(new Paragraph(&quot;1 page&quot;)); doc.add(new Annotation(&quot;Title&quot;, &quot;This is a annotation!&quot;)); doc.newPage(); doc.add(new Paragraph(&quot;2 page&quot;)); Chunk chunk = new Chunk(&quot;\u00a0&quot;); chunk.setAnnotation(PdfAnnotation.createText(writer, null, &quot;Title&quot;, &quot;This is a another annotation!&quot;, false, &quot;Comment&quot;)); doc.add(chunk); //添加附件 doc.newPage(); doc.add(new Paragraph(&quot;3 page&quot;)); Chunk chunk2 = new Chunk(&quot;\u00a0\u00a0&quot;); PdfAnnotation annotation = PdfAnnotation.createFileAttachment( writer, null, &quot;Title&quot;, null, &quot;resource/test2.jpg&quot;, &quot;img.jpg&quot;); annotation.put(PdfName.NAME, new PdfString(&quot;Paperclip&quot;)); chunk2.setAnnotation(annotation); doc.add(chunk2); 插入一个TablePdfPTable table = new PdfPTable(3); PdfPCell cell; cell = new PdfPCell(new Phrase(&quot;Cell with colspan 3&quot;)); cell.setColspan(3); table.addCell(cell); cell = new PdfPCell(new Phrase(&quot;Cell with rowspan 2&quot;)); cell.setRowspan(2); table.addCell(cell); table.addCell(&quot;row 1; cell 1&quot;); table.addCell(&quot;row 1; cell 2&quot;); table.addCell(&quot;row 2; cell 1&quot;); table.addCell(&quot;row 2; cell 2&quot;); document.add(table); 表格嵌套PdfPTable table = new PdfPTable(4); //1行2列 PdfPTable nested1 = new PdfPTable(2); nested1.addCell(&quot;1.1&quot;); nested1.addCell(&quot;1.2&quot;); //2行1列 PdfPTable nested2 = new PdfPTable(1); nested2.addCell(&quot;2.1&quot;); nested2.addCell(&quot;2.2&quot;); //将表格插入到指定位置 for (int k = 0; k &lt; 24; ++k) { if (k == 1) { table.addCell(nested1); } else if (k == 20) { table.addCell(nested2); } else { table.addCell(&quot;cell &quot; + k); } } document.add(table); 设置表格宽度PdfPTable table = new PdfPTable(3); PdfPCell cell; cell = new PdfPCell(new Phrase(&quot;Cell with colspan 3&quot;)); cell.setColspan(3); table.addCell(cell); cell = new PdfPCell(new Phrase(&quot;Cell with rowspan 2&quot;)); cell.setRowspan(2); table.addCell(cell); table.addCell(&quot;row 1; cell 1&quot;); table.addCell(&quot;row 1; cell 2&quot;); table.addCell(&quot;row 2; cell 1&quot;); table.addCell(&quot;row 2; cell 2&quot;); //100% table.setWidthPercentage(100); document.add(table); document.add(new Paragraph(&quot;\n\n&quot;)); //宽度50% 居左 table.setHorizontalAlignment(Element.ALIGN_LEFT); document.add(table); document.add(new Paragraph(&quot;\n\n&quot;)); //宽度50% 居中 table.setHorizontalAlignment(Element.ALIGN_CENTER); document.add(table); document.add(new Paragraph(&quot;\n\n&quot;)); //宽度50% 居右 table.setWidthPercentage(50); table.setHorizontalAlignment(Element.ALIGN_RIGHT); document.add(table); document.add(new Paragraph(&quot;\n\n&quot;)); //固定宽度 table.setTotalWidth(300); table.setLockedWidth(true); document.add(table); 设置表格前后间隔PdfPTable table = new PdfPTable(3); PdfPCell cell = new PdfPCell(new Paragraph(&quot;合并3个单元格&quot;,fontZH)); cell.setColspan(3); table.addCell(cell); table.addCell(&quot;1.1&quot;); table.addCell(&quot;2.1&quot;); table.addCell(&quot;3.1&quot;); table.addCell(&quot;1.2&quot;); table.addCell(&quot;2.2&quot;); table.addCell(&quot;3.2&quot;); cell = new PdfPCell(new Paragraph(&quot;红色边框&quot;,fontZH)); cell.setBorderColor(new BaseColor(255, 0, 0)); table.addCell(cell); cell = new PdfPCell(new Paragraph(&quot;合并单2个元格&quot;,fontZH)); cell.setColspan(2); cell.setBackgroundColor(new BaseColor(0xC0, 0xC0, 0xC0)); table.addCell(cell); table.setWidthPercentage(50); document.add(new Paragraph(&quot;追加2个表格&quot;,fontZH)); document.add(table); document.add(table); document.newPage(); document.add(new Paragraph(&quot;使用&apos;SpacingBefore&apos;和&apos;setSpacingAfter&apos;&quot;,fontZH)); table.setSpacingBefore(15f); document.add(table); document.add(table); document.add(new Paragraph(&quot;这里没有间隔&quot;,fontZH)); table.setSpacingAfter(15f); 设置单元格宽度//按比例设置单元格宽度 float[] widths = {0.1f, 0.1f, 0.05f, 0.75f}; PdfPTable table = new PdfPTable(widths); table.addCell(&quot;10%&quot;); table.addCell(&quot;10%&quot;); table.addCell(&quot;5%&quot;); table.addCell(&quot;75%&quot;); table.addCell(&quot;aa&quot;); table.addCell(&quot;aa&quot;); table.addCell(&quot;a&quot;); table.addCell(&quot;aaaaaaaaaaaaaaa&quot;); table.addCell(&quot;bb&quot;); table.addCell(&quot;bb&quot;); table.addCell(&quot;b&quot;); table.addCell(&quot;bbbbbbbbbbbbbbb&quot;); table.addCell(&quot;cc&quot;); table.addCell(&quot;cc&quot;); table.addCell(&quot;c&quot;); table.addCell(&quot;ccccccccccccccc&quot;); document.add(table); document.add(new Paragraph(&quot;\n\n&quot;)); //调整比例 widths[0] = 20f; widths[1] = 20f; widths[2] = 10f; widths[3] = 50f; table.setWidths(widths); document.add(table); //按绝对值设置单元格宽度 widths[0] = 40f; widths[1] = 40f; widths[2] = 20f; widths[3] = 300f; Rectangle r = new Rectangle(PageSize.A4.getRight(72), PageSize.A4.getTop(72)); table.setWidthPercentage(widths, r); document.add(new Paragraph(&quot;\n\n&quot;)); document.add(table); 设置单元格高度PdfPTable table = new PdfPTable(2); PdfPCell cell; //折行 table.addCell(new PdfPCell(new Paragraph(&quot;折行&quot;, fontZH))); cell = new PdfPCell(new Paragraph(&quot;blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah&quot;)); cell.setNoWrap(false); table.addCell(cell); //不折行 table.addCell(new PdfPCell(new Paragraph(&quot;不折行&quot;, fontZH))); cell.setNoWrap(true); table.addCell(cell); //设置高度 table.addCell(new PdfPCell(new Paragraph(&quot;任意高度&quot;,fontZH))); cell = new PdfPCell(new Paragraph(&quot;1. blah blah\n2. blah blah blah\n3. blah blah\n4. blah blah blah\n5. blah blah\n6. blah blah blah\n7. blah blah\n8. blah blah blah&quot;)); table.addCell(cell); //固定高度 table.addCell(new PdfPCell(new Paragraph(&quot;固定高度&quot;,fontZH))); cell.setFixedHeight(50f); table.addCell(cell); //最小高度 table.addCell(new PdfPCell(new Paragraph(&quot;最小高度&quot;,fontZH))); cell = new PdfPCell(new Paragraph(&quot;最小高度：50&quot;,fontZH)); cell.setMinimumHeight(50f); table.addCell(cell); //最后一行拉长到page底部 table.setExtendLastRow(true); table.addCell(new PdfPCell(new Paragraph(&quot;拉长最后一行&quot;,fontZH))); cell = new PdfPCell(new Paragraph(&quot;最后一行拉长到page底部&quot;,fontZH)); table.addCell(cell); document.add(table); 设置单元格颜色PdfPTable table = new PdfPTable(4); PdfPCell cell; cell = new PdfPCell(new Paragraph(&quot;颜色测试&quot;,fontZH)); table.addCell(cell); //红色背景，无边框 cell = new PdfPCell(new Paragraph(&quot;红色背景，无边框&quot;,fontZH)); cell.setBorder(Rectangle.NO_BORDER); cell.setBackgroundColor(BaseColor.RED); table.addCell(cell); //绿色背景，下边框 cell = new PdfPCell(new Paragraph(&quot;绿色背景，下边框&quot;,fontZH)); cell.setBorder(Rectangle.BOTTOM); cell.setBorderColorBottom(BaseColor.MAGENTA); cell.setBorderWidthBottom(5f); cell.setBackgroundColor(BaseColor.GREEN); table.addCell(cell); //蓝色背景，上边框 cell = new PdfPCell(new Paragraph(&quot;蓝色背景，上边框&quot;,fontZH)); cell.setBorder(Rectangle.TOP); cell.setUseBorderPadding(true); cell.setBorderWidthTop(5f); cell.setBorderColorTop(BaseColor.CYAN); cell.setBackgroundColor(BaseColor.BLUE); table.addCell(cell); cell = new PdfPCell(new Paragraph(&quot;背景灰色度&quot;,fontZH)); table.addCell(cell); cell = new PdfPCell(new Paragraph(&quot;0.25&quot;)); cell.setBorder(Rectangle.NO_BORDER); cell.setGrayFill(0.25f); table.addCell(cell); cell = new PdfPCell(new Paragraph(&quot;0.5&quot;)); cell.setBorder(Rectangle.NO_BORDER); cell.setGrayFill(0.5f); table.addCell(cell); cell = new PdfPCell(new Paragraph(&quot;0.75&quot;)); cell.setBorder(Rectangle.NO_BORDER); cell.setGrayFill(0.75f); table.addCell(cell); document.add(table); 插入图像Image image = Image.getInstance(&quot;resource/test2.jpg&quot;); float[] widths = { 1f, 4f }; PdfPTable table = new PdfPTable(widths); //插入图片 table.addCell(new PdfPCell(new Paragraph(&quot;图片测试&quot;, fontZH))); table.addCell(image); //调整图片大小 table.addCell(&quot;This two&quot;); table.addCell(new PdfPCell(image, true)); //不调整 table.addCell(&quot;This three&quot;); table.addCell(new PdfPCell(image, false)); document.add(table); 设置表头String[] bogusData = { &quot;M0065920&quot;, &quot;SL&quot;, &quot;FR86000P&quot;, &quot;PCGOLD&quot;, &quot;119000&quot;, &quot;96 06&quot;, &quot;2001-08-13&quot;, &quot;4350&quot;, &quot;6011648299&quot;, &quot;FLFLMTGP&quot;, &quot;153&quot;, &quot;119000.00&quot; }; int NumColumns = 12; // 12 PdfPTable datatable = new PdfPTable(NumColumns); int headerwidths[] = { 9, 4, 8, 10, 8, 11, 9, 7, 9, 10, 4, 10 }; // percentage datatable.setWidths(headerwidths); datatable.setWidthPercentage(100); datatable.getDefaultCell().setPadding(3); datatable.getDefaultCell().setBorderWidth(2); datatable.getDefaultCell().setHorizontalAlignment(Element.ALIGN_CENTER); datatable.addCell(&quot;Clock #&quot;); datatable.addCell(&quot;Trans Type&quot;); datatable.addCell(&quot;Cusip&quot;); datatable.addCell(&quot;Long Name&quot;); datatable.addCell(&quot;Quantity&quot;); datatable.addCell(&quot;Fraction Price&quot;); datatable.addCell(&quot;Settle Date&quot;); datatable.addCell(&quot;Portfolio&quot;); datatable.addCell(&quot;ADP Number&quot;); datatable.addCell(&quot;Account ID&quot;); datatable.addCell(&quot;Reg Rep ID&quot;); datatable.addCell(&quot;Amt To Go &quot;); datatable.setHeaderRows(1); //边框 datatable.getDefaultCell().setBorderWidth(1); //背景色 for (int i = 1; i &lt; 1000; i++) { for (int x = 0; x &lt; NumColumns; x++) { datatable.addCell(bogusData[x]); } } document.add(datatable); 分割表格//横向分割 PdfContentByte cb = writer.getDirectContent(); PdfPTable table = new PdfPTable(10); for (int k = 1; k &lt;= 100; ++k) { table.addCell(&quot;The number &quot; + k); } table.setTotalWidth(400); table.writeSelectedRows(0, 5, 0, -1, 5, 700, cb); table.writeSelectedRows(5, -1, 0, -1, 210, 700, cb); 设置单元格留白PdfPTable table = new PdfPTable(2); PdfPCell cell; Paragraph p = new Paragraph(&quot;Quick brown fox jumps over the lazy dog. Quick brown fox jumps over the lazy dog.&quot;); table.addCell(new PdfPCell(new Paragraph(&quot;默认&quot;,fontZH))); table.addCell(p); table.addCell(new PdfPCell(new Paragraph(&quot;Padding：10&quot;,fontZH))); cell = new PdfPCell(p); cell.setPadding(10f); table.addCell(cell); table.addCell(new PdfPCell(new Paragraph(&quot;Padding：0&quot;,fontZH))); cell = new PdfPCell(p); cell.setPadding(0f); table.addCell(cell); table.addCell(new PdfPCell(new Paragraph(&quot;上Padding：0 左Padding：20&quot;,fontZH))); cell = new PdfPCell(p); cell.setPaddingTop(0f); cell.setPaddingLeft(20f); table.addCell(cell); document.add(table); document.newPage(); table = new PdfPTable(2); table.addCell(new PdfPCell(new Paragraph(&quot;没有Leading&quot;,fontZH))); table.getDefaultCell().setLeading(0f, 0f); table.addCell(&quot;blah blah\nblah blah blah\nblah blah\nblah blah blah\nblah blah\nblah blah blah\nblah blah\nblah blah blah\n&quot;); table.getDefaultCell().setLeading(14f, 0f); table.addCell(new PdfPCell(new Paragraph(&quot;固定Leading：14pt&quot;,fontZH))); table.addCell(&quot;blah blah\nblah blah blah\nblah blah\nblah blah blah\nblah blah\nblah blah blah\nblah blah\nblah blah blah\n&quot;); table.addCell(new PdfPCell(new Paragraph(&quot;相对于字体&quot;,fontZH))); table.getDefaultCell().setLeading(0f, 1.0f); table.addCell(&quot;blah blah\nblah blah blah\nblah blah\nblah blah blah\nblah blah\nblah blah blah\nblah blah\nblah blah blah\n&quot;); document.add(table); 设置单元格边框 //没有边框 PdfPTable table1 = new PdfPTable(3); table1.getDefaultCell().setBorder(PdfPCell.NO_BORDER); table1.addCell(new Paragraph(&quot;Cell 1&quot;)); table1.addCell(new Paragraph(&quot;Cell 2&quot;)); table1.addCell(new Paragraph(&quot;Cell 3&quot;)); document.add(table1); //边框粗细颜色 document.newPage(); Rectangle b1 = new Rectangle(0f, 0f); b1.setBorderWidthLeft(6f); b1.setBorderWidthBottom(5f); b1.setBorderWidthRight(4f); b1.setBorderWidthTop(2f); b1.setBorderColorLeft(BaseColor.RED); b1.setBorderColorBottom(BaseColor.ORANGE); b1.setBorderColorRight(BaseColor.YELLOW); b1.setBorderColorTop(BaseColor.GREEN); PdfPTable table2 = new PdfPTable(1); PdfPCell cell = new PdfPCell(new Paragraph(&quot;Cell 1&quot;)); cell.cloneNonPositionParameters(b1); table2.addCell(cell); document.add(table2); 生成Barcode QRCodeString myString = &quot;http://www.google.com&quot;; Barcode128 code128 = new Barcode128(); code128.setCode(myString.trim()); code128.setCodeType(Barcode128.CODE128); Image code128Image = code128.createImageWithBarcode(cb, null, null); code128Image.setAbsolutePosition(10,700); code128Image.scalePercent(125); doc.add(code128Image); BarcodeQRCode qrcode = new BarcodeQRCode(myString.trim(), 1, 1, null); Image qrcodeImage = qrcode.getImage(); qrcodeImage.setAbsolutePosition(10,600); qrcodeImage.scalePercent(200); doc.add(qrcodeImage); HTML to PDFDocument document = new Document(PageSize.LETTER); PdfWriter.getInstance(document, new FileOutputStream(&quot;c://testpdf1.pdf&quot;)); document.open(); HTMLWorker htmlWorker = new HTMLWorker(document); htmlWorker.parse(new StringReader(&quot;&lt;h1&gt;This is a test!&lt;/h1&gt;&quot;)); document.close(); JFreeReportJFreeReport的数据继承自Swing组件的TableModel接口。JFreeReport生成的报表可以分页预览、打印或者保存为多种格式的文件包括pdf、Excel、html等。该项目主页:http://www.jfree.org/jfreereport/ PJXPJX支持读取，组合，处理，和生成PDF文档（注意：PJX需要 J2SE 1.4.0 或更高版本）。该项目主页:http://www.etymon.com/epub.html Apache FOPFOP是由James Tauber发起的一个开源项目，原先的版本是利用xsl-fo将xml文件转换成pdf文件。但最新的版本它可以将xml文件转换成pdf，mif，pcl，txt等多种格式以及直接输出到打印机，并且支持使用SVG描述图形。该项目主页:http://xml.apache.org/fop/ gnujpdfgnujpdf是一个java类包(gnu.jpdf.*),它提供了一个简单的API来创建与打印PDF文件。遵循LGPL开源协议。该项目主页:http://gnujpdf.sourceforge.net/ PDF BoxPDFBox是一个开源的可以操作PDF文档的Java PDF类库。它可以创建一个新PDF文档,操作现有PDF文档并提取文档中的内容。它具有以下特性:1.将一个PDF文档转换输出为一个文本文件。2.可以从文本文件创建一个PDF文档。3.加密/解密PDF文档。4.向已有PDF文档中追加内容。5.可以从PDF文档生成一张图片。6.可以与Jakarta Lucene搜索引擎的整合。该项目主页:http://www.pdfbox.org/ ConnlaConnla是一个Java包用于创建可导成TXT,CSV,HTML,XHTML,XML,PDF和XLS等格式的数据集。该项目主页:http://connla.sourceforge.net/ PDF Split &amp; MergePDF Split&amp;Merge是一款实用基于GPL许可协议发布的PDF文件分割与合并工具。您可以指定页码范围将一个PDF文件分割为若干PDF 文件(支持单页和多页混合)，或将多个PDF文件按指定顺序合并成一个PDF文件。其转换速度非常快。它采用Java Swing开发,运用到的第三方组件包括：iText,jcmdline和JGoodies界面包。该项目主页:http://pdfsam.sourceforge.net/ PDF Clown for JavaPDF Clown for Java是一个基于Java1.5用于读，写和操作PDF文档的Java类包。它提供多个抽象层来满足不同的编程风格：从底层(PDF对象模型)到高级(PDF文档结构和内容流)。该项目主页:http://www.stefanochizzolini.it/en/projects/clown/ iText toolboxiText toolbox是一个Java Swing应用程序,其起初是iText类库的一部分。iText toolbox既可以作为一个可执行的Jar，也可作为Java Webstart应用程序运行。对于完成各种类型的PDF相关文件操作，iText toolbox是一个非常有用的工具比如：把一个目录下的所有图片转换成一个PDF文档，合并现有PDF文档等。此外开发人员可以把它当成一个学习iText类库各项功能的工具。该项目主页:http://itexttoolbox.sourceforge.net/ PDFjetPDFjet是一个用于动态生成PDF文档的Java类库。支持绘制点、线、方框、圆、贝塞尔曲线(Bezier Curves) 、多边形、星形、复杂路径和形状。支持unicode，文本间距调整，嵌入超链接等。该项目主页:http://pdfjet.com/os/edition.html ICEpdfICEpdf是一个开源Java PDF引擎，用于展示/查看PDF文档，转换和抽取PDF文档的内容。可以集成到Java桌面应用程序或Web服务器中使用。该项目主页:http://www.icepdf.org/ JSignPdfJSignPdf是一个用于为PDF文档添加数字签名的Java应用程序。它既可以单独使用，也可以作为OpenOffice.org的插件使用。支持设置验证级别，PDF加密和设置权限，添加签名图标，批量转换（通过命令行参数控制）。该项目主页:http://jsignpdf.sourceforge.net/ PDF RendererPDF Renderer是一个采用纯Java实现的PDF阅读器和渲染器。 可以利用它实现在你的应用中查看PDF文件；在导出PDF文件之前进行预览；在服务器端Web应用中将PDF转成PNGs图片；在一个3D场景中查看PDF。该项目主页:https://pdf-renderer.dev.java.net/ 文章来自 http://blog.csdn.net/rongbo_j/article/details/50196909]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--Excel读写库]]></title>
    <url>%2F2018%2F03%2F13%2Fjava-api03%2F</url>
    <content type="text"><![CDATA[Excel是我们平时工作中比较常用的用于存储二维表数据的，JAVA也可以直接对Excel进行操作，在这篇博客中将为大家介绍两种操作Excel的方式，分别为：jxl和poi。 对于两者的区别网上有测试如下： 在小数据量时jxl快于poi，在大数据量时poi要快于jxl。但差距都不明显。 jxljxl.jar是通过java操作excel表格的工具类库； 在开始进行Java读写Excel前，我们需要先下一个jxl的jar包，这个jar包中提供了相关读写Excel的方法，在百度里所搜一下jxl.jar下载就会出现很多下载地址了，这里不再累述。随后我们将jxl.jar放到classpath下或者在工程的buildpath中添加jxl.jar后，便可以开始Java读写Excel的神秘之旅了。 通过模拟实现创建一个表格，然后模拟添加到表格中数据，实际开发过程中都是通过从数据库导入到表格中的数据: 写xls/xlsxpackage com.bie; import java.io.File; import java.io.IOException; import jxl.Workbook; import jxl.write.Label; import jxl.write.WritableSheet; import jxl.write.WritableWorkbook; import jxl.write.WriteException; import jxl.write.biff.RowsExceededException; /** * @author BieHongLi * @version 创建时间：2017年3月3日 下午4:03:18 * 创建excel表格 */ public class CreateExcel { public static void main(String[] args) throws IOException, RowsExceededException, WriteException { //1:创建excel文件 File file=new File(&quot;test.xls&quot;); file.createNewFile(); //2:创建工作簿 WritableWorkbook workbook=Workbook.createWorkbook(file); //3:创建sheet,设置第二三四..个sheet，依次类推即可 WritableSheet sheet=workbook.createSheet(&quot;用户管理&quot;, 0); //4：设置titles String[] titles={&quot;编号&quot;,&quot;账号&quot;,&quot;密码&quot;}; //5:单元格 Label label=null; //6:给第一行设置列名 for(int i=0;i&lt;titles.length;i++){ //x,y,第一行的列名 label=new Label(i,0,titles[i]); //7：添加单元格 sheet.addCell(label); } //8：模拟数据库导入数据 for(int i=1;i&lt;10;i++){ //添加编号，第二行第一列 label=new Label(0,i,i+&quot;&quot;); sheet.addCell(label); //添加账号 label=new Label(1,i,&quot;10010&quot;+i); sheet.addCell(label); //添加密码 label=new Label(2,i,&quot;123456&quot;); sheet.addCell(label); } //写入数据，一定记得写入数据，不然你都开始怀疑世界了，excel里面啥都没有 workbook.write(); //最后一步，关闭工作簿 workbook.close(); } } 读取excel表格里面的数据，案例如下所示: 读xls/xlsxpackage com.bie; import java.io.File; import jxl.Cell; import jxl.Sheet; import jxl.Workbook; /** * @author BieHongLi * @version 创建时间：2017年3月3日 下午5:28:53 * 读取excel的表格的数据 */ public class ReadExcel { public static void main(String[] args) throws Exception{ //1:创建workbook Workbook workbook=Workbook.getWorkbook(new File(&quot;test.xls&quot;)); //2:获取第一个工作表sheet Sheet sheet=workbook.getSheet(0); //3:获取数据 System.out.println(&quot;行：&quot;+sheet.getRows()); System.out.println(&quot;列：&quot;+sheet.getColumns()); for(int i=0;i&lt;sheet.getRows();i++){ for(int j=0;j&lt;sheet.getColumns();j++){ Cell cell=sheet.getCell(j,i); System.out.print(cell.getContents()+&quot; &quot;); } System.out.println(); } //最后一步：关闭资源 workbook.close(); } } poi工作中经常需要对Excel进行读写操作，java操作excel文件比较流行的是apache poi包，excel分为xls（2003）和xlsx（2007）两种格式，操作这两种格式的excel需要不同的poi包。 注意别弄混 xls格式&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi&lt;/artifactId&gt; &lt;version&gt;3.11-beta1&lt;/version&gt; &lt;/dependency&gt; xlsx格式&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;3.11-beta1&lt;/version&gt; &lt;/dependency&gt; 读xlsFile file = new File(&quot;src/test/resources/test.xls&quot;); POIFSFileSystem poifsFileSystem = new POIFSFileSystem(new FileInputStream(file)); HSSFWorkbook hssfWorkbook = new HSSFWorkbook(poifsFileSystem); HSSFSheet hssfSheet = hssfWorkbook.getSheetAt(0); int rowstart = hssfSheet.getFirstRowNum(); int rowEnd = hssfSheet.getLastRowNum(); for(int i=rowstart;i&lt;=rowEnd;i++) { HSSFRow row = hssfSheet.getRow(i); if(null == row) continue; int cellStart = row.getFirstCellNum(); int cellEnd = row.getLastCellNum(); for(int k=cellStart;k&lt;=cellEnd;k++) { HSSFCell cell = row.getCell(k); if(null==cell) continue; System.out.print(&quot;&quot; + k + &quot; &quot;); //System.out.print(&quot;type:&quot;+cell.getCellType()); switch (cell.getCellType()) { case HSSFCell.CELL_TYPE_NUMERIC: // 数字 System.out.print(cell.getNumericCellValue() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_STRING: // 字符串 System.out.print(cell.getStringCellValue() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_BOOLEAN: // Boolean System.out.println(cell.getBooleanCellValue() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_FORMULA: // 公式 System.out.print(cell.getCellFormula() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_BLANK: // 空值 System.out.println(&quot; &quot;); break; case HSSFCell.CELL_TYPE_ERROR: // 故障 System.out.println(&quot; &quot;); break; default: System.out.print(&quot;未知类型 &quot;); break; } } System.out.print(&quot;\n&quot;); } 读xlsxFile file = new File(&quot;src/test/resources/test.xlsx&quot;); XSSFWorkbook xssfWorkbook = new XSSFWorkbook(file); XSSFSheet xssfSheet = xssfWorkbook.getSheetAt(0); int rowstart = xssfSheet.getFirstRowNum(); int rowEnd = xssfSheet.getLastRowNum(); for(int i=rowstart;i&lt;=rowEnd;i++) { XSSFRow row = xssfSheet.getRow(i); if(null == row) continue; int cellStart = row.getFirstCellNum(); int cellEnd = row.getLastCellNum(); for(int k=cellStart;k&lt;=cellEnd;k++) { XSSFCell cell = row.getCell(k); if(null==cell) continue; switch (cell.getCellType()) { case HSSFCell.CELL_TYPE_NUMERIC: // 数字 System.out.print(cell.getNumericCellValue() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_STRING: // 字符串 System.out.print(cell.getStringCellValue() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_BOOLEAN: // Boolean System.out.println(cell.getBooleanCellValue() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_FORMULA: // 公式 System.out.print(cell.getCellFormula() + &quot; &quot;); break; case HSSFCell.CELL_TYPE_BLANK: // 空值 System.out.println(&quot; &quot;); break; case HSSFCell.CELL_TYPE_ERROR: // 故障 System.out.println(&quot; &quot;); break; default: System.out.print(&quot;未知类型 &quot;); break; } } System.out.print(&quot;\n&quot;); } 写xlsHSSFWorkbook workbook = null; workbook = new HSSFWorkbook(); //获取参数个数作为excel列数 int columeCount = 6; //获取List size作为excel行数 int rowCount = 20; HSSFSheet sheet = workbook.createSheet(&quot;sheet name&quot;); //创建第一栏 HSSFRow headRow = sheet.createRow(0); String[] titleArray = {&quot;id&quot;, &quot;name&quot;, &quot;age&quot;, &quot;email&quot;, &quot;address&quot;, &quot;phone&quot;}; for(int m=0;m&lt;=columeCount-1;m++) { HSSFCell cell = headRow.createCell(m); cell.setCellType(HSSFCell.CELL_TYPE_STRING); sheet.setColumnWidth(m, 6000); HSSFCellStyle style = workbook.createCellStyle(); HSSFFont font = workbook.createFont(); font.setBoldweight(HSSFFont.BOLDWEIGHT_BOLD); short color = HSSFColor.RED.index; font.setColor(color); style.setFont(font); //填写数据 cell.setCellStyle(style); cell.setCellValue(titleArray[m]); } int index = 0; //写入数据 for(RowEntity entity : pRowEntityList) { //logger.info(&quot;写入一行&quot;); HSSFRow row = sheet.createRow(index+1); for(int n=0;n&lt;=columeCount-1;n++) row.createCell(n); row.getCell(0).setCellValue(entity.getId()); row.getCell(1).setCellValue(entity.getName()); row.getCell(2).setCellValue(entity.getAge()); row.getCell(3).setCellValue(entity.getEmail()); row.getCell(4).setCellValue(entity.getAddress()); row.getCell(5).setCellValue(entity.getPhone()); index++; } //写到磁盘上 try { FileOutputStream fileOutputStream = new FileOutputStream(new File(path)); workbook.write(fileOutputStream); fileOutputStream.close(); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } 写xlsx和写xls类似，使用2007对应的对象即可。 参考代码： http://blog.csdn.net/Augus6/article/details/51463478https://www.cnblogs.com/wangyang108/p/6030420.htmlhttp://blog.csdn.net/a214919447/article/details/54601237]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--单元测试库]]></title>
    <url>%2F2018%2F03%2F13%2Fjava-api02%2F</url>
    <content type="text"><![CDATA[测试是开发的一个非常重要的方面，可以在很大程度上决定一个应用程序的命运。良好的测试可以在早期捕获导致应用程序崩溃的问题，但较差的测试往往总是导致故障和停机。 三种主要类型的软件测试：单元测试，功能测试和集成测试 软件开发测试的类型单元测试用于测试各个代码组件，并确保代码按照预期的方式工作。单元测试由开发人员编写和执行。大多数情况下，使用JUnit或TestNG之类的测试框架。测试用例通常是在方法级别写入并通过自动化执行。 集成测试检查系统是否作为一个整体而工作。集成测试也由开发人员完成，但不是测试单个组件，而是旨在跨组件测试。系统由许多单独的组件组成，如代码，数据库，Web服务器等。集成测试能够发现如组件布线，网络访问，数据库问题等问题。 功能测试通过将给定输入的结果与规范进行比较来检查每个功能是否正确实现。通常，这不是在开发人员级别的。功能测试由单独的测试团队执行。测试用例基于规范编写，并且实际结果与预期结果进行比较。有若干工具可用于自动化的功能测试，如Selenium和QTP。 如前所述，单元测试可帮助开发人员确定代码是否正常工作。在这篇博文中，我将提供在Java中单元测试的有用提示。 使用框架来用于单元测试Java提供了若干用于单元测试的框架。TestNG和JUnit是最流行的测试框架。JUnit和TestNG的一些重要功能： 易于设置和运行。 支持注释。 允许忽略或分组并一起执行某些测试。 支持参数化测试，即通过在运行时指定不同的值来运行单元测试。 通过与构建工具，如Ant，Maven和Gradle集成来支持自动化的测试执行。 EasyMock是一个模拟框架，是单元测试框架，如JUnit和TestNG的补充。EasyMock本身不是一个完整的框架。它只是添加了创建模拟对象以便于测试的能力。例如，我们想要测试的一个方法可以调用从数据库获取数据的DAO类。在这种情况下，EasyMock可用于创建返回硬编码数据的MockDAO。这使我们能够轻松地测试我们意向的方法，而不必担心数据库访问。 谨慎使用测试驱动开发！测试驱动开发（TDD）是一个软件开发过程，在这过程中，在开始任何编码之前，我们基于需求来编写测试。由于还没有编码，测试最初会失败。然后写入最小量的代码以通过测试。然后重构代码，直到被优化。 目标是编写覆盖所有需求的测试，而不是一开始就写代码，却可能甚至都不能满足需求。TDD是伟大的，因为它导致简单的模块化代码，且易于维护。总体开发速度加快，容易发现缺陷。此外，单元测试被创建作为TDD方法的副产品。 然而，TDD可能不适合所有的情况。在设计复杂的项目中，专注于最简单的设计以便于通过测试用例，而不提前思考可能会导致巨大的代码更改。此外，TDD方法难以用于与遗留系统，GUI应用程序或与数据库一起工作的应用程序交互的系统。另外，测试需要随着代码的改变而更新。 因此，在决定采用TDD方法之前，应考虑上述因素，并应根据项目的性质采取措施。 测量代码覆盖率代码覆盖率衡量（以百分比表示）了在运行单元测试时执行的代码量。通常，高覆盖率的代码包含未检测到的错误的几率要低，因为其更多的源代码在测试过程中被执行。测量代码覆盖率的一些最佳做法包括： 使用代码覆盖工具，如Clover，Corbetura，JaCoCo或Sonar。使用工具可以提高测试质量，因为这些工具可以指出未经测试的代码区域，让你能够开发开发额外的测试来覆盖这些领域。每当写入新功能时，立即写新的测试覆盖。确保有测试用例覆盖代码的所有分支，即if / else语句。高代码覆盖不能保证测试是完美的，所以要小心！ 下面的 concat 方法接受布尔值作为输入，并且仅当布尔值为true时附加传递两个字符串： stringUtil类的方法 public String concat(boolean append, String a,String b) { String result = null; If (append) { result = a + b; } return result.toLowerCase(); } 以下是上述方法的测试用例： @Test public void testStringUtil() { String result = stringUtil.concat(true, “Hello “, “World”); System.out.println(“Result is “+result); } 在这种情况下，执行测试的值为true。当测试执行时，它将通过。当代码覆盖率工具运行时，它将显示100%的代码覆盖率，因为 concat 方法中的所有代码都被执行。但是，如果测试执行的值为false，则将抛出 NullPointerException 。所以100%的代码覆盖率并不真正表明测试覆盖了所有场景，也不能说明测试良好。 尽可能将测试数据外部化在JUnit4之前，测试用例要运行的数据必须硬编码到测试用例中。这导致了限制，为了使用不同的数据运行测试，测试用例代码必须修改。但是，JUnit4以及TestNG支持外部化测试数据，以便可以针对不同的数据集运行测试用例，而无需更改源代码。 下面的 MathChecker 类有方法可以检查一个数字是否是奇数： public class MathChecker { public Boolean isOdd(int n) { if (n%2 != 0) { return true; } else { return false; } } } 以下是MathChecker类的TestNG测试用例： public class MathCheckerTest { private MathChecker checker; @BeforeMethod public void beforeMethod() { checker = new MathChecker(); } @Test @Parameters(&quot;num&quot;) public void isOdd(int num) { System.out.println(&quot;Running test for &quot;+num); Boolean result = checker.isOdd(num); Assert.assertEquals(result, new Boolean(true)); } } 以下是testng.xml（用于TestNG的配置文件），它具有要为其执行测试的数据： &lt; class=&quot;hljs xml&quot;&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;suite name=&quot;ParameterExampleSuite&quot; parallel=&quot;false&quot;&gt; &lt;test name=&quot;MathCheckerTest&quot;&gt; &lt;classes&gt; &lt;parameter name=&quot;num&quot; value=&quot;3&quot;&gt;&lt;/parameter&gt; &lt;class name=&quot;com.stormpath.demo.MathCheckerTest&quot;/&gt; &lt;/classes&gt; &lt;/test&gt; &lt;test name=&quot;MathCheckerTest1&quot;&gt; &lt;classes&gt; &lt;parameter name=&quot;num&quot; value=&quot;7&quot;&gt;&lt;/parameter&gt; &lt;class name=&quot;com.stormpath.demo.MathCheckerTest&quot;/&gt; &lt;/classes&gt; &lt;/test&gt; &lt;/suite&gt; 可以看出，在这种情况下，测试将执行两次，值3和7各一次。除了通过XML配置文件指定测试数据之外，还可以通过DataProvider注释在类中提供测试数据。 与TestNG类似，测试数据也可以外部化用于JUnit。以下是与上述相同MathChecker类的JUnit测试用例： @RunWith(Parameterized.class) public class MathCheckerTest { private int inputNumber; private Boolean expected; private MathChecker mathChecker; @Before public void setup(){ mathChecker = new MathChecker(); } // Inject via constructor public MathCheckerTest(int inputNumber, Boolean expected) { this.inputNumber = inputNumber; this.expected = expected; } @Parameterized.Parameters public static Collection&lt;Object[]&gt; getTestData() { return Arrays.asList(new Object[][]{ {1, true}, {2, false}, {3, true}, {4, false}, {5, true} }); } @Test public void testisOdd() { System.out.println(“Running test for:”+inputNumber); assertEquals(mathChecker.isOdd(inputNumber), expected); } } 可以看出，要对其执行测试的测试数据由getTestData（）方法指定。此方法可以轻松地修改为从外部文件读取数据，而不是硬编码数据。这样避免了重复写测试框架的麻烦,只需要外部文件中读取测试数据即可. 使用断言而不是Print语句许多新手开发人员习惯于在每行代码之后编写System.out.println语句来验证代码是否正确执行。这种做法常常扩展到单元测试，从而导致测试代码变得杂乱。除了混乱，这需要开发人员手动干预去验证控制台上打印的输出，以检查测试是否成功运行。更好的方法是使用自动指示测试结果的断言。 下面的 StringUti 类是一个简单类，有一个连接两个输入字符串并返回结果的方法： public class StringUtil { public String concat(String a,String b) { return a + b; } } 以下是上述方法的两个单元测试： @Test public void testStringUtil_Bad() { String result = stringUtil.concat(&quot;Hello &quot;, &quot;World&quot;); System.out.println(&quot;Result is &quot;+result); } @Test public void testStringUtil_Good() { String result = stringUtil.concat(&quot;Hello &quot;, &quot;World&quot;); assertEquals(&quot;Hello World&quot;, result); } testStringUtil_Bad将始终传递，因为它没有断言。开发人员需要手动地在控制台验证测试的输出。如果方法返回错误的结果并且不需要开发人员干预，则testStringUtil_Good将失败。 一开始，是通过输出结果判断输出结果是否正确来判断，console虽然输出是true，false 和预期的是一样，但是junit显示都是成功的，并没有出现报错，达不到使用junit测试的效果。 junit 出现failures 和 errors 的情况： Failure指的是由于预期的结果与实际运行的测试的结果不同而导致的，例如当使用assertEquals()或其它assertXXX()方法断言失败时，就会报出Failure，如果发现Faulure，你就要去检查你的测试方法或者是被测试方法中编写的逻辑是否有误。 Error指的是编写程序时没有考虑到的问题。在执行测试的断言之前，程序就因为某种类型的意外而停止，比喻说我们在操作数组的时候，因为存取超出索引会引发ArrayIndexOutOfBoundsException，这个时候程序就会报出Error，程序将无法运行下去，提前结束，这个时候你要检查被测试方法中是不是有欠缺考虑到地方。 Junit的断言方法： assertEquals 和 assertTrue 区别 相同之处：都能判断两个值是否相等assertTrue 如果为true，则运行success，反之FailureassertEquals 如果预期值与真实值相等，则运行success，反之Failure 不同之处：assertEquals 运行Failure会有错误提示，提示预期值是xxx，而实际值是xxx。容易调式assertTrue 没有错误提示 两种方法都可以判断，一般建议使用assertEquals 容易调试 构建具有确定性结果的测试一些方法不具有确定性结果，即该方法的输出不是预先知道的，并且每一次都可以改变。例如，考虑以下代码，它有一个复杂的函数和一个计算执行复杂函数所需时间（以毫秒为单位）的方法： public class DemoLogic { private void veryComplexFunction(){ //This is a complex function that has a lot of database access and is time consuming //To demo this method, I am going to add a Thread.sleep for a random number of milliseconds try { int time = (int) (Math.random()*100); Thread.sleep(time); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } public long calculateTime(){ long time = 0; long before = System.currentTimeMillis(); veryComplexFunction(); long after = System.currentTimeMillis(); time = after - before; return time; } } 在这种情况下，每次执行 calculateTime 方法时，它将返回一个不同的值。为该方法编写测试用例不会有任何用处，因为该方法的输出是可变的。因此，测试方法将不能验证任何特定执行的输出。 除了正面情景外，还要测试负面情景和边缘情况 通常，开发人员会花费大量的时间和精力编写测试用例，以确保应用程序按预期工作。然而，测试负面测试用例也很重要。负面测试用例指的是测试系统是否可以处理无效数据的测试用例。例如，考虑一个简单的函数，它能读取长度为8的字母数字值，由用户键入。除了字母数字值，应测试以下负面测试用例： 用户指定非字母数字值，如特殊字符。用户指定空值。用户指定大于或小于8个字符的值。类似地，边界测试用例测试系统是否适用于极端值。例如，如果用户希望输入从1到100的数字值，则1和100是边界值，对这些值进行测试系统是非常重要的。 本文转载自：http://www.linuxprobe.com/write-java-test.html JUnit是由 Erich Gamma 和 Kent Beck 编写的一个回归测试框架（regression testing framework）。Junit测试是程序员测试，即所谓白盒测试，因为程序员知道被测试的软件如何（How）完成功能和完成什么样（What）的功能。Junit是一套框架，继承TestCase类，就可以用Junit进行自动测试了。 推荐这个文章 http://hao.jobbole.com/junit/ 官方手册： https://junit.org/junit4/cookbook.html API手册： https://junit.org/junit4/javadoc/latest/ 新手指南： https://github.com/junit-team/junit4/wiki/Getting-started 关于JAVA springMVC开发中的常用测试模块在java开发中，WEB端开发会用到框架来开发，在开发的过程中要不断的通过重启服务器来修改配置及页面访问控制，这给系统造成很大的负担，那么如何通过不启动服务器就可以完成页面访问测试呢？ 在常用的测试框架中Junit就提供了很好的功能API–MockMVC。 基于RESTful风格的SpringMVC的测试，我们可以测试完整的Spring MVC流程，即从URL请求到控制器处理，再到视图渲染都可以测试。 目前主流的Web MVC框架除了Struts之外就是SpringMVC，不过要想灵活运用SpringMVC来应对大多说的web开发除了必须掌握其配置和原理外还需要会测试。在Spring3.2版本之前测试一般都是直接new控制器注入依赖进行判断返回，但无法测试基础配置参数如拦截器、数据绑定、类型转换等等。（struts2只需要了解即可，现在市面上基本不用Struts2作为开发框架了，难以维护不过也不排除一些老项目的维护） springboot开发中只需要在测试模块写： @RunWith(SpringJUnit4ClassRunner.class)//使用 Spring-Test 框架;在使用所有注释前必须使用@RunWith(SpringJUnit4ClassRunner.class),让测试运行于Spring测试环境。 @WebAppConfiguration()//由于是web项目，Junit需要模拟ServletContext，需要给测试类加上@WebAppConfiguration注解//@WebAppConfiguration是一级注释，用于声明一个ApplicationContext集成测试加载WebApplicationContext。作用是模拟ServletContext 这两个注解即可使用MockMVC来完成模拟测试。（如果不用Springboot框架只用springMVC的话，那就要麻烦些）具体参考：https://baijiahao.baidu.com/s?id=1578508515485033349&amp;wfr=spider&amp;for=pc http://www.cnblogs.com/lyy-2016/p/6122144.html 如果不是springboot项目的话需要加@ContextConfiguration这个注解，因为springboot的配置文件都在中央配置文件里，而springMVC的配置文件则是分散的，所以这就需要声明要测试的组件的配置文件在哪里。一下我归纳的都是springboot项目的MockMVC测试。 那么我们该如何使用MockMVC去模拟页面访问控制呢？我们以前的做法是通过前端页面（HTML或JSP的FORM属性指定请求方式和发送什么数据，更高级的话采用浏览器PostMan插件来模拟）有了MockMVC以后我们就可以不用到前端页面和启动服务器的情况下来做测试了。 如何使用MockMVCMockMvcBuilder是用来构造MockMvc的构造器，其主要有两个实现：StandaloneMockMvcBuilder和DefaultMockMvcBuilder，StandaloneMockMvcBuilder继承了DefaultMockMvcBuilder。 分别对应两种测试方式，即独立安装和集成Web环境测试（此种方式并不会集成真正的web环境，而是通过相应的Mock API进行模拟测试，无须启动服务器）。对于我们来说直接使用静态工厂MockMvcBuilders创建即可。 StandaloneMockMvcBuilder对应独立安装DefaultMockMvcBuilder（默认的MockMVC工厂）对应WEB集成环境 关于这两种方式的应用场景嘛，我觉得普通测试的话两者并没区别，以后再总结。。。 集成Web环境方式MockMvcBuilders.webAppContextSetup(new XXXController().build();：指定XXXController，将会从该上下文获取相应的控制器并得到相应的MockMvc；通常我们是在Test案例中注入要测试的Controller来代入的 比如： @RunWith(SpringJUnit4ClassRunner.class) @WebAppConfiguration public class HelloControllerTest(){ //注入要测试的HelloController @Autowired private HelloController helloController; @Autowired private UserController userController; private MockMvc mvc;//创建MockMvc以便于在每个测试方法中调用 @Before public void setUp() throws Exception{ //用webAppContextSetup创建这个Controller的Mock（模拟）,创建一个MockMvc进行测试 //如果有多个Controller也可以多次调用webAppContextSetup方法进行多次创建 mvc=MockMvcBuilders.webAppContextSetup(helloController,userController).build(); } //测试案例 @Test 。。。 } 独立测试方式MockMvcBuilders.standaloneSetup(Object… controllers)：通过参数指定一组控制器，这样就不需要从上下文获取了； 通过MockMvcBuilders.standaloneSetup模拟一个Mvc测试环境，通过build得到一个MockMvc 跟上面的集成方法代入参数一样，只是调用的方法不一致 MockMvc测试看一个例子用于测试： @Test public void testView() throws Exception { //MockMvc对象来实现 mvc.perform(MockMvcRequestBuilders.get(&quot;/user/1&quot;)) .andExpect(MockMvcResultMatchers.view().name(&quot;user/view&quot;)) .andExpect(MockMvcResultMatchers.model().attributeExists(&quot;user&quot;)) .andDo(MockMvcResultHandlers.print()) .andReturn(); Assert.assertNotNull(result.getModelAndView().getModel().get(&quot;user&quot;)); } perform：执行一个RequestBuilder请求，会自动执行SpringMVC的流程并映射到相应的控制器执行处理；andExpect：添加ResultMatcher验证规则，验证控制器执行完成后结果是否正确；andDo：添加ResultHandler结果处理器，比如调试时打印结果到控制台；andReturn：最后返回相应的MvcResult；然后进行自定义验证/进行下一步的异步处理； @Test public void testUserController() throws Exception { // 测试UserController mvc.perform(RequestBuilder.get(&quot;/users/&quot;)) .andExpect(status().isOk()) .andExpect(content().string(equalTo(&quot;[]&quot;))); } RequestBuilder/MockMvcRequestBuilders都可以来模拟请求 从名字可以看出，RequestBuilder用来构建请求的，其提供了一个方法buildRequest(ServletContext servletContext)用于构建MockHttpServletRequest；其主要有两个子类MockHttpServletRequestBuilder和MockMultipartHttpServletRequestBuilder（如文件上传使用），即用来Mock客户端请求需要的所有数据。 MockMvcRequestBuilders主要APIMockHttpServletRequestBuilder get(String urlTemplate, Object… urlVariables)：根据uri模板和uri变量值得到一个GET请求方式的MockHttpServletRequestBuilder；如get(/user/{id}, 1L)； MockHttpServletRequestBuilder post(String urlTemplate, Object… urlVariables)：同get类似，但是是POST方法； MockHttpServletRequestBuilder put(String urlTemplate, Object… urlVariables)：同get类似，但是是PUT方法； MockHttpServletRequestBuilder delete(String urlTemplate, Object… urlVariables) ：同get类似，但是是DELETE方法； MockHttpServletRequestBuilder options(String urlTemplate, Object… urlVariables)：同get类似，但是是OPTIONS方法； MockHttpServletRequestBuilder request(HttpMethod httpMethod, String urlTemplate, Object… urlVariables)： 提供自己的Http请求方法及uri模板和uri变量，如上API都是委托给这个API； MockMultipartHttpServletRequestBuilder fileUpload(String urlTemplate, Object… urlVariables)：提供文件上传方式的请求，得到MockMultipartHttpServletRequestBuilder； RequestBuilder asyncDispatch(final MvcResult mvcResult)：创建一个从启动异步处理的请求的MvcResult进行异步分派的RequestBuilder； MockHttpServletRequestBuilder和MockMultipartHttpServletRequestBuilder API(1)MockHttpServletRequestBuilder API MockHttpServletRequestBuilder header(String name, Object… values)/MockHttpServletRequestBuilder headers(HttpHeaders httpHeaders)：添加头信息；MockHttpServletRequestBuilder contentType(MediaType mediaType)：指定请求的contentType头信息；MockHttpServletRequestBuilder accept(MediaType… mediaTypes)/MockHttpServletRequestBuilder accept(String… mediaTypes)：指定请求的Accept头信息；MockHttpServletRequestBuilder content(byte[] content)/MockHttpServletRequestBuilder content(String content)：指定请求Body体内容；MockHttpServletRequestBuilder cookie(Cookie… cookies)：指定请求的Cookie；MockHttpServletRequestBuilder locale(Locale locale)：指定请求的Locale；MockHttpServletRequestBuilder characterEncoding(String encoding)：指定请求字符编码；MockHttpServletRequestBuilder requestAttr(String name, Object value) ：设置请求属性数据；MockHttpServletRequestBuilder sessionAttr(String name, Object value)/MockHttpServletRequestBuilder sessionAttrs(Map&lt;string, object=””&gt; sessionAttributes)：设置请求session属性数据；MockHttpServletRequestBuilder flashAttr(String name, Object value)/MockHttpServletRequestBuilder flashAttrs(Map&lt;string, object=””&gt; flashAttributes)：指定请求的flash信息，比如重定向后的属性信息；MockHttpServletRequestBuilder session(MockHttpSession session) ：指定请求的Session；MockHttpServletRequestBuilder principal(Principal principal) ：指定请求的Principal；MockHttpServletRequestBuilder contextPath(String contextPath) ：指定请求的上下文路径，必须以“/”开头，且不能以“/”结尾；MockHttpServletRequestBuilder pathInfo(String pathInfo) ：请求的路径信息，必须以“/”开头；MockHttpServletRequestBuilder secure(boolean secure)：请求是否使用安全通道；MockHttpServletRequestBuilder with(RequestPostProcessor postProcessor)：请求的后处理器，用于自定义一些请求处理的扩展点； (2)MockMultipartHttpServletRequestBuilder继承自MockHttpServletRequestBuilder，又提供了如下API MockMultipartHttpServletRequestBuilder file(String name, byte[] content)/MockMultipartHttpServletRequestBuilder file(MockMultipartFile file)：指定要上传的文件； ResultActions调用MockMvc.perform(RequestBuilder requestBuilder)后将得到ResultActions，通过ResultActions完成如下三件事：ResultActions andExpect(ResultMatcher matcher) ：添加验证断言来判断执行请求后的结果是否是预期的；ResultActions andDo(ResultHandler handler) ：添加结果处理器，用于对验证成功后执行的动作，如输出下请求/结果信息用于调试；MvcResult andReturn() ：返回验证成功后的MvcResult；用于自定义验证/下一步的异步处理； ResultMatcher/MockMvcResultMatchers1.ResultMatcher用来匹配执行完请求后的结果验证，其就一个match(MvcResult result)断言方法，如果匹配失败将抛出相应的异常；spring mvc测试框架提供了很多ResultMatchers来满足测试需求。注意这些ResultMatchers并不是ResultMatcher的子类，而是返回ResultMatcher实例的。Spring mvc测试框架为了测试方便提供了MockMvcResultMatchers静态工厂方法方便操作； 2.具体的API如下：HandlerResultMatchers handler()：请求的Handler验证器，比如验证处理器类型/方法名；此处的Handler其实就是处理请求的控制器；RequestResultMatchers request()：得到RequestResultMatchers验证器；ModelResultMatchers model()：得到模型验证器；ViewResultMatchers view()：得到视图验证器；FlashAttributeResultMatchers flash()：得到Flash属性验证；StatusResultMatchers status()：得到响应状态验证器；HeaderResultMatchers header()：得到响应Header验证器；CookieResultMatchers cookie()：得到响应Cookie验证器；ContentResultMatchers content()：得到响应内容验证器；JsonPathResultMatchers jsonPath(String expression, Object … args)/ResultMatcher jsonPath(String expression, Matcher matcher)：得到Json表达式验证器；XpathResultMatchers xpath(String expression, Object… args)/XpathResultMatchers xpath(String expression, Map&lt;string, string=””&gt; namespaces, Object… args)：得到Xpath表达式验证器；ResultMatcher forwardedUrl(final String expectedUrl)：验证处理完请求后转发的url（绝对匹配）；ResultMatcher forwardedUrlPattern(final String urlPattern)：验证处理完请求后转发的url（Ant风格模式匹配，@since spring4）；ResultMatcher redirectedUrl(final String expectedUrl)：验证处理完请求后重定向的url（绝对匹配）；ResultMatcher redirectedUrlPattern(final String expectedUrl)：验证处理完请求后重定向的url（Ant风格模式匹配，@since spring4）； 一些常用测试测试普通控制器mockMvc.perform(get(&quot;/user/{id}&quot;, 1)) //执行请求 .andExpect(model().attributeExists(&quot;user&quot;)) //验证存储模型数据 .andExpect(view().name(&quot;user/view&quot;)) //验证viewName .andExpect(forwardedUrl(&quot;/WEB-INF/jsp/user/view.jsp&quot;))//验证视图渲染时forward到的jsp .andExpect(status().isOk())//验证状态码 .andDo(print()); //输出MvcResult到控制台 得到MvcResult自定义验证MvcResult result = mockMvc.perform(get(&quot;/user/{id}&quot;, 1))//执行请求 .andReturn(); //返回MvcResult Assert.assertNotNull(result.getModelAndView().getModel().get(&quot;user&quot;)); //自定义断言 验证请求参数绑定到模型数据及Flash属性mockMvc.perform(post(&quot;/user&quot;).param(&quot;name&quot;, &quot;zhang&quot;)) //执行传递参数的POST请求(也可以post(&quot;/user?name=zhang&quot;)) .andExpect(handler().handlerType(UserController.class)) //验证执行的控制器类型 .andExpect(handler().methodName(&quot;create&quot;)) //验证执行的控制器方法名 .andExpect(model().hasNoErrors()) //验证页面没有错误 .andExpect(flash().attributeExists(&quot;success&quot;)) //验证存在flash属性 .andExpect(view().name(&quot;redirect:/user&quot;)); //验证视图 文件上传byte[] bytes = new byte[] {1, 2}; mockMvc.perform(fileUpload(&quot;/user/{id}/icon&quot;, 1L).file(&quot;icon&quot;, bytes)) //执行文件上传 .andExpect(model().attribute(&quot;icon&quot;, bytes)) //验证属性相等性 .andExpect(view().name(&quot;success&quot;)); //验证视图 JSON请求/响应验证String requestBody = &quot;{\&quot;id\&quot;:1, \&quot;name\&quot;:\&quot;zhang\&quot;}&quot;; mockMvc.perform(post(&quot;/user&quot;) .contentType(MediaType.APPLICATION_JSON).content(requestBody) .accept(MediaType.APPLICATION_JSON)) //执行请求 .andExpect(content().contentType(MediaType.APPLICATION_JSON)) //验证响应contentType .andExpect(jsonPath(&quot;$.id&quot;).value(1)); //使用Json path验证JSON 请参考http://goessner.net/articles/JsonPath/ String errorBody = &quot;{id:1, name:zhang}&quot;; MvcResult result = mockMvc.perform(post(&quot;/user&quot;) .contentType(MediaType.APPLICATION_JSON).content(errorBody) .accept(MediaType.APPLICATION_JSON)) //执行请求 .andExpect(status().isBadRequest()) //400错误请求 .andReturn(); Assert.assertTrue(HttpMessageNotReadableException.class.isAssignableFrom(result.getResolvedException().getClass()));//错误的请求内容体 异步测试//Callable MvcResult result = mockMvc.perform(get(&quot;/user/async1?id=1&amp;name=zhang&quot;)) //执行请求 .andExpect(request().asyncStarted()) .andExpect(request().asyncResult(CoreMatchers.instanceOf(User.class))) //默认会等10秒超时 .andReturn(); mockMvc.perform(asyncDispatch(result)) .andExpect(status().isOk()) .andExpect(content().contentType(MediaType.APPLICATION_JSON)) .andExpect(jsonPath(&quot;$.id&quot;).value(1)); 全局配置mockMvc = webAppContextSetup(wac) .defaultRequest(get(&quot;/user/1&quot;).requestAttr(&quot;default&quot;, true)) //默认请求 如果其是Mergeable类型的，会自动合并的哦mockMvc.perform中的RequestBuilder .alwaysDo(print()) //默认每次执行请求后都做的动作 .alwaysExpect(request().attribute(&quot;default&quot;, true)) //默认每次执行后进行验证的断言 .build(); mockMvc.perform(get(&quot;/user/1&quot;)) .andExpect(model().attributeExists(&quot;user&quot;)); 整个测试过程非常有规律：1、准备测试环境2、通过MockMvc执行请求3.1、添加验证断言3.2、添加结果处理器3.3、得到MvcResult进行自定义断言/进行下一步的异步请求4、卸载测试环境 参考实例：https://www.cnblogs.com/MaxElephant/p/8135322.html]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据--VMware搭建虚拟机集群]]></title>
    <url>%2F2018%2F03%2F11%2Fbigdata01%2F</url>
    <content type="text"><![CDATA[什么是IP地址？这个问题，首先你要了解一下什么是IP地址：人们为了通信方便给每一台计算机都事先分配一个类似电话号码一样的标识地址，即IP地址。根据TCP/IP协议，IP地址由32位二进制数组成，而且在INTERNET范围内是唯一的。如：某IP地址为11000000 10101000 00001010 00000010为了方便记忆，人们把32位的IP地址分成四段，每段8位，中间用小数点“.”隔开，然后再将每8位二进制换成十进制，例如：192.168.0.0 就是由每8位二进制转换而成的IP地址又分为：固定IP和动态IP固定IP地址是长期分配给一台计算机或网络设备使用的IP地址。一般来说，采用专线上网的计算机才拥有固定的IP地址。动态IP地址:当您拨号（或以其他方式）连接互联网时，通常您会从您的ISP（互联网服务商）分配到一个动态的IP地址。这意味着您每次连接互联网时得到的IP地址是不同的。尽管这不影响您访问互联网，但是您的朋友、商业伙伴（他们可能这时也在互联网上）却不能访问到您。因为，他们不知道您的计算机在哪里。这就像每个人都有一部电话，但电话号码每天都在改变。 我们常用的电信信号光纤是属于哪种呢？静态IP也叫固定IP，一般运营商用来作为专线宽带提供给企业用户。价格较高。是从用户处直接光纤到运营商机房，用户专享开通的带宽，带宽不受周围用户的影响。静态IP地址一般都用在专线网络上，比如网吧所用的网线。电信公司通常会给网吧分配固定的IP地址，永远都不会变化的，像这种网络就要在路由器上输IP地址来连接。价格和质量也是不一样的哦，要不然怎么能带几十台甚至是上百台电脑。 动态IP是普通用户常用的方式，光纤入户接光猫，通过光猫拨号上网。例如家庭用户开通的10M宽带，是从用户家到运营商机房交换机开通的是10M，交换机上联的端口是小区用户共享，如果小区内其他用户很少，那你这10M是和专线一样的。但如果小区内用户多了，大家共用交换机总出口带宽，可能实际就达不到10M带宽了。当你从邻居家路由器那里连一条线过来，然后自己又想再用一个路由器分几台电脑，上头的路由器用的是DHCP分配IP地址的，那下头的路由器就要用动态IP，让路由器自动获取IP地址。不用输入任何东西。设置动态IP连接有一点要非常非常注意的地方。就是要更改LAN接口的IP地址。更改成除192.168.1.1所在的网段之外的其它网段，如192.168.2.1或者172.16.0.1都行。因为现在大部分路由器厂商设置的LAN接口的IP地址都是192.168.1.1，下头路由器WAN接口从上头路由器那里获取的IP地址一般都是192.168.1.1所在的网段，这会和下头LAN接口的网段冲突，获取不到IP地址的，也就是上不了网。说简单点就是一个路由器的WAN外网接口和LAN内网接口不能是同一个网段。 通过cmd命令也可以知道自己的宽带是属于哪种？ 单击开始，在运行中输入cmd。 然后在命令提示符输入‘ipconfig空格/all’查看本地连接‘dhcp Enable’这项（如下图） 如果为NO则是静态的，如果为yes则是动态的。 VMwareVMware,Inc. (Virtual Machine ware）是一个“虚拟PC”软件公司，提供服务器、桌面虚拟化的解决方案。其虚拟化平台的产品包括播放器；它能使个人用台式电脑运行虚拟机器，融合器，它是用户基于英特尔结构苹果机的桌面虚拟化产品，工作站的软件开发商和企业的资讯科技专才，能使虚拟分区的服务器，ESX服务器（一种能直接在硬件上运行的企业级的虚拟平台），虚拟的SMP让一个虚拟机同时使用四个物理处理器，和VMFS使多个ESX服务器分享块存储器。该公司还提供一个虚拟中心来控制和管理虚拟化的IT环境；VMotion 让用户可以移动虚拟机器；DRS从物理处理器创造资源工具；HA 提供从硬件故障自动回复功能；综合备份可使LAN-free自动备份虚拟机器；VMotion存储器可允许虚拟机磁盘自由移动；更新管理器自动更新修补程序和更新管理；能力规划能使VMware的服务供应商执行能力评估；转换器把本地和远程物理仪器转换到虚拟机器；实验室管理可自动化安装，捕捉，存储和共享，多机软件配置；ACE允许桌面系统管理包括公司资源以防止不可控台式电脑带来的风险。虚拟桌面基础设施可主导个人台式电脑在虚拟机运行的中央管理器；虚拟桌面管理，它是联系用户到数据库中的虚拟电脑的桌面管理服务器；VMware 生命管理周期可通过虚拟环境提供控制权，实现计算机的多性能。该公司成立于1998，总部在加州的Palo Alto。它的产品可以使你在一台机器上同时运行二个或更多Windows、DOS、LINUX系统。与“多启动”系统相比，VMWare采用了完全不同的概念。多启动系统在一个时刻只能运行一个系统，在系统切换时需要重新启动机器。VMWare是真正“同时”运行，多个操作系统在主系统的平台上，就象标准Windows应用程序那样切换。而且每个操作系统你都可以进行虚拟的分区、配置而不影响真实硬盘的数据，你甚至可以通过网卡将几台虚拟机用网卡连接为一个局域网，极其方便。安装在VMware操作系统性能上比直接安装在硬盘上的系统低不少，因此，比较适合学习和测试。 如何使用VMware虚拟机软件搭建伪机集群？因为要用到大数据的Hadoop框架进行学习，我们要利用VMware来搭建3台伪机(Linux系统)以供学习Hadoop的虚拟机环境分为apache版本和CDH版本的apache版本的比较不吃硬件设备，可以通过伪机群来实现大数据框架，而CDH版本则吃硬件，需要实机搭建。 搭建三台Linux系统伪机CentOSCentOS（Community Enterprise Operating System，中文意思是：社区企业操作系统）是Linux发行版之一，它是来自于Red Hat Enterprise Linux依照开放源代码规定释出的源代码所编译而成。由于出自同样的源代码，因此有些要求高度稳定性的服务器以CentOS替代商业版的Red Hat Enterprise Linux使用。两者的不同，在于CentOS并不包含封闭源代码软件。CentOS 是一个基于Red Hat Linux 提供的可自由使用源代码的企业级Linux发行版本。每个版本的 CentOS都会获得十年的支持（通过安全更新方式）。新版本的 CentOS 大约每两年发行一次，而每个版本的 CentOS 会定期（大概每六个月）更新一次，以便支持新的硬件。这样，建立一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。[1] CentOS是Community Enterprise Operating System的缩写。CentOS 是RHEL（Red Hat Enterprise Linux）源代码再编译的产物，而且在RHEL的基础上修正了不少已知的 Bug ，相对于其他 Linux 发行版，其稳定性值得信赖。 除了CentOS是Linux的发行版之一发行版为许多不同的目的而制作, 包括对不同计算机结构的支持, 对一个具体区域或语言的本地化，实时应用，和嵌入式系统，甚至许多版本故意地只加入免费软件。已经有超过三百个发行版被积极的开发，最普遍被使用的发行版有大约十二个。 Fedora CoreFedora Core（自第七版直接更名为Fedora）是众多 Linux 发行版之一。它是一套从Red Hat Linux发展出来的免费Linux系统。Fedora Core 的前身就是Red Hat Linux。Fedora是一个开放的、创新的、前瞻性的操作系统和平台，基于Linux。它允许任何人自由地使用、修改和重发布，无论现在还是将来。它由一个强大的社群开发，这个社群的成员以自己的不懈努力，提供并维护自由、开放源码的软件和开放的标准。Fedora 项目由 Fedora 基金会管理和控制，得到了 Red Hat, Inc. 的支持。Fedora 是一个独立的操作系统，是Linux的一个发行版，可运行的体系结构包括 x86(即i386-i686), x86_64 和 PowerPC。 DebianDebian Project诞生于1993年8月13日，它的目标是提供一个稳定容错的Linux版本。支持Debian的不是某家公司，而是许多在其改进过程中投入了大量时间的开发人员，这种改进吸取了早期Linux的经验。Debian以其稳定性著称，虽然它的早期版本Slink有一些问题，但是它的现有版本Potato已经相当稳定了。这个版本更多的使用了 pluggable authentication modules (PAM)，综合了一些更易于处理的需要认证的软件（如winbind for Samba）。Debian的安装完全是基于文本的，对于其本身来说这不是一件坏事。但对于初级用户来说却并非这样。因为它仅仅使用fdisk 作为分区工具而没有自动分区功能，所以它的磁盘分区过程令人十分讨厌。磁盘设置完毕后，软件工具包的选择通过一个名为dselect的工具实现，但它不向用户提供安装基本工具组（如开发工具）的简易设置步骤。最后需要使用anXious工具配置X Windows，这个过程与其他版本的X Windows配置过程类似。完成这些配置后，Debian就可以使用了。Debian主要通过基于Web的论坛和邮件列表来提供技术支持。作为服务器平台，Debian提供一个稳定的环境。为了保证它的稳定性，开发者不会在其中随意添加新技术，而是通过多次测试之后才选定合适的技术加入。当前最新正式版本是Debian 6，采用的内核是Linux 2.6.32。Debian 6 第一次 包含了一个100%开源的Linux内核，这个内核中不再包含任何闭源的硬件驱动。所有的闭源软件都被隔离成单独的软件包，放到Debian软件源的 “non-free” 部分。由此，Debian用户便可以自由地选择是使用一个完全开源的系统还是添加一些闭源驱动。[1] MandrakeMandrakeSoft，Linux Mandrake的发行商，在1998年由一个推崇Linux的小组创立，它的目标是尽量让工作变得更简单。最终，Mandrake给人们提供了一个优秀的图形安装界面，它的最新版本还包含了许多Linux软件包。作为Red Hat Linux的一个分支，Mandrake将自己定位在桌面市场的最佳Linux版本上。但该公司还是支持服务器上的安装，而且成绩并不坏。Mandrake的安装非常简单明了，为初级用户设置了简单的安装选项。它完全使用GUI界面，还为磁盘分区制作了一个适合各类用户的简单GUI界面。软件包的选择非常标准，另外还有对软件组和单个工具包的选项。安装完毕后，用户只需重启系统并登录进入即可。Mandrake主要通过邮件列表和Mandrak 自己的Web论坛提供技术支持。Mandrak对桌面用户来说是一个非常不错的选择，它还可作为一款优秀的服务器系统，尤其适合Linux新手使用。它使用最新版本的内核，拥有许多用户需要在Linux服务器环境中使用的软件——数据库和Web服务器。Mandrak没有重大的软件缺陷，只是它更加关注桌面市场，较少关注服务器市场。 UbuntuUbuntu是一个以桌面应用为主的Linux操作系统，其名称来自非洲南部祖鲁语或豪萨语的“ubuntu”一词（译为吾帮托或乌班图），意思是“人性”、“我的存在是因为大家的存在”，是非洲传统的一种价值观，类似华人社会的“仁爱”思想。Ubuntu基于Debian发行版和unity桌面环境，与Debian的不同在于它每6个月会发布一个新版本。Ubuntu的目标在于为一般用户提供一个最新的、同时又相当稳定的主要由自由软件构建而成的操作系统。Ubuntu具有庞大的社区力量，用户可以方便地从社区获得帮助。随着云计算的流行，ubuntu推出了一个云计算环境搭建的解决方案，可以在其官方网站找到相关信息。于2012年4月26日发布最终版ubuntu 12.04，ubuntu 12.04是长期支持的版本。 Red Hat Linux可能这是最著名的Linux版本了，Red Hat Linux已经创造了自己的品牌，越来越多的人听说过它。Red Hat在1994年创业，当时聘用了全世界500多名员工，他们都致力于开放的源代码体系。Red Hat Linux是公共环境中表现上佳的服务器。它拥有自己的公司，能向用户提供一套完整的服务，这使得它特别适合在公共网络中使用。这个版本的Linux也使用最新的内核，还拥有大多数人都需要使用的主体软件包。Red Hat Linux的安装过程也十分简单明了。它的图形安装过程提供简易设置服务器的全部信息。磁盘分区过程可以自动完成，还可以选择GUI工具完成，即使对于 Linux新手来说这些都非常简单。选择软件包的过程也与其他版本类似；用户可以选择软件包种类或特殊的软件包。系统运行起来后，用户可以从Web站点和 Red Hat那里得到充分的技术支持。我发现Red Hat是一个符合大众需求的最优版本。在服务器和桌面系统中它都工作得很好。Red Hat的唯一缺陷是带有一些不标准的内核补丁，这使得它难于按用户的需求进行定制。 Red Hat通过论坛和邮件列表提供广泛的技术支持，它还有自己公司的电话技术支持，后者对要求更高技术支持水平的集团客户更有吸引力。 SuSE总部设在德国的SuSE AG在商界已经奋斗了8年多，它一直致力于创建一个连接数据库的最佳Linux版本。为了实现这一目的，SuSE与Oracle 和IBM合作，以使他们的产品能稳定地工作。SuSE还开发了SuSE Linux eMail Server III，一个非常稳定的电子邮件群组应用。基于2.4.10内核的SuSE 7.3，在原有版本的基础上提高了易用性。安装过程通过GUI完成，磁盘分区过程也非常简单，但它没有为用户提供更多的控制和选择。在SuSE 操作系统下，可以非常方便地访问Windows磁盘，这使得两种平台之间的切换，以及使用双系统启动变得更容易。SuSE的硬件检测非常优秀，该版本在服务器和工作站上都用得很好。SuSE拥有界面友好的安装过程，还有图形管理工具，可方便地访问Windows磁盘，对于终端用户和管理员来说使用它同样方便，这使它成为了一个强大的服务器平台。 SuSE也通过基于Web的论坛提供技术支持，另外我还发现它有电话技术支持。 Linux MintLinux Mint是一份基于Ubuntu的发行版，其目标是提供一种更完整的即刻可用体验，这包括提供浏览器插件、多媒体编解码器、对DVD播放的支持、Java和其他组件。它与Ubuntu软件仓库兼容。Linux Mint 是一个为pc和X86电脑设计的操作系统。因此，一个可以跑得动Windows的电脑也可以使用Linux Mint来代替Windows，或者两个都跑。既有Windows又有Linux的系统就是传说中的“双系统”。同样，MAC，BSD或者其他的Linux版本也可以和Linux Mint 共存。一台装有多系统的电脑在开机的时候会出现一个供你选择操作系统的菜单。Linux Mint可以很好的在一个单系统的电脑上运行，但是它也可以自动检测其他操作系统并与其互动，例如，如果你安装Linux Mint在一个安装了Windows版本的（xp，vista或者其他版本），它会自动检测并建立双启动以供您在开机的时候选择启动哪个系统。并且你可以在Linux Mint下访问Windows分区。Linux是更安全，更稳定，更有效并且日益易于操作的甚至可以和Windows相媲美的系统，它越来越让人感到难以抉择了。 GentooGentoo是Linux世界最年轻的发行版本，正因为年轻，所以能吸取在她之前的所有发行版本的优点。Gentoo最初由Daniel Robbins（FreeBSD的开发者之一）创建，首个稳定版本发布于2002年。由于开发者对FreeBSD的熟识，所以Gentoo拥有媲美FreeBSD的广受美誉的ports系统 ——Portage包管理系统。 centosCentOS（Community ENTerprise Operating System）是Linux发行版之一，它是来自于Red Hat Enterprise Linux依照开放源代码规定释出的源代码所编译而成。由于出自同样的源代码，因此有些要求高度稳定性的服务器以CentOS替代商业版的Red Hat Enterprise Linux使用。两者的不同，在于CentOS并不包含封闭源代码软件,CentOS 是一个基于Red Hat Linux 提供的可自由使用源代码的企业级Linux发行版本。每个版本的 CentOS都会获得十年的支持（通过安全更新方式）。新版本的 CentOS 大约每两年发行一次，而每个版本的 CentOS 会定期（大概每六个月）更新一次，以便支持新的硬件。这样，建立一个安全、低维护、稳定、高预测性、高重复性的 Linux 环境。CentOS是Community Enterprise Operating System的缩写。CentOS 是RHEL（Red Hat Enterprise Linux）源代码再编译的产物，而且在RHEL的基础上修正了不少已知的 Bug ，相对于其他 Linux 发行版，其稳定性值得信赖。RHEL 在发行的时候，有两种方式。一种是二进制的发行方式，另外一种是源代码的发行方式。 CentOS在2014初，宣布加入Red Hat。 Red HatRed Hat（红帽）公司（NYSE：RHT）是一家开源解决方案供应商，也是标准普尔500指数成员。总部位于美国北卡罗来纳州的罗利市，截止2015年3月3日，共有80多个分公司。红帽公司为诸多重要IT技术如操作系统、存储、中间件、虚拟化和云计算提供关键任务的软件与服务。红帽的开放源码模式提供跨物理、虚拟和云端环境的企业运算解决方案，以帮助企业降低成本并提升效能、稳定性与安全性。红帽公司同时也为全球客户或通过领先合作伙伴为客户提供技术支持、培训和咨询服务。红帽的商业模式，简单的说就是红帽将开源社区项目产品化，使普通企业客户更容易消费开源创新技术的一种方法。 从用户角度来看，不同的投资预算与研发能力的企业都可以通过红帽获得开源软件的价值。 红帽产品涉及5大技术领域：云计算、存储、虚拟化、中间件、操作系统。 云计算（1）红帽企业Linux OpenStack平台（2）红帽OpenShift（PASS 产品）（3）红帽CloudForms （混合云管理平台）（4）红帽云基础架构（Cloud Infrastructure）存储（1）红帽Inktank Ceph Enterprise为部署公有云或私有云的企业（包括许多OpenStack的早期采用者）提供了对象和数据块存储软件。（2）红帽存储服务器（Red Hat Storage Server）的先进性能可充分适应数据密集型企业的任务负载，满足包括大数据、运营分析、企业文件共享与协同等在内的数据处理。虚拟化红帽企业虚拟化产品提供了先进的开源企业虚拟化功能，使客户能够优化传统的虚拟化任务负载，为通过OpenStack实现云部署提供一个入口。红帽企业虚拟化帮助企业客户实现传统虚拟化基础架构的流线化和优化，同时为私有云能力奠定了基础。中间件红帽JBoss中间件通过提供快速构建将人员、流程和信息连接在一起的系统所需的工具，来帮助组织发展其中间件基础架构。红帽JBoss中间件主要产品有红帽JBoss企业应用平台、JBoss Web服务器、JBoss 数据网格、JBoss 开发人员工作室、JBoss门户、JBoss运营网络，JBoss Fuse （企业服务总线-ESB)，JBoss A-MQ（消息中间件）、JBoss数据虚拟化、JBoss Fuse Service Works、JBoss BRMS，JBoss BPM套件等。操作系统（1）红帽企业Linux红帽在2014年6月发布了最新旗舰版企业操作系统——红帽企业Linux 7。基于红帽企业Linux 7操作系统，企业可整合裸机服务器、 虚拟机、基础设施即服务(IaaS)和平台即服务(PaaS)，以构建一个强大稳健的数据中心环境，满足不断变化的业务需求。（2）红帽卫星红帽卫星是一个综合性解决方案，它通过配置软件分发、补丁和配置管理，以及物理、虚拟和云环境的订阅管理为红帽系统提供完整的生命周期管理，为管理构建、部署、运行和淘汰系统所需的工具提供了单独的管理控制台和方法论。 在经过严格的基于表现的考试后，经过认证的红帽专业人员保证了企业从其红帽解决方案中获得最大的回报。（1）红帽认证架构师（RHCA）（2）红帽认证系统管理员 (RHCSA)（3）红帽认证工程师（RHCE） 如果你使用Red Hat软件包管理器来管理Linux软件，应该详细了解Red Hat软件仓库的原理，这有助于使用Linux命令将让你更容易管理Red Hat软件。Linux软件包管理使用的Red Hat软件库，Red Hat提供了软件包的安装源。Red Hat管理器会自动查找库和安装包的依赖关系。如果没有Red Hat软件库管理系统，要解决这些Red Hat软件包的依赖关系是一种烦恼。按目前的Linux分布情况，主要是的软件库管理系统是Red Hat的软件库管理系统，主要用在Red Hat Enterprise Linux（RHEL）和其他Red Hat Linux的衍生系统上，如Fedora、CentOS。Red Hat软件库管理系统使用默认的Red Hat软件包格式。Red Hat用.rpm，这些Red Hat软件包是将软件和元数据压缩和档案。Red Hat元数据提供了有关Red Hat软件的版本信息，以及Red Hat软件包的依赖关系。但Red Hat软件包的数据库与软件库很容易混淆。虽然通过Red Hat软件库管理使这些软件包可以让工作更简单，但这些Red Hat软件包同时有另一个数据库。这意味着Red Hat软件包的信息来源可以有多个：包数据库和软件库数据库。使用Red Hat包数据库工作Red Hat包数据库是软件安装了什么在Linux系统里最重要的信息来源。Red Hat包数据库从服务器上检索当前安装的软件，Red Hat包文件通常是一个丰富的文档和软件使用的信息源，这对于Linux系统Red Hat软件包之间的依赖关系非常有用。在基于Red Hat软件包的Linux服务器，使用Red Hat的rpm命令从数据库里获取信息使用Red Hat软件仓库工作在安装新的Red Hat软件或进行软件升级，系统管理员通常使用Red Hat软件仓库。尤其是更新Red Hat服务器补丁的时候，Red Hat软件仓库会更方便：Red Hat软件包管理器只需要更新新的软件包，使Red Hat自动可用，库用户便可以使用Red Hat，这过程完全是透明的。在基于Red Hat软件仓库的系统，使用Red Hat的yum命令请求包的信息和执行任务，如Red Hat更新和安装软件。Red Hat的yum可以周期性的自动下载和更新软件仓库的索引文件。Red Hat系统会自动比较了软件仓库的Red Hat安装包和安装在本地系统的Red Hat软件包列表，以确定一个Red Hat包已经安装，或者是否有可用的更新。Red Hat软件仓库管理系统在处理软件包依赖关系非常好用：目前所有的Linux都提供了丰富的Red Hat资料库，并提供必要的Red Hat依赖，这意味着Linux管理员对Red Hat软件包的依赖恐惧大大减少了。Red Hat软件仓库中没有的包Red Hat软件仓库中没有的软件，意味着Red Hat安装起来比较困难。管理员可以创建自己的Red Hat软件库和复制本地的软件包。这也让管理员从库安装自定义Red Hat软件包的时候，进一步减少Red Hat依赖问题。 利用VMware搭建centOS环境VMware下安装Linux系统，以CentOS为例]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>计算机硬件基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA开发常用类库--JSON解析库]]></title>
    <url>%2F2018%2F03%2F07%2FJAVA-api%2F</url>
    <content type="text"><![CDATA[JSON是什么？尽管有许多宣传关于 XML 如何拥有跨平台，跨语言的优势，然而，除非应用于 Web Services，否则，在普通的 Web 应用中，开发者经常为 XML 的解析伤透了脑筋，无论是服务器端生成或处理 XML，还是客户端用 JavaScript 解析 XML，都常常导致复杂的代码，极低的开发效率。实际上，对于大多数 Web 应用来说，他们根本不需要复杂的 XML 来传输数据，XML 的扩展性很少具有优势，许多 AJAX 应用甚至直接返回 HTML 片段来构建动态 Web 页面。和返回 XML 并解析它相比，返回 HTML 片段大大降低了系统的复杂性，但同时缺少了一定的灵活性。 现在， JSON 为 Web 应用开发者提供了另一种数据交换格式。让我们来看看 JSON 到底是什么，同 XML 或 HTML 片段相比，JSON 提供了更好的简单性和灵活性。 JSON的数据格式是怎么样的？和 XML 一样，JSON 也是基于纯文本的数据格式。由于 JSON 天生是为 JavaScript 准备的，因此，JSON 的数据格式非常简单，您可以用 JSON 传输一个简单的 String，Number，Boolean，也可以传输一个数组，或者一个复杂的 Object 对象。 String，Number 和 Boolean 用 JSON 表示非常简单。例如，用 JSON 表示一个简单的 String “ abc ”，其格式为： “abc”除了字符 “，\，/ 和一些控制符（\b，\f，\n，\r，\t）需要编码外，其他 Unicode 字符可以直接输出。 下图是一个 String 的完整表示结构： 一个 Number 可以根据整型或浮点数表示如下： 这与绝大多数编程语言的表示方法一致，例如： 12345（整数） -3.9e10（浮点数） Boolean 类型表示为 true 或 false 。此外，JavaScript 中的 null 被表示为 null，注意，true、false 和 null 都没有双引号，否则将被视为一个 String 。JSON 还可以表示一个数组对象，使用 [] 包含所有元素，每个元素用逗号分隔，元素可以是任意的 Value，例如，以下数组包含了一个 String，Number，Boolean 和一个 null： [&quot;abc&quot;,12345,false,null] Object 对象在 JSON 中是用 {} 包含一系列无序的 Key-Value 键值对表示的，实际上此处的 Object 相当于 Java 中的 Map&lt;String, Object&gt;，而不是 Java 的 Class 。注意 Key 只能用 String 表示。例如，一个 Address 对象包含如下 Key-Value： city:Beijing street:Chaoyang Road postcode:100025（整数） 用 JSON 表示如下： {&quot;city&quot;:&quot;Beijing&quot;,&quot;street&quot;:&quot; Chaoyang Road &quot;,&quot;postcode&quot;:100025} 其中 Value 也可以是另一个 Object 或者数组，因此，复杂的 Object 可以嵌套表示，例如，一个 Person 对象包含 name 和 address 对象，可以表示如下： {&quot;name&quot;:&quot;Michael&quot;,&quot;address&quot;: {&quot;city&quot;:&quot;Beijing&quot;,&quot;street&quot;:&quot; Chaoyang Road &quot;,&quot;postcode&quot;:100025} } #JavaScript 处理 JSON 数据上面介绍了如何用 JSON 表示数据，接下来，我们还要解决如何在服务器端生成 JSON 格式的数据以便发送到客户端，以及客户端如何使用 JavaScript 处理 JSON 格式的数据。我们先讨论如何在 Web 页面中用 JavaScript 处理 JSON 数据。我们通过一个简单的 JavaScript 方法就能看到客户端如何将 JSON 数据表示给用户： &apos;function handleJson() { var j={&quot;name&quot;:&quot;Michael&quot;,&quot;address&quot;: {&quot;city&quot;:&quot;Beijing&quot;,&quot;street&quot;:&quot; Chaoyang Road &quot;,&quot;postcode&quot;:100025} }; document.write(j.name); document.write(j.address.city); }&apos; 假定服务器返回的 JSON 数据是上文的： ‘{“name”:”Michael”,”address”: {“city”:”Beijing”,”street”:” Chaoyang Road “,”postcode”:100025} }’ 只需将其赋值给一个 JavaScript 变量，就可以立刻使用该变量并更新页面中的信息了，相比 XML 需要从 DOM 中读取各种节点而言，JSON 的使用非常容易。我们需要做的仅仅是发送一个 Ajax 请求，然后将服务器返回的 JSON 数据赋值给一个变量即可。有许多 Ajax 框架早已包含了处理 JSON 数据的能力，例如 Prototype（一个流行的 JavaScript 库：http://prototypejs.org）提供了 evalJSON() 方法，能直接将服务器返回的 JSON 文本变成一个 JavaScript 变量： new Ajax.Request(&quot;http://url&quot;, { method: &quot;get&quot;, onSuccess: function(transport) { var json = transport.responseText.evalJSON(); // TODO: document.write(json.xxx); } }); #服务器端输出 JSON 格式数据 讲完客户端的JSON处理，接下来到服务端的处理 下面我们讨论如何在服务器端输出 JSON 格式的数据。以 Java 为例，我们将演示将一个 Java 对象编码为 JSON 格式的文本。将 String 对象编码为 JSON 格式时，只需处理好特殊字符即可。另外，必须用 (“) 而非 (‘) 表示字符串： static String string2Json(String s) { StringBuilder sb = new StringBuilder(s.length()+20); sb.append(&apos;\&quot;&apos;); for (int i=0; i&lt;s.length(); i++) { char c = s.charAt(i); switch (c) { case &apos;\&quot;&apos;: sb.append(&quot;\\\&quot;&quot;); break; case &apos;\\&apos;: sb.append(&quot;\\\\&quot;); break; case &apos;/&apos;: sb.append(&quot;\\/&quot;); break; case &apos;\b&apos;: sb.append(&quot;\\b&quot;); break; case &apos;\f&apos;: sb.append(&quot;\\f&quot;); break; case &apos;\n&apos;: sb.append(&quot;\\n&quot;); break; case &apos;\r&apos;: sb.append(&quot;\\r&quot;); break; case &apos;\t&apos;: sb.append(&quot;\\t&quot;); break; default: sb.append(c); } } sb.append(&apos;\&quot;&apos;); return sb.toString(); } 将 Number 表示为 JSON 就容易得多，利用 Java 的多态，我们可以处理 Integer，Long，Float 等多种 Number 格式： static String number2Json(Number number) { return number.toString(); } Boolean 类型也可以直接通过 toString() 方法得到 JSON 的表示： static String boolean2Json(Boolean bool) { return bool.toString(); } 要将数组编码为 JSON 格式，可以通过循环将每一个元素编码出来： static String array2Json(Object[] array) { if (array.length==0) return &quot;[]&quot;; StringBuilder sb = new StringBuilder(array.length &lt;&lt; 4); sb.append(&apos;[&apos;); for (Object o : array) { sb.append(toJson(o)); sb.append(&apos;,&apos;); } // 将最后添加的 &apos;,&apos; 变为 &apos;]&apos;: sb.setCharAt(sb.length()-1, &apos;]&apos;); return sb.toString(); } 最后，我们需要将 Map&lt;String, Object&gt; 编码为 JSON 格式，因为 JavaScript 的 Object 实际上对应的是 Java 的 Map&lt;String, Object&gt; 。该方法如下： static String map2Json(Map&lt;String, Object&gt; map) { if (map.isEmpty()) return &quot;{}&quot;; StringBuilder sb = new StringBuilder(map.size() &lt;&lt; 4); sb.append(&apos;{&apos;); Set&lt;String&gt; keys = map.keySet(); for (String key : keys) { Object value = map.get(key); sb.append(&apos;\&quot;&apos;); sb.append(key); sb.append(&apos;\&quot;&apos;); sb.append(&apos;:&apos;); sb.append(toJson(value)); sb.append(&apos;,&apos;); } // 将最后的 &apos;,&apos; 变为 &apos;}&apos;: sb.setCharAt(sb.length()-1, &apos;}&apos;); return sb.toString(); } 为了统一处理任意的 Java 对象，我们编写一个入口方法 toJson(Object)，能够将任意的 Java 对象编码为 JSON 格式： public static String toJson(Object o) { if (o==null) return &quot;null&quot;; if (o instanceof String) return string2Json((String)o); if (o instanceof Boolean) return boolean2Json((Boolean)o); if (o instanceof Number) return number2Json((Number)o); if (o instanceof Map) return map2Json((Map&lt;String, Object&gt;)o); if (o instanceof Object[]) return array2Json((Object[])o); throw new RuntimeException(&quot;Unsupported type: &quot; + o.getClass().getName()); } 我们并未对 Java 对象作严格的检查。不被支持的对象（例如 List）将直接抛出 RuntimeException 。此外，为了保证输出的 JSON 是有效的，Map&lt;String, Object&gt; 对象的 Key 也不能包含特殊字符。细心的读者可能还会发现循环引用的对象会引发无限递归，例如，精心构造一个循环引用的 Map，就可以检测到 StackOverflowException： @Test(expected=StackOverflowError.class) public void testRecurrsiveMap2Json() { Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(&quot;key&quot;, map); JsonUtil.map2Json(map); } 好在服务器处理的 JSON 数据最终都应该转化为简单的 JavaScript 对象，因此，递归引用的可能性很小。最后，通过 Servlet 或 MVC 框架输出 JSON 时，需要设置正确的 MIME 类型（application/json）和字符编码。假定服务器使用 UTF-8 编码，则可以使用以下代码输出编码后的 JSON 文本： response.setContentType(&quot;application/json;charset=UTF-8&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); PrintWriter pw = response.getWriter(); pw.write(JsonUtil.toJson(obj)); pw.flush(); JSON 已经是 JavaScript 标准的一部分。目前，主流的浏览器对 JSON 支持都非常完善。应用 JSON，我们可以从 XML 的解析中摆脱出来，对那些应用 Ajax 的 Web 2.0 网站来说，JSON 确实是目前最灵活的轻量级方案。 #关于作者# 关于python的学习，可以借鉴这个作家的网络教程来看廖雪峰，十年软件开发经验，业余产品经理，精通Java/Python/Ruby/Scheme/Objective C等，对开源框架有深入研究，著有《Spring 2.0核心技术与最佳实践》一书，多个业余开源项目托管在GitHub https://github.com/michaelliao JSON技术的调研报告 一 、各个JSON技术的简介和优劣1.json-libjson-lib最开始的也是应用最广泛的json解析工具，json-lib 不好的地方确实是依赖于很多第三方包，包括commons-beanutils.jar，commons-collections-3.2.jar，commons-lang-2.6.jar，commons-logging-1.1.1.jar，ezmorph-1.0.6.jar，对于复杂类型的转换，json-lib对于json转换成bean还有缺陷，比如一个类里面会出现另一个类的list或者map集合，json-lib从json到bean的转换就会出现问题。json-lib在功能和性能上面都不能满足现在互联网化的需求。2.开源的Jackson相比json-lib框架，Jackson所依赖的jar包较少，简单易用并且性能也要相对高些。而且Jackson社区相对比较活跃，更新速度也比较快。Jackson对于复杂类型的json转换bean会出现问题，一些集合Map，List的转换出现问题。Jackson对于复杂类型的bean转换Json，转换的json格式不是标准的Json格式3.Google的GsonGson是目前功能最全的Json解析神器，Gson当初是为因应Google公司内部需求而由Google自行研发而来，但自从在2008年五月公开发布第一版后已被许多公司或用户应用。Gson的应用主要为toJson与fromJson两个转换函数，无依赖，不需要例外额外的jar，能够直接跑在JDK上。而在使用这种对象转换之前需先创建好对象的类型以及其成员才能成功的将JSON字符串成功转换成相对应的对象。类里面只要有get和set方法，Gson完全可以将复杂类型的json到bean或bean到json的转换，是JSON解析的神器。Gson在功能上面无可挑剔，但是性能上面比FastJson有所差距。4.阿里巴巴的FastJsonFastjson是一个Java语言编写的高性能的JSON处理器,由阿里巴巴公司开发。无依赖，不需要例外额外的jar，能够直接跑在JDK上。FastJson在复杂类型的Bean转换Json上会出现一些问题，可能会出现引用的类型，导致Json转换出错，需要制定引用。FastJson采用独创的算法，将parse的速度提升到极致，超过所有json库。 综上4种Json技术的比较，在项目选型的时候可以使用Google的Gson和阿里巴巴的FastJson两种并行使用，如果只是功能要求，没有性能要求，可以使用google的Gson，如果有性能上面的要求可以使用Gson将bean转换json确保数据的正确，使用FastJson将Json转换Bean二、Google的Gson包的使用简介。Gson类：解析json的最基础的工具类JsonParser类：解析器来解析JSON到JsonElements的解析树JsonElement类：一个类代表的JSON元素JsonObject类：JSON对象类型JsonArray类：JsonObject数组TypeToken类：用于创建type，比如泛型List&lt;?&gt;(1)maven依赖 com.google.code.gsongson2.2.4 (2)基础转换类 public class Book{ private String id; private String name; public Book() { super(); } public String getId() { return id; } public void setId(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } } public class Student{ private String name; private int age; private String sex; private String describe; private Set books; public Student() { super(); } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public String getSex() { return sex; } public void setSex(String sex) { this.sex = sex; } public Set getBooks() { return books; } public void setBooks(Set books) { this.books = books; } public String getDescribe() { return describe; } public void setDescribe(String describe) { this.describe = describe; } }(3)bean转换json Gson gson = new Gson(); String json = gson.toJson(obj);obj是对象(4)json转换bean Gson gson = new Gson(); String json = “{\”id\”:\”2\”,\”name\”:\”Json技术\”}”; Book book = gson.fromJson(json, Book.class);(5)json转换复杂的bean，比如List，Set将json转换成复杂类型的bean,需要使用TypeToken Gson gson = new Gson(); String json = “[{\”id\”:\”1\”,\”name\”:\”Json技术\”},{\”id\”:\”2\”,\”name\”:\”java技术\”}]”;//将json转换成List List list = gson.fromJson(json,new TypeToken() {}.getType());//将json转换成Set Set set = gson.fromJson(json,new TypeToken() {}.getType());(6)通过json对象直接操作json以及一些json的工具a)格式化Json String json = “[{\”id\”:\”1\”,\”name\”:\”Json技术\”},{\”id\”:\”2\”,\”name\”:\”java技术\”}]”; Gson gson = new GsonBuilder().setPrettyPrinting().create(); JsonParser jp = new JsonParser(); JsonElement je = jp.parse(json); json = gson.toJson(je);b)判断字符串是否是json,通过捕捉的异常来判断是否是json String json = “[{\”id\”:\”1\”,\”name\”:\”Json技术\”},{\”id\”:\”2\”,\”name\”:\”java技术\”}]”; boolean jsonFlag; try { new JsonParser().parse(str).getAsJsonObject(); jsonFlag = true; } catch (Exception e) { jsonFlag = false; }c)从json串中获取属性 String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; String propertyName = ‘id’; String propertyValue = “”; try { JsonParser jsonParser = new JsonParser(); JsonElement element = jsonParser.parse(json); JsonObject jsonObj = element.getAsJsonObject(); propertyValue = jsonObj.get(propertyName).toString(); } catch (Exception e) { propertyValue = null; }d)除去json中的某个属性 String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; String propertyName = ‘id’; JsonParser jsonParser = new JsonParser(); JsonElement element = jsonParser.parse(json); JsonObject jsonObj = element.getAsJsonObject(); jsonObj.remove(propertyName); json = jsonObj.toString();e)向json中添加属性 String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; String propertyName = ‘desc’; Object propertyValue = “json各种技术的调研”; JsonParser jsonParser = new JsonParser(); JsonElement element = jsonParser.parse(json); JsonObject jsonObj = element.getAsJsonObject(); jsonObj.addProperty(propertyName, new Gson().toJson(propertyValue)); json = jsonObj.toString();f)修改json中的属性 String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; String propertyName = ‘name’; Object propertyValue = “json各种技术的调研”; JsonParser jsonParser = new JsonParser(); JsonElement element = jsonParser.parse(json); JsonObject jsonObj = element.getAsJsonObject(); jsonObj.remove(propertyName); jsonObj.addProperty(propertyName, new Gson().toJson(propertyValue)); json = jsonObj.toString();g)判断json中是否有属性 String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; String propertyName = ‘name’; boolean isContains = false ; JsonParser jsonParser = new JsonParser(); JsonElement element = jsonParser.parse(json); JsonObject jsonObj = element.getAsJsonObject(); isContains = jsonObj.has(propertyName);h)json中日期格式的处理 GsonBuilder builder = new GsonBuilder(); builder.setDateFormat(“yyyy-MM-dd HH:mm:ss.SSS”); Gson gson = builder.create();然后使用gson对象进行json的处理，如果出现日期Date类的对象，就会按照设置的格式进行处理i)json中对于Html的转义 Gson gson = new Gson();这种对象默认对Html进行转义，如果不想转义使用下面的方法 GsonBuilder builder = new GsonBuilder(); builder.disableHtmlEscaping(); Gson gson = builder.create();三、阿里巴巴的FastJson包的使用简介。(1)maven依赖 com.alibabafastjson1.1.22 (2)基础转换类同上(3)bean转换json将对象转换成格式化的json JSON.toJSONString(obj,true);将对象转换成非格式化的json JSON.toJSONString(obj,false);obj设计对象对于复杂类型的转换,对于重复的引用在转成json串后在json串中出现引用的字符,比如 $ref”:”$[0].books[1] Student stu = new Student(); Set books= new HashSet(); Book book = new Book(); books.add(book); stu.setBooks(books); List list = new ArrayList(); for(int i=0;i&lt;5;i++) list.add(stu); String json = JSON.toJSONString(list,true);(4)json转换bean String json = “{\”id\”:\”2\”,\”name\”:\”Json技术\”}”; Book book = JSON.parseObject(json, Book.class);(5)json转换复杂的bean，比如List，Map String json = “[{\”id\”:\”1\”,\”name\”:\”Json技术\”},{\”id\”:\”2\”,\”name\”:\”java技术\”}]”;//将json转换成List List list = JSON.parseObject(json,new TypeReference(){});//将json转换成Set Set set = JSON.parseObject(json,new TypeReference(){});(6)通过json对象直接操作jsona)从json串中获取属性 String propertyName = ‘id’; String propertyValue = “”; String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject obj = JSON.parseObject(json); propertyValue = obj.get(propertyName));b)除去json中的某个属性 String propertyName = ‘id’; String propertyValue = “”; String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject obj = JSON.parseObject(json); Set set = obj.keySet(); propertyValue = set.remove(propertyName); json = obj.toString();c)向json中添加属性 String propertyName = ‘desc’; Object propertyValue = “json的玩意儿”; String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject obj = JSON.parseObject(json); obj.put(propertyName, JSON.toJSONString(propertyValue)); json = obj.toString();d)修改json中的属性 String propertyName = ‘name’; Object propertyValue = “json的玩意儿”; String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject obj = JSON.parseObject(json); Set set = obj.keySet(); if(set.contains(propertyName)) obj.put(propertyName, JSON.toJSONString(propertyValue)); json = obj.toString();e)判断json中是否有属性 String propertyName = ‘name’; boolean isContain = false; String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject obj = JSON.parseObject(json); Set set = obj.keySet(); isContain = set.contains(propertyName);f)json中日期格式的处理 Object obj = new Date(); String json = JSON.toJSONStringWithDateFormat(obj, “yyyy-MM-dd HH:mm:ss.SSS”);使用JSON.toJSONStringWithDateFormat,该方法可以使用设置的日期格式对日期进行转换 三、json-lib包的使用简介。(1)maven依赖 net.sf.json-libjson-libjdk152.2.2 commons-beanutilscommons-beanutils1.8.3 commons-collectionscommons-collections3.2 commons-langcommons-lang2.6 commons-loggingcommons-logging1.1.1 net.sf.ezmorphezmorph1.0.6 (2)基础转换类同上(3)bean转换jsona)将类转换成Json,obj是普通的对象，不是List，Map的对象 String json = JSONObject.fromObject(obj).toString();b)将List，Map转换成Json String json = JSONArray.fromObject(list).toString(); String json = JSONArray.fromObject(map).toString();(4)json转换bean String json = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject jsonObj = JSONObject.fromObject(json); Book book = (Book)JSONObject.toBean(jsonObj,Book.class);(5)json转换List,对于复杂类型的转换会出现问题 String json = “[{\”id\”:\”1\”,\”name\”:\”Json技术\”},{\”id\”:\”2\”,\”name\”:\”Java技术\”}]”; JSONArray jsonArray = JSONArray.fromObject(json); JSONObject jsonObject; T bean; int size = jsonArray.size(); List list = new ArrayList(size); for (int i = 0; i &lt; size; i++) { jsonObject = jsonArray.getJSONObject(i); bean = (T) JSONObject.toBean(jsonObject, beanClass); list.add(bean); }(6)json转换Map String jsonString = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; JSONObject jsonObject = JSONObject.fromObject(jsonString); Iterator keyIter = jsonObject.keys(); String key; Object value; Map valueMap = new HashMap(); while (keyIter.hasNext()) { key = (String) keyIter.next(); value = jsonObject.get(key).toString(); valueMap.put(key, value); }(7)json对于日期的操作比较复杂，需要使用JsonConfig,比Gson和FastJson要麻烦多了创建转换的接口实现类，转换成指定格式的日期 class DateJsonValueProcessor implements JsonValueProcessor{ public static final String DEFAULT_DATE_PATTERN = “yyyy-MM-dd HH:mm:ss.SSS”; private DateFormat dateFormat; public DateJsonValueProcessor(String datePattern) { try { dateFormat = new SimpleDateFormat(datePattern); } catch (Exception ex) { dateFormat = new SimpleDateFormat(DEFAULT_DATE_PATTERN); } } public Object processArrayValue(Object value, JsonConfig jsonConfig) { return process(value); } public Object processObjectValue(String key, Object value, JsonConfig jsonConfig) { return process(value); } private Object process(Object value) { return dateFormat.format[1]; Map&lt;STRING,DATE&gt; birthDays = new HashMap&lt;STRING,DATE&gt;(); birthDays.put(“WolfKing”,new Date()); JSONObject jsonObject = JSONObject.fromObject(birthDays, jsonConfig); String json = jsonObject.toString(); System.out.println(json); } }(8)JsonObject 对于json的操作和处理a)从json串中获取属性 String jsonString = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; Object key = “name”; Object value = null; JSONObject jsonObject = JSONObject.fromObject(jsonString); value = jsonObject.get(key); jsonString = jsonObject.toString();b)除去json中的某个属性 String jsonString = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; Object key = “name”; Object value = null; JSONObject jsonObject = JSONObject.fromObject(jsonString); value = jsonObject.remove(key); jsonString = jsonObject.toString();c)向json中添加和修改属性，有则修改，无则添加 String jsonString = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; Object key = “desc”; Object value = “json的好东西”; JSONObject jsonObject = JSONObject.fromObject(jsonString); jsonObject.put(key,value); jsonString = jsonObject.toString();d)判断json中是否有属性 String jsonString = “{\”id\”:\”1\”,\”name\”:\”Json技术\”}”; boolean containFlag = false; Object key = “desc”; JSONObject jsonObject = JSONObject.fromObject(jsonString); containFlag = jsonObject.containsKey(key);]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>大数据</tag>
        <tag>java类库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HEXO技巧分享--markdown语法归纳]]></title>
    <url>%2F2018%2F03%2F07%2Fhexo-tech3%2F</url>
    <content type="text"><![CDATA[什么是 Markdown 感谢大佬写得非常好的新手指南 引用自简书的https://www.jianshu.com/p/q81RER Markdown 是一种「电子邮件」风格的「标记语言」什么是 Markdown？简单地说，它就是一种语法，一门适合用于写作的简单语言。用过 Markdown 的人都知道，它的高效在于能有效避免不规范的行高、行距、首行缩紧等格式要求，以及繁乱复杂的字体，这样用户就能专注于写作本身，抛开杂项带来的烦恼。如果你从来没用过 Markdown，那我可以非常明确地告诉你——学习入门级的 Markdown 用法只需要 10 分钟，就足够了！在此，我们总结 Markdown 的优点如下： 纯文本，所以兼容性极强，可以用所有文本编辑器打开。 让你专注于文字而不是排版。 格式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。 Markdown 的标记语法有极好的可读性。 要开始 Markdown 的写作，首先要找到一款适合自己的 Markdown 编辑器工具。下面三篇文章详细介绍了各个平台（Win，Mac，iOS，Android）上优秀的 Markdown 编辑器，可以根据自己的喜好和经济能力选择一个或几个： 在这篇“解决作者们的焦虑：7 款优秀 Markdown 编辑工具推荐”中总结一些使用率高的md编辑工具 传送门 https://sspai.com/post/27792 那么 Markdown 是通过什么方法去避免发生这些问题的呢？答案其实也并没有那么复杂，通俗地说就俩字：转码。 是的，不要单纯地以为只有多媒体文件才能用「转码」形容，在计算机的世界里，任何东西都不是我们肉眼凡胎，直接看到的那个样子。比如说文字，从像素到一个完整的字体库，期间经历的是数不尽的设计、再设计，再加上软件工程及硬件工程的完美协作，最终才能获得一个在计算机中最常见的元素：文字。 而在文字创作领域，Markdown 的作用就是把一篇纯文本文章转换为 富文本，让读者得到清晰明了的阅读体验。当然，由于各平台对于「格式」的兼容性不同，有时也会发生一些不那么「友善」的事情。比如用 OS X 上的 Chrome 浏览器阅读「两端对齐」格式的文章时，为了符合要求，在遇到中英文交替出现的段落，很有可能导致「大空格」现象的发生，继而在一定程度上影响了读者的阅读体验，但 OS X 的 Safari 和 Windows 的 IE 就可以轻松避免。所以针对这种问题，你既可以理解为这是 Markdown 转码富文本的不够完善，也能看作是浏览器的兼容性问题。总之，在这个越来越多人选择自己写文章的年代，Markdown 语法一定是该群体独一无二的选择。 有了想法，接下来要做的就是付诸实践，而这其中，最让人头疼的问题就是 Markdown 编辑器的选择，因为有的人喜欢在移动平台上创作文字，比如 iPhone、iPad 等，但更多的人会选择在（便携式）电脑上完成这项工作。说实话，我个人属于电脑撰稿用户，理由很简单： 第一，就目前而言，虚拟键盘在文字输入的体验上远不及实体键盘来得有效率。第二，支持的格式范围广，保存、导出、备份、发布的方法简单又便捷。第三，大多数人在使用电脑时，周围的场景会是家里、办公室、咖啡厅、图书馆，至少不会是非常吵闹的地方，这样有助于保持专注的思维；而习惯用手机码字的人，很多都是因为受到环境的限制，不得已出此下策，而非「情怀」作祟。或许现在的你有千万种理由可以反驳我，但本文的目的不在于此，只是要分享 7 款我使用过，及正在使用的 Markdown 编辑器，它们有不同的分类、定位、售价，也涵盖了 Mac、iOS、Windows 平台，它们都是让笔者留下深刻印象的选择。所以，为了尽可能避免「独立观点」的干扰，希望大家以技术探讨为优先，相互推荐更多、更有价值的选择。 Ulysses一款由国外开发商 The Soulmen 制作的 Markdown 编辑器。与其它同类应用相比，Ulysses 最大的不同在于，它能根据内置的文件管理器，以及与 iCloud 云服务器的实时同步方案，达到最快捷的文章整理效率。这么说可能不够细致，那我们不妨简单试想一下：当你正在写一篇文章，突然由于外在原因（比如出门），干扰了原本的写作计划，那么很有可能也就同时打断了思路，继而最终影响到文章的完成。对于作者而言，这丝毫不亚于「冬天洗澡没热水」的混蛋事，但你又能怎么办呢？ 所以，云同步的好处在此时就能体现出来了！记得 乔布斯 在刚回归苹果的一次开发者大会演讲上，着重强调了「云」在未来计算机发展领域的重要性。在综合现代环境的因素，我得出了以下结论： 如果有一天，我们电脑里的数据可以随时保存在云端服务器，并做到随用随取，那么就可以解决当下科技领域几大严重的先天缺陷，第一，设备丢失导致的数据损失；第二，使用内置硬盘带来无法避免的厚度和重量，导致便携式电脑的「便携性」降低；第三，不同的数据转换方式，导致文件结构的分裂，继而影响到操作系统的稳定性和数据相关行为的有效性。 话虽如此，但要具体的实现，还需要等到全球网络环境的大进步，软硬件水准提升到相当高的阶段才行。不过，在当下数不尽的解决方案面前，我认为 Apple 作了一个不算最好，但合理可行的选择：iCloud。iCloud 是个涉及领域颇广的话题，这里不适合分类讨论，读者请自行联想，但针对 Markdown 编辑器，我认为及时的云同步确实可以部分实现上文提到的目标，即数据找回、随用随取、多平台覆盖等。这就是我推荐 Ulysses 的理由。 Byword一款轻量级的 Markdown 编辑器，人称 Markdown 写作新手的必选，不过我个人不是非常同意这一观点。回想较早面世的几款 Markdown 工具，大多都会加入「实时预览」特性，（笔者推测）这是因为当时的 Markdown 语法还没有形成一个合理规范的体系，这些应用的开发者为了保证用户体验，不得已而加入了这一特性，可在此之后，大多数 Markdown 编辑器则又都反其道而行之（例如本文的介绍对象 Ulysses，Byword 和 Typed），这不禁要引起用户的怀疑：难道「实时预览」不是 Markdown 编辑器应有的功能？当然不是，因为这类应用的目的就是要帮助作者生产出高质量的文章，也就是说，只要你会写，并且能通过 Markdown 来完成自己的工作，这就足够了，与选择 Markdown 编辑器无关，与用户能力的强弱无关！所以我的观点非常简单：能达到目的的应用就是好应用。 开发商除了制作 Mac 平台的版本，同样也带来了 iOS 客户端。很多人建议已经在使用 Byword for Mac 的人应该首选 Byword for iOS 而不是其它（可能更好的）编辑器，是因为它能经由 OS X 10.10 Handoff 特性实现文档编辑环境的无缝切换，这种畅快淋漓的感觉是无法在不同的应用间体验到的。 Mou一款由国人独立开发者 罗晨 开发的实时预览型 Markdown 编辑器，也因此成为（目前）同类应用中，对汉字兼容性最好的代表作。不信？你一用便知。反过来讲，要论它有多人性化，我有点说不上来，毕竟每个人都有自己的偏好、习惯和审视角度，任何文章作者都不能以偏概全，但有一点是为大众认可的——Mou 是目前最好用的免费 Markdown 编辑器，没有之一！顺便也恭喜下不久前 Mou 1.0 版本开发资金的成功众筹，也衷心希望国内能有更多靠个人实力登上国际舞台的独立开发者。罗晨，好样的！ Typed一款由国外软件开发商 Realmac 制作的 Markdown 编辑器，于 2014 年 12 月份刚刚发布。对于这款应用，笔者的评价是：噱头大于亮点。这么说可能有些刻薄，但我却丝毫没有自己有一张「毒舌」的感觉，因为事实即是如此。在看了其特性列表后，大多数人都认为 Typed 的最大亮点就在于 Zen Mode（姑且解释为「禅模式」），号称能让用户更加专注于写作，而实际体验下来我发现，这一功能倒不如形容为「背景音乐播放」来得直接，因为它只是用来播放一些从自然界采集到的声音（例如鸟鸣、水流、风刮、雨打），但我却丝毫没有从中体会到写作应有的环境氛围。个人认为，码字就是要安安静静，才能更全面地发挥逻辑思维和想象空间，以进一步充实文章的内涵，保证内容的连贯有序，提高可读性。安静，是一个要求相当高的环境，不是所谓的「自然之声」就能做到，或者说弥补得了的。如果您同意笔者的愚见，那么从现在起，我建议读者不要（再）相信任何带有类似「禅模式」功能的写作工具，都是忽悠人的！否则，烦请三思而后行呀。 Sublime Text 3一款基于 Vim 开发的跨平台代码编辑器，支持 OS X、Windows、Ubuntu 等 UNIX 及 Linux 操作系统，并由于其功能的多样性而广受好评，在代码工作者圈内相当出名！关于 Vim，《MacTalk 人生元编程》一书的作者 @池建强 是这么评价的： 操作系统、编程语言和编辑器是程序员永恒的吐槽话题，技术发展了几十年，争论起来依然是「此恨绵绵无绝期」。在本文编辑器领域，Vim 和 Emacs 是永恒的焦点。Vim 号称编辑器之神，Emacs 则是神的编辑器；Vim 编程唯快不破，插件遍天下，Emacs 则宣称自己是伪装成文本编辑器的操作系统。 既然是「神的编辑器」，那么基于它开发的 Sublime Text 又会有怎样的表现呢？仅目前来看，它原生支持的编程语言就能多达十几种，其中包括大家熟知的 C、C++、C#、Objective-C、AppleScript、HTML、Java、Python 等。通过第三方插件，Sublime Text 还能实现更多语法的支持，这之中的代表就是 Markdown。其实，Sublime Text 在老版本中就已经支持 Markdown 了，但苦于没有像样的「预览」功能，其用户大多只是通过一种叫 Markdown Preview 的插件实现对 Markdown 的预览，而且还必须要有浏览器的支持。所以，笔者不推荐大家首选 Sublime Text 作为 Markdown 编辑器，但如果读者已经是一名代码工作者，并且正在使用它，那么考虑到这部分群体的使用习惯和口味，将就着用也是可以的。 Editorial一款 iOS 平台上支持 Workflow 的纯文本编辑器。论性质，倒确实与 Sublime Text 有那么几分相似，因为它也支持脚本代码的编译，比如说 Python。但这并不属于本文的介绍对象。更让我印象深刻的是它对 Markdown 语法的支持，主题体现在这三方面：界面、键盘和预览模式。 Editorial 的界面非常简洁，完全是按照 iOS 7 的扁平化风格设计的，可不少人在 App Store 中第一眼看到它的时候，都立刻失去了兴趣，理由竟然是：太单调了！好吧，这让我感到无言以对… 作为一款本文编辑器，尤其是 Markdown 编辑器，应该且必须以文字为核心，其它元素皆为陪衬或补偿，而不是代替前者的价值和意义，所以笔者觉得 Editorial 在用户界面的设计上还是很符合主流需求的。键盘方面，基本的语法符号很全，再配合文字输入辅助应用 TextExpander touch 可以让码字变得更有效率，此外，Editorial 还内置了 Snippets 功能。这是一个相当实用的功能，它可以直接代替 TextExpander touch，实现基于基本语法的输入，举两个简单的例子：[Clipboard] 能提取剪贴板内容，[yyy]-[MM]-[dd] 能提取实时日期等。这方面的例子有很多，这里不做扩展，感兴趣的读者可以自行研究一下。关于「预览」模式，一句话就可以概括：请往左划！ 简书不同于上文介绍的对象，简书 是一家由国内初创团队建立的在线文字创作及发布平台，而非客户端，所以相对来说，我更加建议 Windows 用户可以着重考虑一下。仔细想想，其实笔者个人接触它的时间不多，但很快就发现自己已经上瘾了，虽然这其中的因素有很多，可主要还是因为它有两个方面做得非常到位：后台、专题。回顾发展史，简书从一开始就已经支持 Markdown 和富文本编辑，对于像我这样的 Markdown 控而言是绝对的好事！另外，每位作者都可以通过连续撰稿和收集建立自己的专题、文集，甚至是一本看得见也摸得着的纸质作品。所以综合评定下来，它注定就是一个为作者打造的平台。 摸着良心说，推荐「简书」完全是因为它作为初创团队的成果，能做到如此精良的水准实属不易！就像我喜欢 Mou 一样，只要东西好，无论它来自国人之手，还是「进口」海外，每个人都有选择它的理由。因此我还是那句话，体验过后你就知道了。 关于MarkDown语法点击这里可以看到 https://www.jianshu.com/p/b03a8d7b1719https://www.jianshu.com/p/q81RER]]></content>
      <categories>
        <category>Hexo教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链技术解读--区块链结构解释]]></title>
    <url>%2F2018%2F03%2F06%2F%E5%8C%BA%E5%9D%97%E9%93%BE03%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>区块链技术</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>数字货币</tag>
        <tag>互联网金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链技术解读--数字签名]]></title>
    <url>%2F2018%2F03%2F06%2F%E5%8C%BA%E5%9D%97%E9%93%BE02%2F</url>
    <content type="text"><![CDATA[#数字签名# 数字签名即哈希算法+非对称加密相结合的技术 百度概念：数字签名（又称公钥数字签名、电子签章）是一种类似写在纸上的普通的物理签名，但是使用了公钥加密领域的技术实现，用于鉴别数字信息的方法。一套数字签名通常定义两种互补的运算，一个用于签名，另一个用于验证。数字签名，就是只有信息的发送者才能产生的别人无法伪造的一段数字串，这段数字串同时也是对信息的发送者发送信息真实性的一个有效证明。数字签名是非对称密钥加密技术与数字摘要技术的应用。 原理数字签名的文件的完整性是很容易验证的（不需要骑缝章，骑缝签名，也不需要笔迹专家），而且数字签名具有不可抵赖性（不需要笔迹专家来验证）。简单地说,所谓数字签名就是附加在数据单元上的一些数据,或是对数据单元所作的密码变换。这种数据或变换允许数据单元的接收者用以确认数据单元的来源和数据单元的完整性并保护数据,防止被人(例如接收者)进行伪造。它是对电子形式的消息进行签名的一种方法,一个签名消息能在一个通信网络中传输。基于公钥密码体制和私钥密码体制都可以获得数字签名,主要是基于公钥密码体制的数字签名。包括普通数字签名和特殊数字签名。普通数字签名算法有RSA、ElGamal、Fiat-Shamir、Guillou- Quisquarter、Schnorr、Ong-Schnorr-Shamir数字签名算法、Des/DSA,椭圆曲线数字签名算法和有限自动机数字签名算法等。特殊数字签名有盲签名、代理签名、群签名、不可否认签名、公平盲签名、门限签名、具有消息恢复功能的签名等,它与具体应用环境密切相关。显然,数字签名的应用涉及到法律问题,美国联邦政府基于有限域上的离散对数问题制定了自己的数字签名标准(DSS)。 主要功能保证信息传输的完整性、发送者的身份认证、防止交易中的抵赖发生。数字签名技术是将摘要信息用发送者的私钥加密，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。数字签名是个加密的过程，数字签名验证是个解密的过程。 数字签名即哈希算法+非对称加密相结合的技术提到非对称算法就会想到对称算法 对称算法对称密码算法有时又叫传统密码算法，就是加密密钥能够从解密密钥中推算出来，反过来也成立。在大多数对称算法中，加密解密密钥是相同的。这些算法也叫秘密密钥算法或单密钥算法，它要求发送者和接收者在安全通信之前，商定一个密钥。对称算法的安全性依赖于密钥，泄漏密钥就意味着任何人都能对消息进行加密解密。只要通信需要保密，密钥就必须保密。 即消息的发送方和接收方都拥有共同的密钥，可以将传递的明文进行解密 不对称算法不对称加密算法使用两把完全不同但又是完全匹配的一对钥匙—公钥和私钥。在使用不对称加密算法加密文件时，只有使用匹配的一对公钥和私钥，才能完成对明文的加密和解密过程。 加密明文时采用公钥加密，解密密文时使用私钥才能完成，而且发信方（加密者）知道收信方的公钥，只有收信方（解密者）才是唯一知道自己私钥的人。不对称加密算法的基本原理是，如果发信方想发送只有收信方才能解读的加密信息，发信方必须首先知道收信方的公钥，然后利用收信方的公钥来加密原文；收信方收到加密密文后，使用自己的私钥才能解密密文。显然，采用不对称加密算法，收发信双方在通信之前，收信方必须将自己早已随机生成的公钥送给发信方，而自己保留私钥。由于不对称算法拥有两个密钥，因而特别适用于分布式系统中的数据加密。广泛应用的不对称加密算法有RSA算法和美国国家标准局提出的DSA。 与对称加密算法的不同与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publickey）和私 有 密钥（privatekey）。公开密钥与私有密钥是一对，如果用公开密钥对数据进行加 密，只有用对应的私有密钥才能解密；因为加密和解密使用的是两个不同的密钥，所以这种算法叫作非对称加密算法。 非对称加密算法实现机密信息交换的基本过程是：甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开；得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方；甲方再用自己保存的另一把专用密钥对加密后的信息进行解密。甲方只能用其专用密钥解密由其公用密钥加密后的任何信息。 非对称加密算法的保密性比较好，它消除了最终用户交换密钥的需要，但加密和解密花费时间长、速度慢，它不适合于对文件加密而只适用于对少量数据进行加密。 经典的非对称加密算法如RSA算法等安全性都相当高。对称加密对称加密算法是应用较早的加密算法，技术成熟。在对称加密算法中，数据发信方将明文（原始数据）和加密密钥一起经过特殊加密算法处理后，使其变成复杂的加密密文发送出去。收信方收到密文后，若想解读原文，则需要使用加密用过的密钥及相同算法的逆算法对密文进行解密，才能使其恢复成可读明文。在对称加密算法中，使用的密钥只有一个，发收信双方都使用这个密钥对数据进行加密和解密，这就要求解密方事先必须知道加密密钥。对称加密算法的特点是算法公开、计算量小、加密速度快、加密效率高。不足之处是，交易双方都使用同样钥匙，安全性得不到保证。此外，每对用户每次使用对称加密算法时，都需要使用其他人不知道的惟一钥匙，这会使得发收信双方所拥有的钥匙数量成几何级数增长。对称加密算法在分布式网络系统上使用较为困难，主要是因为密钥管理困难，使用成本较高。在计算机专网系统中广泛使用的对称加密算法有DES、IDEA和AES。 DES传统的DES由于只有56位的密钥，因此已经不适应当今分布式开放网络对数据加密安全性的要求。1997年RSA数据安全公司发起了一项“DES挑战赛”的活动，志愿者四次分别用四个月、41天、56个小时和22个小时破解了其用56位密钥DES算法加密的密文。在计算机速度提升后的今天，DES加密算法被认为是不安全的。 AESAES是美国联邦政府采用的商业及政府数据加密标准，预计将在未来几十年里代替DES在各个领域中得到广泛应用。AES提供128位密钥，因此，128位AES的加密强度是56位DES加密强度的1021倍还多。假设可以制造一部可以在1秒内破解DES密码的机器，那么使用这台机器破解一个128位AES密码需要大约149亿万年的时间。（更深一步比较而言，宇宙一般被认为存在了还不到200亿年）因此可以预计，美国国家标准局倡导的AES即将作为新标准取代DES。]]></content>
      <categories>
        <category>区块链技术</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>数字货币</tag>
        <tag>互联网金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链技术解读--哈希算法]]></title>
    <url>%2F2018%2F03%2F06%2F%E5%8C%BA%E5%9D%97%E9%93%BE01%2F</url>
    <content type="text"><![CDATA[哈希算法 Hash算法是密码学基础，较常用的是MD5系和SHA系的散列算法结构最重要的两条特性为不可逆和无冲突但是这两条特性在数学上是不成立的不可逆即不可能反向推出哈希码所对应的明文内容，但是既然明文对应密文，非动态，那么就一直可以推出明文，在算法上利用穷举法或者彩虹表可以反解出明文内容因为一个函数必然可逆，且由于HASH函数的值域有限，理论上会有无穷多个不同的原始值，它们的hash值都相同，就是一个密文对应无限明文。无冲突不算是真的无冲突，密文对应明文，但是密文根据算法的不同，有限制规定的长度，而明文是无限的，所以是有可能发生哈希碰撞的 MD5和SHA做到的，是求逆和求冲突在计算上不可能，也就是正向计算很容易，而反向计算即使穷尽人类所有的计算资源都做不到。 哈希加密算法 MD5,SHA-1,SHA-2,SHA-256,SHA-512,SHA-3,RIPEMD-160等引用自 http://www.atool.org/hash.php MD5算法MD5即Message-Digest Algorithm 5（信息-摘要算法 5），用于确保信息传输完整一致。是计算机广泛使用的散列算法之一（又译摘要算法、哈希算法），主流编程语言普遍已有MD5实现。 将数据（如汉字）运算为另一固定长度值，是散列算法的基础原理，MD5的前身有MD2、MD3和MD4。MD5一度被广泛应用于安全领域。但是由于MD5的弱点被不断发现以及计算机能力不断的提升，现在已经可以构造两个具有相同MD5的信息[2]，使本算法不再适合当前的安全环境。目前，MD5计算广泛应用于错误检查。例如在一些BitTorrent下载中，软件通过计算MD5和检验下载到的碎片的完整性。MD5是输入不定长度信息，输出固定长度128-bits的算法。经过程序流程，生成四个32位数据，最后联合起来成为一个128-bits散列。基本方式为，求余、取余、调整长度、与链接变量进行循环运算。得出结果。 SHA-1哈希加密算法SHA-1在许多安全协议中广为使用，包括TLS和SSL、PGP、SSH、S/MIME和IPsec，曾被视为是MD5（更早之前被广为使用的散列函数）的后继者。但SHA-1的安全性如今被密码学家严重质疑。 SHA-2哈希加密算法SHA-224、SHA-256、SHA-384，和SHA-512并称为SHA-2。新的散列函数并没有接受像SHA-1一样的公众密码社区做详细的检验，所以它们的密码安全性还不被大家广泛的信任。虽然至今尚未出现对SHA-2有效的攻击，它的算法跟SHA-1基本上仍然相似；因此有些人开始发展其他替代的散列算法。 SHA-3哈希加密算法SHA-3，之前名为Keccak算法，是一个加密杂凑算法。SHA-3并不是要取代SHA-2，因为SHA-2目前并没有出现明显的弱点。由于对MD5出现成功的破解，以及对SHA-0和SHA-1出现理论上破解的方法，NIST感觉需要一个与之前算法不同的，可替换的加密杂凑算法，也就是现在的SHA-3。 RIPEMD-160哈希加密算法RIPEMD-160 是一个 160 位加密哈希函数。它旨在用于替代 128 位哈希函数 MD4、MD5 和 RIPEMD。RIPEMD 是在 EU 项目 RIPE（RACE Integrity Primitives Evaluation，1988-1992）的框架中开发的。 引用自新浪微博 http://blog.sina.com.cn/s/blog_1799256a10102x3fh.html Hash算法将任意长度的二进制值映射为较短的固定长度的二进制值，这个小的二进制值称为哈希值。哈希值是一段数据唯一且极其紧凑的数值表示形式。如果散列一段明文而且哪怕只更改该段落的一个字母，随后的哈希都将产生不同的值。要找到散列为同一个值的两个不同的输入，在计算上是不可能的，所以数据的哈希值可以检验数据的完整性。一般用于快速查找和加密算法。 再引入一个hash表概念，计算机数据结构中，给定一个表M，关键字key，存在函数H(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为hash表。 简单理解hash算法就是这一种单向的加密，一个明文加密称为密文，不可逆推，只有加密过程，没有解密过程。说明了hash函数和hash表的概念，那么目前常用的hash算法有MD5（已被破解），SHA系列算法（比特币中使用sha-256算法）。SHA这里稍微提下（secure hash algorithm）这不是一个算法，这是一个hash函数集，现在有sha-224、sha-256、sha-384、sha-512等算法。在09年中本聪设计比特币的时候，当时sha-256被认为最安全的算法之一，故选择了sha-256，到目前为止还没有被破解。 解释到这里，可能会联想到，hash算法中key在计算后如果出现了同一位置，冲突的产生，这里简单说下几种冲突处理，如有兴趣可以查看hash算法论文。 1.拉链法：这种方法可以完全避免冲突，将所有关键字为同义词的结点链接在同一个单链表中。若选定的散列表长度为m，则可将散列表定义为一个由m个头指针组成的指针数组t[0..m-1]。凡是散列地址为i的结点，均插入到以t为头指针的单链表中。t中各分量的初值均应为空指针。在拉链法中，装填因子α可以大于1，但一般均取α≤1。 2.多哈希法：设计两种以上的hash函数，避免冲突，这个感觉比较不靠谱，但是从概率上来说多种hash函数还是降低了冲突的出现。 3.开放地址法：开放地址法有一个公式：Hi=(H(key)+di) MOD m i=1,2,…,k(k&lt;=m-1），其中，m为哈希表的表长。di 是产生冲突的时候的增量序列。如果di值可能为1,2,3,…m-1，称线性探测再散列。如果di取1，则每次冲突之后，向后移动1个位置.如果di取值可能为1,-1,4,-4,9,-9,16,-16,…kk,-kk(k&lt;=m/2），称二次探测再散列。如果di取值可能为伪随机数列。称伪随机探测再散列。 Hash算法函数根据分类：加法hash、位运算hash、乘法hash、除法hash、查表hash等。 区块链中的哈希解读因为区块链技术作为比特币的底层技术之一，目的是用于保证数字货币在交易环节中的安全性。 在徐明星的图说区块链中以一种图解更直观的形式这样说道 结合区块链，在区块链中很多地方都用到了hash函数： 1.区块链中节点的地址、公钥、私钥的计算。以地址为例：公钥经过一次SHA256计算，再进行一次RIPEMD160计算，得到一个公钥哈希（20字节\160比特），添加版本信息，再来两次SHA256运算、取前4比特字节，放到哈希公钥加版本信息后，再经过base58编码，最终得到地址。 2.merkle tree：是数据结构中的一种树结构，可以是二叉树，也可以是多叉树，他和数据结构中树的特点几乎一致，和普通树不同的是：merkle tree上的叶节点存放hash计算后的hash值，非叶节点是其对应的子节点串联的字符串的hash值。用于区块头和SPV认证中。 3.比特币中的挖矿，工作量证明（pow），计算的其实就是一个nonce，当这个随机数和其他散列过的数据合并时，产生一个比规定目标小（target）值。挖矿也可以理解一种快速不可逆的计算。SHA256(SHA256(version + prev_hash + merkle_root + ntime + nbits + x )) &lt; TARGET。 4.比特币中的bloom filter布隆过滤器，布隆过滤器基于hash函数的快速查找。解决了客户端检索的问题，原理是Bloom filter可以快速判断出某检索值一定不存在于某个指定的集合，从而可以过滤掉大量无关数据，减少客户端不必要的下载量。 简单介绍了HASH算法，和区块链中用到的HASH算法，区块链是多个技术的结合，结合各自特点出现的一种新的技术架构，HASH算法和加密技术为区块链的自证信任化及安全控制提供了基础，算法的碰撞和现在量子计算的发展，之前在区块链的安全性的文章中笔者有过说明，技术不断发展，肯定会有更适合的技术保障应用的实现。 SHA (Secure Hash Algorithm，译作安全散列算法) 是美国国家安全局 (NSA) 设计，美国国家标准与技术研究院 (NIST) 发布的一系列密码散列函数，经历了SHA-0，SHA-1，SHA-2，SHA-3系列发展。NSA于2007年正式宣布在全球范围内征集新新一代（SHA-3）算法设计，2012年公布评选结果， Keccak算法最终获胜成为唯一官方标准SHA-3算法，但还有四种算法同时进入了第三轮评选，分别是：BLAKE, GrøSTL, JH和SKEIN，这些算法其实也非常安全，而且经受审查，被各种竞争币频繁使用。 比特币采用SHA256算法，该算法属于SHA-2系列，在中本聪发明比特币时（2008）被公认为最安全最先进的算法之一。除了生成地址中有一个环节使用了REPID-160算法，比特币系统中但凡有需要做Hash运算的地方都是用SHA256。随着比特币被更多人了解，大家开始好奇中本聪为何选择了SHA256，同时对SHA256的安全性发表各种意见，SHA256妥妥经受了质疑，到目前为止，没有公开的证据表明SHA256有漏洞，SHA256依然牢牢抗住保卫比特币安全的大旗。当然大家心里都明白，没有永远安全的算法，SHA256被替代是早晚的事，中本聪自己也说明了算法升级的必要和过程。]]></content>
      <categories>
        <category>区块链技术</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>数字货币</tag>
        <tag>互联网金融</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HEXO技巧分享--不借助第三方图库上传图片]]></title>
    <url>%2F2018%2F03%2F06%2Fhexo-photo%2F</url>
    <content type="text"><![CDATA[图片测试，来自Hexo大佬的指导，不通过第三方图床上传图片 1.首先确认_config.yml 中有 post_asset_folder:true。Hexo 提供了一种更方便管理 Asset 的设定：post_asset_folder当您设置post_asset_folder为true参数后，在建立文件时，Hexo会自动建立一个与文章同名的文件夹，您可以把与该文章相关的所有资源都放到那个文件夹，如此一来，您便可以更方便的使用资源。 2.在hexo的目录下执行npm install https://github.com/CodeFalling/hexo-asset-image –save（需要等待一段时间）。 3.完成安装后用hexo新建文章的时候会发现_posts目录下面会多出一个和文章名字一样的文件夹。图片就可以放在文件夹下面。结构如下： 本地图片测试├── apppicker.jpg├── logo.jpg└── rules.jpg本地图片测试.md这样的目录结构（目录名和文章名一致），只要使用 ![logo](本地图片测试/logo.jpg)格式 就可以插入图片。其中[]里面不写文字则没有图片标题。生成的结构为 public/2016/3/9/本地图片测试├── apppicker.jpg├── index.html├── logo.jpg└── rules.jpg同时，生成的 html 是 &lt;img src=&quot;/2016/3/9/本地图片测试/logo.jpg&quot; alt=&quot;logo&quot;&gt; 而不是愚蠢的 &lt;img src=&quot;本地图片测试/logo.jpg&quot; alt=&quot;logo&quot;&gt; 注意:通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。在Hexo2时代，社区创建了很多插件来解决这个问题。但是，随着Hexo3的发布，许多新的标签插件被加入到了核心代码中。这使得你可以更简单地在文章中引用你的资源。 比如说：当你打开文章资源文件夹功能后，你把一个 example.jpg图片放在了你的资源文件夹中，如果通过使用相对路径的常规 markdown 语法 ，它将 不会 出现在首页上。（但是它会在文章中按你期待的方式工作） 正确的引用图片方式是使用下列的标签插件而不是markdown 作者：TSimeon链接：https://www.jianshu.com/p/c2ba9533088a來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>Hexo教程</category>
      </categories>
      <tags>
        <tag>HEXO技巧</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HEXO技巧分享--busuanzi技术]]></title>
    <url>%2F2018%2F03%2F05%2Ftest2%2F</url>
    <content type="text"><![CDATA[在新版的Next主题中，就内置了不蒜子作为统计工具我们只需要在主题config中配置即可 操作： 打开文件：themes/next/_config.yml busuanzi_count: count values only if the other configs are false enable: true custom uv span for the whole site site_uv: true site_uv_header: 访问人数 site_uv_footer: 人 custom pv span for the whole site site_pv: true site_pv_header: 总访问量 site_pv_footer: 次 custom pv span for one page only page_pv: true page_pv_header: 阅读数 page_pv_footer: 相关操作： enable: true 添加相关中文说明，不添加也行。 但若是我们想自己集成不蒜子具体实现方法 打开\themes\next\layout_partials\footer.swig文件,在copyright前加上这段代码 &lt;script async src=&quot;https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt; &lt;/script&gt; 然后再合适的位置添加显示统计的代码 themes\iissnan\layout_third-party\analytics\busuanzi-counter.swig中 在这里有两中不同计算方式的统计代码： pv的方式，单个用户连续点击n篇文章，记录n次访问量&lt;span id=&quot;busuanzi_container_site_pv&quot;&gt; 本站总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次 &lt;/span&gt; uv的方式，单个用户连续点击n篇文章，只记录1次访客数&lt;span id=&quot;busuanzi_container_site_uv&quot;&gt; 本站总访问量&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;次 &lt;/span&gt; 添加之后再执行hexo d -g，然后再刷新页面就能看到效果]]></content>
      <categories>
        <category>Hexo教程</category>
      </categories>
      <tags>
        <tag>HEXO技巧</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HEXO技巧分享--如何建立分类及标签]]></title>
    <url>%2F2018%2F03%2F05%2Fhexo-tech2%2F</url>
    <content type="text"><![CDATA[在hexo框架中，建立分类和标签的方法如下 添加标签hexo new page tags这个是自动创建tags文件夹的命令,里面会自动生成index.md文件，当然也可以手动创建tags文件夹和md文件，不过这样就会没有时间显示 确认站点配置文件里有tag_dir: tags确认主题配置文件里有tags: /tags编辑站点的source/tags/index.md，添加 title: tags #标签的标题，可以说是几个共同特性标签的集合 type: &quot;tags&quot; #这里是对应站点配置文件中的tags comments: false 调好后，我们就可以在文章中自由添加标签了比如 `—title: HEXO技巧分享–如何建立分类及标签date: 2018-03-05 16:03:27tags: - HEXO技巧 - hexo —`HEXO技巧和hexo这两个标签会在tags页面自动生成并导航 添加分类hexo new page categories确认站点配置文件里有category_dir: categories确认主题配置文件里有categories: /categories编辑站点的source/categories/index.md，添加 title: categories type: &quot;categories&quot; comments: false 调好后，我们就可以在文章中自由添加分类了比如 `—title: HEXO技巧分享–如何建立分类及标签date: 2018-03-05 16:03:27categories: “Hexo教程” —` 添加关于hexo new page “about”在about文件夹下的index.md文件中写上文字后更新就可以在页面中看到了]]></content>
      <categories>
        <category>Hexo教程</category>
      </categories>
      <tags>
        <tag>HEXO技巧</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HEXO技巧分享--德文错误显示修改]]></title>
    <url>%2F2018%2F03%2F05%2Fhexo-tech%2F</url>
    <content type="text"><![CDATA[在所应用的主题的languages文件夹中将 zh—Hans.yml 改成 zh—CN.yml然后在HEXO主目录下的_config.yml配置文件中修改 language： zh—CN 即可 更新操作 hexo clean hexo g hexo s（预览检查） hexo deploy #部署]]></content>
      <categories>
        <category>Hexo教程</category>
      </categories>
      <tags>
        <tag>HEXO技巧</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo驱动+Github搭载Blog系统]]></title>
    <url>%2F2018%2F03%2F04%2Fhexo_init1%2F</url>
    <content type="text"><![CDATA[分享HEXO+GITHUB搭载驱动的博文系统使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快；免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；博客内容可以轻松打包、转移、发布到其它平台； 什么是Hexo?Hexo官网中说是这么描述的：A fast, simple &amp; powerful blog framework,即:一个快速、简单且强大的博客快速生产工具。它的简单体现在你完全有可能在30分钟内就生成属于你的个人博客。而它的强大体现在你对细节的调整上完全有可能花上一天的时间。Hexo(中文官方网站)是一个快速, 简洁且高效的博客框架. 让上百个页面在几秒内瞬间完成渲染. Hexo支持Github Flavored Markdown的所有功能, 甚至可以整合Octopress的大多数插件. 并自己也拥有强大的插件系统. Blazing FastNode.js brings you incredible generating speed. Hundreds of files take only seconds to build. Markdown SupportAll features of GitHub Flavored Markdown are supported. You can even use most Octopress plugins in Hexo. One-Command DeploymentYou only need one command to deploy your site to GitHub Pages, Heroku or other sites. Various PluginsHexo has a powerful plugin system. You can install more plugins for Jade, CoffeeScript plugins. 1.安装node.jsNode.js 的实质是一个JavaScript运行环境,这里我们主要使用它来生成我们博客的静态页面.Node.js® is a JavaScript runtime built on Chrome’s V8 JavaScript engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient. Node.js’ package ecosystem, npm, is the largest ecosystem of open source libraries in the world.Window 上安装Node.js你可以采用以下两种方式来安装。 Windows 安装包(.msi)形式32 位安装包下载地址 : https://nodejs.org/dist/v4.4.3/node-v4.4.3-x86.msi64 位安装包下载地址 : https://nodejs.org/dist/v4.4.3/node-v4.4.3-x64.msi步骤 1 : 双击下载后的安装包步骤 2 : 点击以上的Run(运行)步骤 3 : 勾选接受协议选项，点击 next（下一步） 按钮步骤 4 : Node.js默认安装目录为 “C:\Program Files\nodejs\” , 你可以修改目录，并点击 next（下一步）步骤 5 : 点击树形图标来选择你需要的安装模式 , 然后点击下一步 next（下一步）步骤 6 :点击 Install（安装） 开始安装Node.js。你也可以点击 Back（返回）来修改先前的配置。 然后并点击 next（下一步）点击 Finish（完成）按钮退出安装向导即可 检测PATH环境变量是否配置了Node.js，点击开始=》运行=》输入”cmd” =&gt; 输入命令”path”，输出如下结果： PATH=C:\oraclexe\app\oracle\product\10.2.0\server\bin;C:\Windows\system32; C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\; c:\python32\python;C:\MinGW\bin;C:\Program Files\GTK2-Runtime\lib; C:\Program Files\MySQL\MySQL Server 5.5\bin;C:\Program Files\nodejs\; C:\Users\rg\AppData\Roaming\npm 我们可以看到环境变量中已经包含了C:\Program Files\nodejs\检查Node.js版本 Windows 二进制文件 (.exe)安装形式32 位安装包下载地址 : http://nodejs.org/dist/v0.10.26/node.exe64 位安装包下载地址 : http://nodejs.org/dist/v0.10.26/x64/node.exe 步骤 1 : 双击下载的安装包 Node.exe点击 Run（运行）按钮将出现命令行窗口进入 node.exe 所在的目录，若是获得版本显示则证明安装成功 当然也可以在安装完成之后安装完成后可以使用cmd（win+r然后输入cmd进入）测试下是否安装成功。方法：在cmd下输入node -v若是出现版本号则证明安装成功 npm的安装。由于新版的NodeJS已经集成了npm，所以之前npm也一并安装好了。同样可以使用cmd命令行输入”npm -v”来测试是否成功安装 Build amazing thingsnpm is the package manager for JavaScript and the world’s largest software registry. Discover packages of reusable code — and assemble them in powerful new ways. npm Orgs is powerful collaboration — for freeEncourage code discovery and re-use within teamsPublish and control access to your own namespaceManage public and private code with the same workflow What is npm?Use npm to install, share, and distribute code; manage dependencies in your projects; and share &amp; receive feedback with others. NPM是随同NodeJS一起安装的包管理工具，能解决NodeJS代码部署上的很多问题，常见的使用场景有以下几种：允许用户从NPM服务器下载别人编写的第三方包到本地使用。允许用户从NPM服务器下载并安装别人编写的命令行程序到本地使用。允许用户将自己编写的包或命令行程序上传到NPM服务器供别人使用。 安装好Node.js和npm后可以进入下一步 2.安装git环境git是最流行的分布式版本控制系统，我们使用它主要是与github进行交互。安装git使用默认选项安装即可关于Git的学习http://https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/这里即可点击：https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/ Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. Git is easy to learn and has a tiny footprint with lightning fast performance. It outclasses SCM tools like Subversion, CVS, Perforce, and ClearCase with features like cheap local branching, convenient staging areas, and multiple workflows.Git(读音为/gɪt/。)是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。[1] Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。Torvalds 开始着手开发 Git 是为了作为一种过渡方案来替代 BitKeeper，后者之前一直是 Linux 内核开发人员在全球使用的主要源代码工具。开放源码社区中的有些人觉得BitKeeper 的许可证并不适合开放源码社区的工作，因此 Torvalds 决定着手研究许可证更为灵活的版本控制系统。尽管最初 Git 的开发是为了辅助 Linux 内核开发的过程，但是我们已经发现在很多其他自由软件项目中也使用了 Git。例如 很多 Freedesktop 的项目迁移到了 Git 上。 这里引用www.liaoxuefeng.com廖雪峰的官网上的说法，很清楚明朗的了解Git的原理 Git是目前世界上最先进的分布式版本控制系统那什么是版本控制系统？ 如果你用Microsoft Word写过长篇大论，那你一定有这样的经历： 想删除一个段落，又怕将来想恢复找不回来怎么办？有办法，先把当前文件“另存为……”一个新的Word文件，再接着改，改到一定程度，再“另存为……”一个新文件，这样一直改下去，最后你的Word文档变成了这样七零八乱的一改二改的版本过了一周，你想找回被删除的文字，但是已经记不清删除前保存在哪个文件里了，只好一个一个文件去找，真麻烦。 看着一堆乱七八糟的文件，想保留最新的一个，然后把其他的删掉，又怕哪天会用上，还不敢删，真郁闷。 更要命的是，有些部分需要你的财务同事帮助填写，于是你把文件Copy到U盘里给她（也可能通过Email发送一份给她），然后，你继续修改Word文件。一天后，同事再把Word文件传给你，此时，你必须想想，发给她之后到你收到她的文件期间，你作了哪些改动，得把你的改动和她的部分合并，真困难。 于是你想，如果有一个软件，不但能自动帮我记录每次文件的改动，还可以让同事协作编辑，这样就不用自己管理一堆类似的文件了，也不需要把文件传来传去。如果想查看某次改动，只需要在软件里瞄一眼就可以，岂不是很方便？ 这个软件用起来就应该像这个样子，能记录每次文件的改动这样，你就结束了手动管理多个“版本”的史前时代，进入到版本控制的20世纪。 最早Git是在Linux上开发的，很长一段时间内，Git也只能在Linux和Unix系统上跑。不过，慢慢地有人把它移植到了Windows上。现在，Git可以在Linux、Unix、Mac和Windows这几大平台上正常运行了。 在Windows上安装Git在Windows上使用Git，可以从Git官网直接下载安装程序，（网速慢的同学请移步国内镜像），然后按默认选项安装即可。 安装完成后，在开始菜单里找到“Git”-&gt;“Git Bash”，蹦出一个类似命令行窗口的东西，就说明Git安装成功！ 安装完成后，还需要最后一步设置，在命令行输入： $ git config --global user.name &quot;Your Name&quot; $ git config --global user.email &quot;email@example.com&quot; 因为Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址 创建版本库什么是版本库呢？版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 创建一个空目录 通过git init命令把这个目录变成Git可以管理的仓库 把文件添加到版本库 首先这里再明确一下，所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git也不例外。版本控制系统可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，虽然也能由版本控制系统管理，但没法跟踪文件的变化，只能把二进制文件每次改动串起来，也就是只知道图片从100KB改成了120KB，但到底改了啥，版本控制系统不知道，也没法知道。 不幸的是，Microsoft的Word格式是二进制格式，因此，版本控制系统是没法跟踪Word文件的改动的，前面我们举的例子只是为了演示，如果要真正使用版本控制系统，就要以纯文本方式编写文件。 因为文本是有编码的，比如中文有常用的GBK编码，日文有Shift_JIS编码，如果没有历史遗留问题，强烈建议使用标准的UTF-8编码，所有语言使用同一种编码，既没有冲突，又被所有平台所支持。千万不要使用Windows自带的记事本编辑任何文本文件。原因是Microsoft开发记事本的团队使用了一个非常弱智的行为来保存UTF-8编码的文件，他们自作聪明地在每个文件开头添加了0xefbbbf（十六进制）的字符，你会遇到很多不可思议的问题，比如，网页第一行可能会显示一个“?”，明明正确的程序一编译就报语法错误，等等，都是由记事本的弱智行为带来的。建议你下载Notepad++代替记事本，不但功能强大，而且免费！记得把Notepad++的默认编码设置为UTF-8 without BOM即可 搭载好Git后以后就可以通过Git Base Here命令行来操控Hexo了用windows的cmd也可以操作，但是有时候一些命令无法实现，所以尽量全部使用Git Base Here来操作 3.注册githubgithub就不用说了吧，它是一个面向开源及私有软件项目的托管平台。几乎所有的程序员都听说过它的大名。就正常注册一个账号就好了。注册号以后首先给我们的账号添加本机的SSH，具体方法及原因在这篇文章已经有了详细说明，并且方法也很简单]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F03%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 秘密的h收集资源密站 女儿，要对爸爸保密喔[32P]https://www.3344mf.com/tupianqu/katong/10029.html 来次悸动的H好吗？（上）https://www.3344mf.com/tupianqu/katong/10097.html 来次悸动的H好吗？（下）https://www.3344mf.com/tupianqu/katong/10077.html 主婦喝醉后遭遇的瘋狂倫奸[14P]https://www.3344mf.com/tupianqu/katong/11305.html https://www.3344mf.com/tupianqu/katong/12741.html]]></content>
  </entry>
</search>
